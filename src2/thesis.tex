\documentclass[a4paper]{book}

\usepackage[paperwidth=169mm, paperheight=239mm]{geometry}

%\usepackage[latin9]{inputenc}
\usepackage[utf8x]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{pdfpages}
\usepackage[british]{babel}
\usepackage{placeins}
\usepackage{dialogue}
\usepackage{fancyvrb}

\usepackage{t1enc}         
\usepackage{times}
\usepackage{url}
\usepackage{enumerate}

%\usepackage{code}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
%\usepackage{lingmacros}
\usepackage{subfig}
\usepackage{fancyhdr}


\pagestyle{plain}
%\pagenumbering{arabic}


%stuff
\usepackage{latexsym}
\usepackage{tikz,pgflibraryshapes}
\usetikzlibrary{shapes,snakes,positioning,matrix}
\usepackage{pgfplots}
\usepackage{booktabs}
\usepackage{enumerate}
%\usepackage{etoolbox}


\usepackage{multibib} 
\newcites{bb}{Bibliography} 
\newcites{ifl}{Bibliography}
\newcites{papp}{Bibliography}
\newcites{exp}{Bibliography}
\newcites{hl}{Bibliography}
\newcites{csort}{Bibliography}
\newcites{emb}{Bibliography}
\newcites{arbb}{Bibliography}
\newcites{t}{Bibliography}
%\newcites[ifl}{Bibliography}

%\patchcmd{\thebibliography}{\section*}{\section}{}{}

\makeatletter
\renewenvironment{thebibliography}[1]
     {\section*{\bibname}%
      %\@mkboth{\MakeUppercase\bibname}{\MakeUppercase\bibname}%
      \list{\@biblabel{\@arabic\c@enumiv}}%
           {\settowidth\labelwidth{\@biblabel{#1}}%
            \leftmargin\labelwidth
            \advance\leftmargin\labelsep
            \@openbib@code
            \usecounter{enumiv}%
            \let\p@enumiv\@empty
            \renewcommand\theenumiv{\@arabic\c@enumiv}}%
      \sloppy
      \clubpenalty4000
      \@clubpenalty \clubpenalty
      \widowpenalty4000%
      \sfcode`\.\@m}
     {\def\@noitemerr
       {\@latex@warning{Empty `thebibliography' environment}}%
      \endlist}
\makeatother


%%

%% \newcommand{\bequ}{\begin{quote}}
%% \newcommand{\enqu}{\end{quote}}
%% \newcommand{\eop}[1]{\mbox{\sl #1}}
%% \newcommand{\sube}[2]{#1_{#2}}
%% \newcommand{\id}[1]{{\it #1}}
%% \newcommand{\kwd}[1]    {\mbox{\textbf{#1}}}
%% \newcommand{\str}[1]{{\id "#1"}}
%% \newcommand{\lemon}{\emph{lemon}}
%% \newcommand{\textt}[1]{\texttt{#1}}
%% \newcommand{\txttt}[1]{\texttt{#1}}

%\renewcommand*{\thesection}{\arabic{section}}

\newcommand{\thesistitle}{Embedded Languages for Data-Parallel Programming}
\newcommand{\dept}{Department of Computer Science and Engineering}
\newcommand{\uni}{Chalmers University of Technology and G\"oteborg University}
\newcommand{\group}{Functional Programming Research Group}


% ---------------------------------------------------------------------------
% Papers 
% ---------------------------------------------------------------------------
\newcommand{\paperA}{Paper A}
\newcommand{\paperATitle}{Simple and Compositional Reification of Monadic Embedded Languages}
\newcommand{\paperB}{Paper B}
\newcommand{\paperBTitle}{Obsidian: A Domain Specific Embedded Language for Parallel Programming of Graphics Processors}
\newcommand{\paperC}{Paper C} 
\newcommand{\paperCTitle}{GPGPU Kernel Implementation and Refinement using Obsidian} 
\newcommand{\paperD}{Paper D}
\newcommand{\paperDTitle}{Expressive Array Constructs in an Embedded GPU Kernel Programming Language}
\newcommand{\paperE}{Paper E}
\newcommand{\paperETitle}{Counting and Occurrence Sort for GPUs using an Embedded Language}
\newcommand{\paperF}{Paper F}
\newcommand{\paperFTitle}{A High-Level Embedded Language for Low-Level GPU Kernel Programming}
\newcommand{\paperG}{Paper G}
\newcommand{\paperGTitle}{Programming Future Parallel Architectures with Haskell and Intel ArBB}
\newcommand{\paperH}{Paper H}
\newcommand{\paperHTitle}{Parallel Programming in Haskell Almost for Free}

% ---------------------------------------------------------------------------
% BEGIN ! 
% ---------------------------------------------------------------------------
\begin{document}

\begin{titlepage}
\begin{centering}
{\sc Thesis for the Degree of Doctor of Philosophy}
\vspace{30ex}

{\LARGE\bf\thesistitle}

\vspace{7ex}

\large Bo Joel Svensson
\vfill
\includegraphics[width=120mm]{./img/ChalmGUtextsvEng}\\[5mm]
\includegraphics[height=4cm]{./img/ChalmGUmarke}\\

\vspace{1cm}
\normalsize
{\sc \dept}\\
{\sc \uni}\\
G\"oteborg, Sweden 2013

\end{centering}
\end{titlepage}

% ---------------------------------------------------------------------------
% Tryckortssida
% ---------------------------------------------------------------------------
%ditt ISBN är:  978-91-7385-939-4
%ditt Ny serie nr:  3620
%ISSN för publ.serien Doktorsavhandlingar vid Chalmers tekniska högskola : 0346-718X

\quad \vfill

{\noindent\large\bf\thesistitle} \\
\noindent Bo Joel Svensson \\
\noindent ISBN 978-91-7385-939-4 \\

\vspace{1cm}

\noindent\copyright {\sc {Bo Joel Svensson}}, 2013 \\ 
\vspace{1cm} 


\noindent Doktorsavhandlingar vid Chalmers tekniska högskola \\
\noindent Ny serie nr 3620 \\
\noindent ISSN 0346-718X \\

\vspace{1cm}

\noindent Technical Report 101D\\
\noindent \dept \\
\noindent \group\\

\vspace{1cm} 

\noindent \uni \\
\noindent SE--412~96~~G\"oteborg, Sweden\\
\noindent Phone: +46 (0)31--772~1000 \\

\vspace{1cm} 

\noindent Printed in Sweden\\
\noindent Chalmers Reproservice\\
\noindent G\"oteborg, Sweden 2013


\thispagestyle{empty}

\clearpage
\pagenumbering{roman}

% ---------------------------------------------------------------------------
% ABSTRACT !!  
% ---------------------------------------------------------------------------
\section*{Abstract}

Computers today are becoming more and more parallel. General purpose processors (CPUs) 
have multiple processing cores and Single Instruction Multiple Data (SIMD) units for 
data-parallelism. Graphics processors (GPUs) bring massive parallelism at the cost of 
being harder to program than CPUs. This thesis applies embedded language methodology 
to data-parallel programming. Two embedded languages are presented, Obsidian for general 
purpose GPU programming and EmbArBB for data-parallel programming across platforms. 

CPUs and GPUs get more parallel resources with each new generation. The question of how to 
program these processors efficiently arises. We are after efficiency both in programmer 
productivity and in application performance. Using embedded languages allows us to experiment
with what abstractions to present to the programmer at relatively little effort.

Obsidian is an embedded language for general purpose programming of GPUs. We try to strike 
a balance between high level, productivity increasing abstractions and low-level 
control needed for performance. The Obsidian programming model mirrors the GPU architecture 
and the programmer is constrained into writing GPU-friendly code. By supplying hierarchy level 
polymorphic library functions, these constraints does not feel obtrusive. Obsidian programs are 
compiled into CUDA C code. This compilation is based on a simple and elegant monad reification 
technique.  

In cases where the programmer is not interested in low-level details 
or wants the program to run over a range of hardware, a higher level language can be used.
EmbArBB is a Haskell embedding or the Intel ArBB system. EmbArBB relies on the ArBB system 
to generate code (via a Just-In-Time compiler) to a range of hardware. 

EmbArBB embeds a preexisting library for data-parallelism into Haskell and we obtain 
very good performance at little implementation effort. This performance comes from the 
expertise and effort put into the ArBB system and that we get for free. Embedding ArBB is 
a way to provide these benefits to the Haskell programmer and a way to increase usefulness
of an existing system by opening it up to a wider audience. Obsidian is very different; it 
is not based on a set of high-level parallel primitives. The Obsidian programmer 
can implement these primitives in different ways and then select the best one. 
We have obtained very good performance in case studies involving reductions. Obsidian 
programs are also more terse and composable, compared to CUDA. 
 
\vspace{5mm}

\noindent

\textbf{Keywords:} Data-parallelism, Embedded languages, Functional Programming, Graphics Processing Units

\clearpage

% ---------------------------------------------------------------------------
%  ACKS !!! 
% ---------------------------------------------------------------------------
\section*{Acknowledgments}

I would like to thank my supervisor Mary Sheeran. She is a very supportive, motivating and 
understanding supervisor. I also thank my co-supervisor Koen Claessen for all the great advice 
and tips he has provided over the years. I cannot imagine that this would have worked with 
any other supervisors. Thanks to Josef Svenningsson, a late addition to my troop of 
supervisors. Working with Josef has been fun and rewarding. 

I have often turned to Emil Axelsson with technical questions, and he has always been
happy to help. For this I am grateful. 

I thank Manuel Chakravarty for letting me visit his group at the University of New South Wales 
and Trevor McDonell and Sean Lee for taking care of me during my visit. I look back 
very fondly to my stay in Australia. 

For being a great supervisor during my three month internship at Intel, I thank Ryan Newton. 
These where three very intense months away from family but at the same time a very rewarding 
experience. 

Thanks to my thesis opponent Stephen Edwards for valuable comments on a draft of this thesis. 

I want to thank Erik, Ola, Markus and Viktor for our occasional lunches and GPU discussions. 
I also thank my office mates, Michal and Nicholas, for their company. I thank Eva Axelsson, 
who I have felt most comfortable to turn to for help and information, even when she was not 
the appropriate channel. 

Finally I want to thank my family and friends. Thanks to my sisters and parents for their support.
I thank my friends for being understanding during times when I have been very self-absorbed. 
I am very grateful to my wife Andrea for supporting and believing in me and to our daughter Noemi 
for bringing much joy to my breaks from work while writing this thesis; soon she is old enough 
for real LEGO. 

\vspace{5mm}
\noindent This research has been funded by the Swedish Foundation for
Strategic Research (which funds the Resource Aware Functional 
Programming (RAW FP) Project) and by the Swedish Research Council.

\clearpage

% ---------------------------------------------------------------------------
%  Guide for Opponent, grading committee
% ---------------------------------------------------------------------------
\section*{Publications}

This thesis includes the following publications: 

\begin{enumerate}[A.] 
\item Josef Svenningsson and Bo Joel Svensson. Simple and Compositional Reification of Monadic Embedded Languages, 2013. \emph{18th ACM SIGPLAN International Conference of Functional Programming, ICFP 2013.}
\item Bo Joel Svensson, Mary Sheeran and Koen Claessen. Obsidian: A Domain Specific Embedded Language for General Purpose Parallel Programming of Graphics Processors. In \emph{Proc. of Implementation and Applications of Functional Languages (IFL)}, Lecture Notes in Computer Science, Springer Verlag, March 2009.
\item Bo Joel Svensson, Koen Claessen and Mary Sheeran. GPGPU kernel implementation and refinement using Obsidian. \emph{Practical Aspects of High-level Parallel Programming, PaPP 2010.} Procedia Computer Science.
\item Koen Claessen, Mary Sheeran and Bo Joel Svensson. Expressive Array Constructs in an Embedded GPU Kernel Programming Language. In \emph{Proceedings of the 7th workshop on Declarative aspects and applications of multicore programming}, DAMP '12. ACM. 
\item Josef Svenningsson, Bo Joel Svensson and Mary Sheeran. Efficient Counting Sort Implementations using an Embedded GPU Programming Language. \emph{2nd Workshop on Functional High-Performance Computing, FHPC '13.}
\item Bo Joel Svensson and Mary Sheeran. A High-Level Embedded Language for Low-Level GPU Kernel Programming. {\bf\emph{This work has not yet been published.}} 
\item Bo Joel Svensson and Ryan Newton. Programming Future Parallel Architectures with Haskell and ArBB. \emph{Future Architectural Support for Parallel Programming (FASPP), in conjunction with ISCA '11.}
\item Bo Joel Svensson and Mary Sheeran. Parallel Programming in Haskell Almost for Free: an embedding of Intel's Array Building Blocks. \emph{Proceedings of the 1st ACM SIGPLAN Workshop on Functional High-Performance computing, FHPC '12.}
\end{enumerate} 

\clearpage
\subsection*{Personal contributions} 

The following describes my personal contributions to the papers included 
in this thesis. 

\subsubsection{Paper A}
I intuitively applied the monad reification method as part of the implementation 
of Obsidian. Josef Svenningsson identified its importance and suggested we write 
about it. The paper has two distinct parts. Sections one and two describe the 
method as I used it. And section four makes the compositional aspect of the method 
explicit. 

The code and examples in section one and two are my work. The contents of section four was 
developed by Josef Svenningsson and me. For section four we also got very much and valuable help 
from Emil Axelsson. 

For the rest of the paper, just substituting my name for Benny and Josef's for Bj\"orn
will give a good indication of our contributions. 

\subsubsection{Papers B, C, D and E} 
The Obsidian language has grown out of discussions between Mary Sheeran, Koen Claessen and me. 
When it comes to implementation of the ideas we had in these discussions, I have been responsible. 
Push arrays, first appearing in paper D, were invented by Koen Claessen. However, the 
implementation of them in the setting of Obsidian was done by me. Paper D also has large 
contributions by Mary Sheeran when it comes to examples and implementing them. 

Paper E was the first I wrote with Josef Svenningsson, who had been thinking about an interesting
variation of counting sort and wanted to implement it. Together we added atomic operations to 
Obsidian and I modified the code generator to accommodate them. Sections 
1 and 2 in the paper are written by Josef. Sections 3 through 7 are mostly my work, including 
the benchmarking. Section 8 is joint work.
%Me and Josef are equal partners in this paper and got much valuable advice from 
%Mary Sheeran. 

\subsubsection{Papers G and H} 
The work behind paper G, work was performed by me 
under supervision of Ryan R. Newton who is also mostly responsible for the writing of the 
paper. Programming and running time experimentation was performed by me, while getting much 
valuable advice and guidance from Ryan. 

The work behind paper H was done by me as a hobby project and my aspirations were not 
high. I just wanted to experiment with some embedded languages techniques that we had 
been avoiding (had no need for) in Obsidian. Mary discovered what I was doing and applied 
the right amount of pressure, leading to us encompassing much more of ArBB functionality than 
I had in mind. The paper's text is joint work; most examples as well as benchmarking are my work,
but the sparse matrix vector multiplication example is entirely Mary's. 

%Intel has unfortunately retired the ArBB project. This leaves EmbArBB without a functioning 
%code generating backend u

% \subsection*{Guide} 

\tableofcontents


\cleardoublepage
\clearpage

\pagenumbering{arabic}
\pagestyle{fancy}
\fancyfoot{}
\fancyhead[LO]{}
\fancyhead[RO]{\leftmark}
\renewcommand{\headrulewidth}{0.0pt}
\fancyhead[LE,RO]{\thepage}

% --------------------------------------------------------------------------- %
% --------------------------------------------------------------------------- %
%
%  INTRODUCTION 
%
% --------------------------------------------------------------------------- %
% --------------------------------------------------------------------------- %
\chapter{\thesistitle}

% -------------------------------------------------------------------------
%  INTRO SECTION OF INTRO CHAPTER
% -------------------------------------------------------------------------

\section{Introduction} 

This thesis applies embedded language methodology to data-parallel programming of 
massively parallel hardware. An embedded language is a language that is implemented 
as a library for some existing host language. In this case, the functional programming 
language Haskell is used as host language. 

Computers today are increasingly parallel. Each new generation of general purpose processors
(CPUs) and of graphics processors (GPUs), brings more parallelism. The question 
of how to program these computers most efficiently, both from a productivity and 
a performance point of view, arises. Explicit parallel programming is hard and automatic 
means of parallelisation, while giving a sequential programming model to the programmer, 
often misses the target when it comes to performance. The book ``Structured Parallel 
Programming'' by Michael McCool et al argues for the need of explicit parallel programming 
models~\citet{STRUCTURED}.

The embedded language approach to programming language implementation is beneficial, 
since we are not entirely sure of how to program current and future highly parallel 
computers. Implementing an embedded language allows us more rapid prototyping of ideas. 
We can experiment with what abstractions to expose to the programmer, without much 
effort. Other benefits of embedded languages often mentioned are: making use 
of the host language's library functions, if they are general enough. The host language 
can be used as a powerful macro language for the embedded language. An embedded 
language also provides a more leveled learning curve to programmers already familiar 
with the host language. For these programmers, using an embedded language is more like  
learning about a new library. 

This thesis focuses on data-parallelism, as opposed to task or control parallelism.
Data-parallelism is applicable where the input data can be divided into chunks
and operated upon independently. Data-parallelism is scalable; the potential 
for parallelism increases with the problem size. Two different embedded languages 
for data-parallel programming are presented in this thesis: Obsidian, for 
general purpose computations on GPUs and EmbArBB for parallel programming across 
platforms, CPU, GPU and other accelerators. 

Graphics Processors (GPUs) trade programmer convenience for increased parallelism. 
For example, where CPUs dedicate large portions of chip area to caches, branch 
prediction and instruction level parallelism (ILP), GPUs put additional cores. 
This comes at a price of increased programmer effort. GPUs have programmer managed 
shared memories, conditionals are troublesome from a performance 
perspective and memory access patterns can make or break the performance of an 
application. With Obsidian, we try to find the right level of abstraction for 
programming GPUs. We try to strike a balance between low-level control of details 
that influence performance and high-level abstractions to increase productivity and 
make programs more terse. The end goal with Obsidian is a language for writing 
high performance GPU programs that compare well to code written by CUDA~\citet{CUDA}  
(a C-dialect for GPU programming) experts, provided that both programmers have 
similar GPU expertise. The Obsidian programmer would however do so in a more 
composable manner. Obsidian programs are compiled into CUDA.  

Obsidian is geared towards the programmer who is interested in GPU hardware. Making 
good use of Obsidian requires that the programmer is willing to make an effort 
to learn about GPUs and how to program them efficiently. However, another case 
is a programmer who does not want to invest that effort but wants to write a high-level 
parallel program and have it run reasonably well on different parallel architectures. 
In this case, a language with even higher level of abstraction is preferable. It then 
becomes the job of a more advanced compiler to turn that into code for the targeted 
platforms. The EmbArBB language targets this programmer. EmbArBB exposes a set of high-level 
operations on arrays, such as {\tt map}, {\tt reduce} and {\tt scan}. The programmer 
has no power over the parallel strategy that should be used to implement these 
operations but instead relies on a JIT-compiler (Just In Time-compiler). EmbArBB is a 
Haskell embedding of Intel ArBB that was released as an embedded language in C++. 
EmbArBB relies entirely on the ArBB system for code generation and optimisation.  

% -------------------------------------------------------------------------
% Obsidian !
% -------------------------------------------------------------------------

\section {Obsidian: An embedded language for GPU kernel implementation} 

In 2006 NVIDIA released their first Compute Unified Device Architecture (CUDA) 
GPU. With CUDA, NVIDIA also made it a lot easier for programmers to use GPUs
for general purpose programming. A dialect of C, called CUDA C, was released 
specifically to simplify the implementation of general purpose computations on GPUs. 

Before CUDA, programmers still tried (very ingeniously) to make use of GPUs for 
non-graphics applications via the graphics API. The desired computation was expressed 
in graphics terms and the result computed through an act of rendering. Of course, since the 
available abstractions targeted the graphics domain, this added extra
burden on the programmer. CUDA was NVIDIA's first response to this problem. CUDA 
is a dialect of C, extended with a parallel programming model suitable for GPUs. 

Having a language specifically made for general purpose programming on the GPU, such as 
CUDA, is a big step up from earlier. But CUDA still offers a very low-level interface to 
programming the GPU, which is preferred in a situation where the programmer needs to 
think about hardware specific details to get the best performance. CUDA provides this, 
but at the price that it is hard to build upon, abstract and reuse prior programming 
effort. This is where we are trying to go one step further with Obsidian. 

% -------------------------------------------------------------------------
% GPU Hardware 
% -------------------------------------------------------------------------

\subsection{GPU hardware} 

Before going into programming of GPUs, it is important to have some background. The 
GPU programming model exposed by CUDA very much mirrors the underlying hardware. Some 
of the details that make GPU programming hard is more apparent when looking at the 
underlying hardware. 

A CUDA GPU is built around a single kind of processor (as opposed to the different 
kinds of processors found in earlier GPUs). The processors in the GPU (called MPs, 
MultiProcessors) all contain a number of cores called SPs (Streaming Processors). 
Each MP also contains local memory, called shared memory since it can be accessed 
by all of the SPs in that MP. The number of MPs varies over the available GPUs; cheaper 
GPUs have as few as one MP, and as you go up in price the number of MPs increases.

Each MP of the GPU can manage a large number of threads; on today's GPUs up to 
2048 threads can run on a single MP. The GPU schedules threads in groups of 32, called 
{\em Warps}, that are executed in lock-step (SIMD style execution). Threads are 
also divided into {\em Blocks}; the threads within a block can communicate using the 
shared memory. The maximum block size is currently 1024 threads. Since 
the number of threads in a block can be higher than the warp size, a barrier 
synchronisation mechanism exists to ensure that threads within a block 
can communicate safely via the shared memory. Blocks are also grouped into a grid that 
is the collection of blocks executing the same program. 

% -------------------------------------------------------------------------
%  GPU Programming
% -------------------------------------------------------------------------

\subsection{GPU programming} 
\label{GPUPROGRAMMING}

The CUDA programming model reflects the hierarchical architecture of the GPU. The 
programmer writes what are called \emph{kernels}; in CUDA jargon a kernel refers to a sequential 
C program that is parameterised on a thread's identity. The kernel program is executed 
in parallel across all the threads of a block and many such blocks of threads can execute 
in parallel on the available MPs. Kernels may use shared memory that is local to a block; 
the programmer needs to decompose the algorithm and data in such a way that communication 
will be local to the blocks. 


The program below is a very simple example of a CUDA kernel that computes elementwise sums. 

\begin{small}
\begin{Verbatim}[samepage=true]
__global__ void vecAdd(int32_t *i1, int32_t *i2, int32_t *r) {
  
  unsigned int gid = blockIdx.x * 
                     blockDim.x + 
                     threadIdx.x;

  r[gid] = i1[gid] + i2[gid]; 

} 

\end{Verbatim}
\end{small}

\noindent The kernel takes two input arrays, {\tt i1} and {\tt i2}; the result is stored 
in {\tt r}. Note how the kernel makes use of three variables, {\tt threadIdx.x}, 
{\tt blockIdx.x} and {\tt gridDim.x}. These variables identify a thread uniquely within 
a grid. Here they are used to obtain an index into the provided arrays. The dimensions of 
the blocks and of the grid are set by a controlling program upon launch of the kernel. This 
controlling program is run by the CPU of the computer housing the GPU. The kernel is launched 
using the following syntax: 

\begin{small} 
\begin{Verbatim}[samepage=true]
vecAdd<<<1,32,0>>>(d_i1,d_i2,d_r);
\end{Verbatim}
\end{small}

\noindent This line of code launches the kernel on one block of 32 threads. The zero 
indicates that the kernel uses no shared memory. If the kernel did use shared memory, the 
amount needed should have been specified there. Figure~\ref{fig:CUDACODE1} shows the complete 
sequence of commands for a minimal example using the kernel specified above. 

% -------------------------------------------------------------------------
% CUDACODE1 FIGURE
% -------------------------------------------------------------------------

\begin{figure} 
\begin{small}
\begin{Verbatim}[samepage=true]
int main(void) {
  
  int32_t h_i1[32], h_i2[32], h_r[32]; 

  int32_t *d_i1, *d_i2, *d_r;

  
  for (int i = 0; i < 32; ++i) { 
    h_i1[i] = i;
    h_i2[i] = 32 - i;
  }

  // Allocate device memory
  cudaMalloc((void**)&d_i1,32*sizeof(int32_t));
  cudaMalloc((void**)&d_i2,32*sizeof(int32_t));
  cudaMalloc((void**)&d_r,32*sizeof(int32_t));
  
  // Copy inputs to device
  cudaMemcpy(d_i1,
             h_i1,
             32*sizeof(int32_t),
             cudaMemcpyHostToDevice);
  cudaMemcpy(d_i2,
             h_i2,
             32*sizeof(int32_t),
             cudaMemcpyHostToDevice);
  
  // Launch the kernel
  vecAdd<<<1,32,0>>>(d_i1,d_i2,d_r); 

  // Copy result to host
  cudaMemcpy(h_r,
             d_r,
             32*sizeof(int32_t),
             cudaMemcpyDeviceToHost);

  for (int i = 0; i < 32; ++i) { 
    printf("%d ",h_r[i]);
  }

  return 0;
}
\end{Verbatim} 
\end{small}

\caption{Host code for launching a simple CUDA kernel on the GPU.}
\label{fig:CUDACODE1}

\end{figure} 

% -------------------------------------------------------------------------
% 
% -------------------------------------------------------------------------

In Obsidian, the same kernel (the local computation) is implemented as follows: 

\begin{small}
\begin{Verbatim}[samepage=true]
vecAdd :: Num a => SPull a -> SPull a -> SPull a
vecAdd = zipWith (+)
\end{Verbatim}
\end{small}

\noindent The {\tt vecAdd} program takes two {\tt SPull} input arrays and gives one 
as result. The Obsidian {\tt vecAdd} is more general than the CUDA version. From it 
we can generate CUDA kernels for any element type that supports the {\tt (+)} operation. 
In Haskell we know this is true for types that are member of the {\tt Num} typeclass.  

There are some important differences between CUDA and Obsidian programs. One is that with Obsidian 
we generate kernels for fixed size local computations. This is to avoid doing out-of-bounds check 
which would mean conditionals in the code. Conditionals are problematic from a performance 
point of view. Another difference is that in Obsidian the programmer specifies the entire 
computation, including how it is mapped over the blocks of the GPU. This is done implicitly 
in CUDA, based on the indexing arithmetic used in the kernel and the number of blocks and 
threads provided at kernel launch. This means that the Obsidian programmer needs to 
write a program that performs this mapping: 

\begin{small} 
\begin{Verbatim}[samepage=true]
vecAddG :: Num a => DPull a -> DPull a -> DPush Grid a
vecAddG i1 i2  =
  pConcat $
  fmap push $ 
  zipWith vecAdd
          (splitUp 32 i1)
          (splitUp 32 i2)  
\end{Verbatim}
\end{small} 

The {\tt vecAddG} program specifies a computation on large arrays; the large input arrays 
are split up into pieces that are operated upon by the local computation, {\tt vecAdd}, 
specified above. Each input array passed to this function must have length a multiple of 32.
Since the {\tt splitUp} operation splits the arrays into chunks of 32 elements, the body 
of the generated kernel will be constructed specifically for this size. {\tt zipWith} is 
used to map the local computation onto each chunk of the input arrays. The result is an 
array of arrays that is flattened with the {\tt pConcat} function. The CUDA code below 
is generated from the {\tt vecAddG} program: 

\begin{small}
\begin{Verbatim}[samepage=true] 
extern "C" __global__ void vecAdd(int32_t* input0, uint32_t n0,
                                  int32_t* input1, uint32_t n1,
                                  int32_t* output2)
{
    uint32_t tid = threadIdx.x;
    
    if (blockIdx.x < min(n0 / 32U, n1 / 32U)) {
        output2[blockIdx.x * 32U + tid] = 
            input0[blockIdx.x * 32U + tid] +
            input1[blockIdx.x * 32U + tid];
    }
}
\end{Verbatim}
\end{small} 

There are some superficial differences between the code generated using Obsidian and the 
handwritten example above. One such difference is that the index computation is not shared. 
Obsidian relies on the CUDA compiler to discover such sharing via a Common Subexpression 
Elimination (CSE) optimisation pass. Another difference is that Obsidian has inserted a 
conditional that only executes the body for certain blocks. Discussion of the causes for 
that conditional is postponed until a later section~\ref{sec:OBSARRAYS}. A real 
difference is that the kernel is generated for a fixed size, here there is a constant (32) 
where in the handwritten code we used {\tt blockDim.x}. If those differences are set 
aside, the two kernels are identical. Figure~\ref{fig:OBSIDIANHOST1} shows how the 
{\tt vecAddG} kernel can be launched from within Haskell, using Obsidian's rudimentary 
support for launching GPU computations. 

An alternative way to implement {\tt vecAddG} is to inline the local computation 
directly. This makes the entire program look like this. 

\begin{small}
\begin{Verbatim}[samepage=true]
vecAddG :: Num a => DPull a -> DPull a -> DPush Grid a
vecAddG i1 i2  =
  pConcat $
  fmap push $
  zipWith (zipWith (+)) 
          (splitUp 32 i1)
          (splitUp 32 i2)
\end{Verbatim} 
\end{small} 

\noindent The generated code looks exactly as before. 

% -------------------------------------------------------------------------
% OBSIDIANHOST1 FIGURE
% -------------------------------------------------------------------------

\begin{figure} 
\begin{small}
\begin{Verbatim}[samepage=true]
performVecAdd =
  withCUDA $
  do
    kern <- capture 32 (vecAddG :: DPull EInt32
                                 -> DPull EInt32
                                 -> DPush Grid EInt32)

    useVector (V.fromList [0..32::Int32]) $ \i1 ->
      useVector (V.fromList [0..32::Int32]) $ \i2 -> 
        allocaVector 32 $ \(o :: CUDAVector Int32) ->
        do
          fill o 0
          o <== (1,kern) <> i1 <> i2 

          r <- peekCUDAVector o
          lift $ putStrLn $ show r
\end{Verbatim} 
\end{small}

\caption{Host code for launching a simple Obsidian kernel on the GPU. \\ 
 {\tt capture} compiles an Obsidian program into CUDA. At this point the 
  types of the Obsidian program need to be made concrete. The number, 32, used as 
 an argument to {\tt capture} specifies that generated code should be specialised 
 for 32 threads per block. }
\label{fig:OBSIDIANHOST1}
\end{figure} 

\pagebreak 

\noindent A CUDA kernel that uses shared memory takes the form: 

\begin{small}
\begin{Verbatim}[samepage=true]
__global__ void kernel(int32_t *i, int32_t *r) {

  extern __shared__ int32_t sm[]; 

  unsigned int tid = threadIdx.x; 
  unsigned int gid = blockIdx.x * 
                     blockDim.x + 
                     threadIdx.x;

  sm[tid] = i[gid]; 
  __syncthreads();

  /* Compute on sm */ 
     
  __syncthreads();
  r[gid] = sm[tid]; 
  
}
\end{Verbatim} 
\end{small} 

\noindent A portion of the input array is stored into shared memory. A 
barrier synchronisation is used to ensure that each thread within the block 
has stored its value into shared memory (only necessary if threads will 
communicate across warp boundaries). After computing on the data locally, 
it is written back into global memory. This kernel sets up two aliases for 
thread identities. A thread's global identity is called {\tt gid} and its local 
identity is called {\tt tid}. The global ID is used to index into the large global 
array; indexing into local shared memory is done using the local ID. 

In Obsidian, local computations can also use shared memory. The Obsidian arrays 
we have seen in the examples, {\tt Pull} and {\tt Push} arrays do not directly 
correspond to actual data in memory. Pull and push arrays, rather represent ways 
to compute arrays. More information about these array representations is available 
in section~\ref{sec:OBSARRAYS}. The Obsidian programmer can store intermediate 
arrays in shared memory by using a {\tt force} operation. 
%The 
%{\tt force} operation computes an array and stores it into shared memory. 

The following kernel multiplies each element in an array by two and then adds one: 

\begin{small}
\begin{Verbatim}[samepage=true] 
kernel :: Num a => SPull a -> SPull a
kernel = fmap (+1) . fmap (*2)
\end{Verbatim}
\end{small}

\noindent This kernel computes its result without any storage of intermediate results;
the code is equivalent to \verb!fmap ((+1).(*2))!. 
In the generated CUDA code below, it can be seen that the (*2) and (+1) operations 
are applied without any intermediate storage. 

\begin{small}
\begin{Verbatim}[samepage=true]
extern "C" __global__ void kernel(int32_t* input0, uint32_t n0,
                                  int32_t* output1)
{
    uint32_t tid = threadIdx.x;
    
    if (blockIdx.x < n0 / 32U) {
        output1[blockIdx.x * 32U + tid] = 
          input0[blockIdx.x * 32U + tid] * 2 + 1;
    }
}
\end{Verbatim}
\end{small}

Storing an intermediate result between the two operations can be accomplished by 
changing the kernel as follows. 

\begin{small}
\begin{Verbatim}[samepage=true]
kernelF :: (MemoryOps a, Num a)
         => SPull a -> BProgram (SPull a)
kernelF = liftM (fmap (+1)) . force . fmap (*2)
\end{Verbatim} 
\end{small} 

This change introduced some noise. The reason for this is that Obsidian programs that 
use memory are monadic. The {\tt BProgram} in the result type is a monad. Another difference 
is that we need to be able to write the array elements to memory; in Obsidian this means 
that the element type must be in the {\tt MemoryOps} class. 

The code below is generated using the {\tt kernelF} program: 

\begin{small} 
\begin{Verbatim}[samepage=true]
extern "C" __global__ void kernel(int32_t* input0, uint32_t n0,
                                  int32_t* output1)
{
    extern __shared__ uint8_t sbase[];
    uint32_t tid = threadIdx.x;
    
    if (blockIdx.x < n0 / 32U) {
        ((int32_t*) sbase)[tid] = input0[blockIdx.x * 32U + tid] * 2;
        __syncthreads();
        output1[blockIdx.x * 32U + tid] = ((int32_t*) sbase)[tid] + 1;
    }
}
\end{Verbatim}
\end{small} 

\noindent This code uses a shared memory array called {\tt sbase}. There 
is also a call to {\tt \_\_syncthreads} to ensure that all the writes have 
completed before proceeding. In this case the synchronisation is not really
necessary since all threads belong to the same warp. There is a {\tt unsafeforce} 
operation that naively removes synchronisations if the array being forced is short 
enough. More advanced analysis could also be performed to remove synchronisations 
whenever communication is entirely warp local. We do not apply that optimisation, 
but work on warp level computations is in progress and would put this ability 
in the hands of the programmer.  

\FloatBarrier 
\subsection{Writing efficient GPU Kernels} 
\label{sec:efficient}
 
Writing efficient GPU kernels is hard. There are many details that the CUDA 
programmer must be aware of in order to make optimal use of the GPU. 
The NVIDIA ``CUDA C Best Practices Guide''~\citet{BestPrac}, is a good source 
to learn more about these details. Here is a list, with explanations of some 
of the practices that the guide deems most important. The list also contains 
information about how these best practices influence our work on Obsidian.  
  
\begin{itemize} 

\item {\bf Minimise data transfers between host and device:} The bandwidth between device memory 
and GPU is much higher than the bandwidth between host memory and the GPU device via the PCIe bus.
This means that, first of all one must evaluate if the computation to be performed is significant 
enough to warrant a transfer to the GPU. If the computation is not arithmetic intensive, it may be 
better to do the work on the CPU. However, if the data is already in device memory and 
the operation that is about to perform on it is more efficiently handled by the CPU, it may still 
be better to let the GPU do the work. In other words, compute near the data unless transferring it 
pays off. 

Data transfers between the host memory and device (GPU) memory is out of Obsidian's scope. 
If Obsidian is used to generate kernels for a computation, the programmer has already decided that 
a GPU should be used. However, even after making the decision to use the GPU the programmer 
may need to consider how to transfer the data. In some cases it is possible to overlap 
computation on the GPU and data transfer. These kinds of optimisations are considered 
when writing the C program that launches the CUDA kernels. Obsidian does have rudimentary 
support for launching kernels from within Haskell but currently we consider this mostly a
tool for quickly being able to run a kernel and see what it does, not as a means to implement 
applications. This part of Obsidian is future work. 

\item {\bf Ensure that global memory accesses are coalesced:} Certain access patterns into 
GPU device memory can be {\em coalesced} (combined into few memory transactions). Unfortunately 
what these access patterns are varies somewhat depending on the GPU. For a typical model of 
GPU, the rule is that concurrent memory accesses (by threads within a warp) will be coalesced 
to as many transactions as cache lines touched (128b l1 cache lines). This means that strided 
accesses from within a warp are not favoured. 

Inputs to an Obsidian program are pull arrays. Pull arrays can be arbitrarily permuted; 
the permutation used will decide the access pattern. If this method can be applied or 
not partly also depends on the algorithm being implemented. 

\item {\bf Minimise use of global memory:} Accessing global device memory can take as 
many as 400 to 600 clock cycles if the data is not cached. The programmer should 
use shared memory as much as possible to avoid redundant loading from global memory.
Shared memory can also be used to coalesce reads from global memory. Shared memory is 
a limited resource in the MP, so if a kernel uses very much shared memory it means 
fewer such blocks can be active on the MP. 

Intermediate arrays are stored into shared memory when the Obsidian programmer uses 
the {\tt force} function. 

\item {\bf Use a multiple of 32 threads per block:} Groups of 32 threads, warps, are the 
scheduled unit on a GPU. If a warp accesses memory (and will be waiting for it arrive during 
600 cycles) it will be swapped out and another warp will take its place. This means that 
the number of available warps decides how well memory latency can be hidden. A block that is 
not a multiple of 32 leads to wasting GPU resources; some processing elements will stand 
idle during the execution of the not full warps. 

Obsidian requires regularity even more than CUDA does. Since we focus on generating high 
performance kernels, we rule out generation of kernels that work on different sizes (per block). 
The choice of how many threads to use per block is in the hands of the Obsidian programmer.
The method for making that choice has changed over time. Earlier, the number of threads 
needed by a kernel was decided by the size of the arrays it computed. This has recently changed
and is now set by a parameter passed to the code generation process.  

\item {\bf Avoid different execution paths within a block:} Any branching instruction that 
causes the execution to branch into different code (in CUDA jargon called a diverging branch) 
within a warp will affect performance. When the execution paths within a warp diverge, the 
computation is serialised. 

Sometimes divergent code cannot be avoided. However, it is important not to have threads 
diverge unnecessarily. When using pull arrays alone this was often a problem; with the 
addition of push arrays the situation improved. Using push arrays some divergent computations 
can be explicitly split up into phases, each phase with all threads following the 
same execution path. This means that fewer threads are required to do the same work. 

\item {\bf Do not use {\tt \_\_synchthreads()} in diverging code:} Extreme care must be taken 
when synchronising within any conditionally executed code; every thread must reach this barrier.
Failing to ensure this will likely lead to the kernel producing incorrect results. Note that 
this is a major limitation to the composability of CUDA functions. Wheather or not it is safe to 
call some function in a certain place in your code can be decided only by inspecting that 
function's implementation. 

In Obsidian it is impossible for a barrier synchronisation to appear within diverging 
conditional code. Conditionals can be introduced either by an \\{\tt ifThenElse} function 
at the element level or by a {\tt Cond} statement that is an operation in the {\tt Program} 
monad; we have seen an example of the program monad in section~\ref{sec:GPUPROGRAMMING}. 
The {\tt Program} monad is parameterised on the GPU hierarchy level. There are 
thread programs ({\tt TProgram}), block programs ({\tt BProgram}) and grid programs 
({\tt GProgram}). the {\tt Cond} conditional choose between two thread programs while a 
synchronisation can only appear in an Obsidian block program. More information about programs 
and the GPU hierarchy is available in section~\ref{sec:OBSIMP}.

%f it is safe or 
%not to call some function at a certain place in your code, can only be decided by inspecting 
%that functions implementation. 
\end{itemize}  

The list above provides some very general guidelines on what to think about when 
writing GPU kernels. The lesson is that fast memory near the processing element 
should be preferred over slow memory far away. However, this comes at the effort 
of decomposing computation and data in a way that makes this possible. 

% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
%
% Obsidian Implementation 
%
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
\subsection{Obsidian Implementation}
\label{sec:OBSIMP}

Obsidian is implemented as an embedded language using Haskell as host. Obsidian 
is a compiled embedded language; a very good paper that is highly relevant to our 
approach, is ``Compiling Embedded Languages'' by Conal Elliott et al~\citet{COMPILEEDSL}. 
This paper describes how to embed a compiler backend and generate efficient code 
in some target language. One key aspect of this approach is that the embedded 
language is a library of functions that create and compose Abstract Syntax Trees 
(ASTs). Obsidian (chapter \ref{chap:GPUProgramming}) as well as EmbArBB 
(chapter \ref{chap:ArBB}) are implemented in a similar way. An embedding that builds 
ASTs is called a deep embedding. Shallow embeddings, on the other hand, do not build 
ASTs. In ``Combining Deep and Shallow Embedding for EDSL'' Josef Svenningsson and 
Emil Axelsson combine the two methods, shallow and deep, in order to simplify the 
AST data type (fewer constructors) and make more extensible embedded languages 
\citet{DEEPSHALLOW}. Obsidian also uses a combination of shallow 
and deep embeddings. 

The embedded language approach has become popular to use as a way of raising the 
level of abstraction in fields where the default is to use low-level languages. 
For example, Lava and Wired~\citet{LAVA,Wired} raises the level of abstraction in the 
fields of hardware design and verification and Feldspar does the same in the area of  
digital signal processing~\citet{FELDSPAR2010}. For GPU programming, there are numerous 
embedded approaches using various host languages. Two languages for GPU programming 
embedded in Haskell are  Accelerate and Nikola \citet{ACCELERATEDAMP11, NIKOLA}. 
And there is Intel ArBB, embedded in C++, with the goal of being parallel across 
platforms \citet{ARBB2011}. Related work and information about the tools, libraries 
and language that make the wider context surrounding Obsidian can be found in 
section~\ref{sec:relatedwork}. 

\subsubsection{Deep embedding: The {\tt Program} data type}

Obsidian uses a deeply embedded {\tt Program} data type that represents GPU Kernels. 
A CUDA GPU has a hierarchy of parallel resources. At the bottom there are threads, each 
executing sequential programs. There are groups of threads (of 32) called {\em Warps} that 
execute in lock-step. Warps are the scheduled unit of work on a GPU. There are {\em Blocks} 
of threads, a set of threads that run as a group and share local (shared) memory. And 
lastly there is a {\em Grid} of blocks that specifies the total number of threads involved 
in a computation (Number\_of\_Blocks * Number\_of\_Threads). The deeply embedded program 
data type of Obsidian models this hierarchy by parameterising programs on a hierarchy 
level type parameter (the  {\tt t} parameter below).

\begin{verbatim} 
data Program t a where
\end{verbatim}

The {\tt t} parameter can be either {\tt Thread}, {\tt Warp}, {\tt Block} or {\tt Grid}.
These types are related to each other via a {\tt Step} type constructor that represents 
going upward one step in the hierarchy. 

\begin{verbatim} 
data Step a -- A step in the hierarchy
data Zero
\end{verbatim} 

The {\tt Thread} type is level {\tt Zero} ({\tt type Thread = Zero}). Then {\tt Block} is 
{\tt Step Thread} and {\tt Grid} is {\tt Step Block}. Currently {\tt Warp} is not included 
in the hierarchy leading to warp programming being a special case. One benefit a warp has is 
that threads can communicate via shared memory without using synchronisation primitives. There 
is no programmer support for warp level programming in CUDA (as there is for block level 
programs). The programmer needs to take care to ensure that the communication patterns are 
within a warp and is then free to remove synchronisations. If warps were to be put in the 
program level hierarchy of Obsidian, its place would be between the thread and block level. 
This would force the programmer to go via warps when putting together a block computation even 
if the communication pattern is not suited for that, leading to inconvenience with no gain. 

As an example of how the hierarchy level parameter is used, I show the {\tt ForAll} constructor 
from the {\tt Program} data type.  This constructor represents parallelism either over threads 
or blocks. 

\begin{verbatim}
ForAll :: EWord32 
            -> (EWord32 -> Program t ())
            -> Program (Step t) ()
\end{verbatim}

{\tt ForAll} takes a number of parallel iterations, and a body represented by a function 
from an index to a program. The body is a {\tt Program} at some level {\tt t} while the 
resulting program is a step above. This means that a thread program can be turned into 
a block program by using {\tt ForAll}, or that a block program can be turned into a grid program. 

Information about the {\tt Program} data type can be found in paper F, \paperFTitle, in 
section \ref{sec:paperF}. 

\subsubsection{Scalars} 

Scalars and operations on scalars are represented by an expression data type ({\tt Exp a}) in 
Obsidian. This is another example of a deep embedding; the expression data type contains 
constructors for literal values, arithmetic operations, and conditionals. 

Haskell's type class system allows us to overload arithmetic operations such as {\tt (+)}, 
{\tt (-)} and {\tt (*)} by making expressions an instance of {\tt Num}. This allows the 
Obsidian arithmetic expressions to look exactly like corresponding native Haskell arithmetic. 

The approach taken to embed the scalar language is very similar to what is described 
in ``Compiling Embedded Languages''~\citet{COMPILEEDSL}; but we use a Generalised Algebraic 
Datatype (GADT) to obtain typed expressions. An alternative to the GADT, is to use phantom 
types. Earlier versions of Obsidian (section~\ref{sec:paperB}) used phantom types. 


\subsubsection{Arrays} 
\label{sec:OBSARRAYS}

Obsidian has two array representations. These array representations are implemented as 
shallow embeddings on top of the expression and program data types. This means that these 
arrays will disappear during Haskell evaluation of the Obsidian program.
 
\paragraph{Pull arrays:}

Pull arrays have been part of Obsidian from the very beginning~\citet{JMT} but called either 
{\tt Array} or just {\tt Arr}. A pull array represents an array as an indexing function; given 
an index it provides an element: 

\begin{verbatim} 
data Pull s a = Pull {pullLen :: s, 
                      pullFun :: EWord32 -> a}
\end{verbatim} 

Pull arrays are parameterised on both element ({\tt a}) and length ({\tt s}) type. The length 
parameter is used to be able to represent arrays with both static (known at Haskell runtime) 
or dynamic (length represented by an expression, an unknown length). When creating block 
level computations, we want to know the exact lengths in order to avoid generating code 
riddled with conditionals concerning array lengths. It is possible to write kernels that 
are not specialised for a certain size. This is accomplished by adding bounds checks. Since 
conditionals  are problematic from a performance point of view, fixed size kernels are recommended 
when performance is crucial. Dynamic lengths are used for grid level computations, this allows 
a local computation of a fixed size to be run in parallel over arrays a multiple of the size that 
the local computation handles. In generated code this is currently visible as a conditional 
on {\tt blockIdx.x}. However, this is still considered work in progress; the conditional is 
only really necessary for Obsidian kernels that have more than one output array and the outputs
are of different length. 
%Conditionals are problematic on GPUs and 
%while general programs that run for any size is possible when performance is crucial a fixed 
%size is recommended. 

One benefit of pull arrays is that they give fusion of all operations on them for free. 
For example, {\tt map f} is implemented on pull arrays by composing {\tt f} with the indexing function: 

\begin{verbatim} 
map f (Pull n ixf) = Pull n (f . ixf)
\end{verbatim}

Now, we see that \verb!map f . map g! is the same as \verb!map (f . g)! since both result in {\tt f}
and {\tt g} becoming composed onto the indexing function. No intermediate array is built in 
memory.  

Another benefit of pull arrays are that they are parallel. Simply put, early Obsidian was pull 
arrays in addition to a way to compute the array and store its elements to memory 
(called {\tt sync} in early versions of Obsidian and later {\tt force}). A very 
direct way to compile a pull array to a GPU is to launch as many threads as there are 
elements in the array and apply the indexing function to the thread id. Now each thread 
will have computed an element of the array and can store that to memory. 

 
\paragraph{Push arrays:}

Push arrays were added to Obsidian as a complement to pull arrays. Pull arrays are 
good because they are so simple to understand and easy to parallelise. However, certain
operations on pull arrays yield code that is not suitable for GPU execution. Concatenation 
or interleaving of pull arrays result in conditionals in the indexing function. The 
interleaving case, which is worst, leads to diverging branching in every warp. If branches 
diverge, the GPU will turn off the processing elements taking one path and 
allow the others to progress; then turning to the other branch. The computation of the 
branches is serialised and compute resources are wasted. 

Push arrays, like pull arrays, are functions. A Push array is a higher order 
function, taking a function from value and index to a thread program and giving 
as result a program at some level in the hierarchy. The function passed to the 
push array function we call a write-function. Below is the definition of 
push arrays that we use in Obsidian. Push arrays are parameterised on element 
type ({\tt a}), size type ({\tt s}) and program hierarchy level ({\tt p}). 

\begin{verbatim}
data Push p s a =
  Push s ((a -> EWord32 -> TProgram ()) -> Program p ())
\end{verbatim} 

When working with pull arrays, the consumer of such an array decides how to 
iterate over the array and what elements to compute. This is done applying the 
push array function to those indices that are interesting to the consumer. Push 
arrays work in the opposite way. The Push array function (the result program) contain 
the iteration schema; the consumer of the push only decides what write function 
to supply. The iteration schema used is decided upon when creating a push arrays. 
The code below is an example of how to create a push array from a pull array.

\begin{verbatim} 
convertToPush :: Pull Word32 e 
               -> Push Block Word32 e 
convertToPush (Pull n ixf) =
    Push n $ \wf -> 
      ForAll (fromIntegral n) $ \i -> wf (ixf i) i
\end{verbatim} 

The function above takes a pull array and creates a push array with a parallel 
iteration schema over threads within a block. There are other iteration schemas to 
chose from such as sequential, combinations of sequential and parallel
across both threads and blocks (two level parallelism). 

Push arrays give many of the same benefits that pull arrays have. Using push 
arrays we also get fusion of operations as default. The implementation of map 
(shown below) illustrates how mapping a function {\tt f} over a push array results 
in an application of {\tt f} in the write-function. 

\begin{verbatim} 
map f (Push s p) = Push s $ \wf -> p (\e ix -> wf (f e) ix)
\end{verbatim}

\subsubsection{Compilation to CUDA}

When compiling Obsidian programs to CUDA, we only need to be concerned with the 
{\tt Program} datatype. Given the hierarchy level type parameters to the program datatype, 
we know that the AST can be quite directly translated to CUDA. We know that there will 
be at most one level of nestedness in the parallelism (several blocks, each of several threads). 
This means that we do not need to do any transformations on the AST before generating CUDA code. 
Paper F (section~\ref{sec:paperF}) contains technical details of the compilation process. 

\subsection{Results}
\FloatBarrier 

We have implemented basic algorithms such as sorting, merging, fractal generation, reductions 
and parallel prefix (scans). Often we have managed to get very good performance, close to that 
of hand optimised CUDA code. However, there are also cases where we just cannot seem to reach 
the level of performance that we would want. One such example is parallel prefix, an 
application that has been haunting us from the very beginning. Scan is problematic to us because 
it is best implemented as an in-place algorithm, while our array representations do 
not allow for in-place update; there is no memory associated with them to update. 
We are trying to improve this situation by adding a third array representation to Obsidian,
mutable arrays.   

Paper F (section~\ref{sec:paperF}) tells a story about how to implement and tune 
a reduction kernel in Obsidian. At the end, the implemented reduction kernel is  
faster than the reduction skeleton used in Accelerate. This is quite a feat, since 
reduction in Accelerate corresponds to just one single hand optimised skeleton. We would have 
been happy just getting close.   

In paper E (section~\ref{sec:paperE}), we implement counting sort and occurrence sort 
using Obsidian. We use very low-level features of Obsidian and end up producing 
code that performs very well. 

Paper D (section~\ref{sec:paperD}) introduces push arrays and uses them in the 
implementation of sorting networks to quite good results. With push arrays we 
were able to experiment with more detailed decompositions (what thread computes 
what value) of the algorithms. 

In section \ref{sec:efficient}, guidelines for writing efficient CUDA code is listed. 
Here we revisit these guidelines and explain how they are approached by Obsidian and 
the Obsidian programmer. 

\FloatBarrier 
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
%
% ArBB work
%
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
\section{Embedding Intel Array Building Blocks} 

Intel ArBB~\citet{ARBB2011} is a system for high-level data-parallel programming with the 
ability to generate code for a variety of different hardware configurations. It is implemented 
as an embedded language in C++. One motivation for ArBB that is mentioned in~\citet{ARBB2011} is 
that while high-performance computing specialists are highly competent at implementing 
kernels using low-level programming techniques, mainstream developers are not. ArBB offers 
a more composable way to write programs that make use of core and vector (SIMD) parallelism and 
doing this while using a familiar language, C++.  

One of the greatest strengths of ArBB, as I see it, is that it also comes with a low-level C 
interface. The main purpose of this C interface is to make it easier to use the ArBB system 
from other languages. Many languages have foreign function interfaces that are geared towards C.
This gives ArBB a level of language independence on top of its cross platform abilities.  


ArBB is based on a set of parallel primitives (or structures) on dense one, two and three dimensional vectors and nested vectors: 
\begin{itemize} 
\item {\bf Reductions:}   {\tt add\_reduce}, {\tt mul\_reduce}, {\tt max\_reduce} ... 
\item {\bf Scans:}        {\tt add\_scan}, {\tt mul\_scan}, {\tt max\_scan} ... 
\item {\bf Sorting:}      {\tt sort} 
\item {\bf Permutations:} {\tt gather}, {\tt scatter}, {\tt reverse} ...
\item {\bf Sequential loops:} {\tt \_for}, {\tt \_while} 
\item {\bf Map, zipWith, stencil:}  {\tt map} 
\end{itemize}

These operations (and many more, there are about 120 operations in total) can be used 
by the programmer when writing programs using ArBB. Note that there are sequential looping 
constructs with special names, {\tt \_for} and {\tt \_while}. ArBB is a deeply embedded 
language and calling the ArBB functions does not actually execute them, it just ads a node to 
some internal representation. Therefore, the use of normal C++ for loops leads to unrolled 
ArBB programs and a special {\tt \_for} loop is needed to get a loop in the generated program. 
This is exactly the way these things work in Haskell embedded languages, but it may be even 
more surprising to a C++ programmer. Before an ArBB function can be computed it needs to be 
captured. For this, ArBB has a {\tt call} function that is used to run functions using ArBB 
functionality. The first time {\tt call} is used on a function pointer that function is 
executed and the ArBB functions used in it build an AST (we say the function has been 
captured). The AST is then compiled and specialised for the hardware available. Any 
subsequent calls of a function using ArBB functionality do not lead to a recompilation, 
the first call caches this compiled code. 

The first step of our work with embedding Intel ArBB functionality was to implement very direct 
and low-level bindings to the C interface. This provides a Haskell function (in the IO Monad) 
for each of the functions in the ArBB C interface. More details and results of this work 
can be found in paper G in section~\ref{sec:paperG}.

%\noindent\emph{What does ArBB do, what are the operations and datastructures it support} 

%\noindent\emph{What does EmbArBB do} 

\subsection{EmbArBB: Embedding ArBB in Haskell}
\label{sec:EmbArBB} 

The ArBB Haskell bindings provide a very rudimentary interface to ArBB functionality and 
is not useful for application implementation. A higher-level interface to ArBB to offer 
the Haskell programmer is needed. Our first attempt was to implement an Accelerate backend 
using the ArBB bindings. However, this had some hard problems to solve. Accelerate is a 
more richer language and there was an API mismatch between Accelerate and ArBB. These problems 
are mentioned in in paper G, section~\ref{sec:paperG}. As a way to offer ArBB functionality 
to the Haskell in a way that is useful and simpler to implement, we start working on EmbArBB. 

With EmbArBB we improve the interfacing with Haskell by implementing a deeply embedded language. 
Now, programs in EmbArBB create ASTs. These ASTs are compiled via the ArBB bindings creating 
function objects that can then be run. 

\subsubsection{Dense and nested arrays}

ArBB supports one, two and three dimensional arrays, called dense vectors. In EmbArBB we 
represent these with a data type, {\tt DVector}, parameterised by dimensionality and 
element type. The dimension parameter can be either {\tt Dim1}, {\tt Dim2} or {\tt Dim3}. 
There are also nested vectors, in EmbArBB called {\tt NVector}, parameterised on element type. 

\subsubsection{Operations on arrays} 

EmbArBB provides many of the ArBB operations on vectors but provides a 
shape-polymorphic interface to them. For example, a reduction operation can take a two  
dimensional vector and reduce all rows or columns and provide a one dimensional result. Or 
it takes a one dimensional vector and returns a single value (zero dimensional vector). 

\begin{verbatim} 
addReduce :: Num a 
           => Exp USize 
           -> Exp (DVector (t:.Int) a) 
           -> Exp (DVector t a) 
\end{verbatim} 

The {\tt addReduce} function takes an input vector that is at least one dimensional 
({\tt t:.Int}) and returns a vector that has one less dimension ({\tt t}).
 
\subsubsection{Deep embedding}

EmbArBB is a traditional deep embedding; there is an AST data type with constructors 
representing each ArBB language feature. It is true that a deep embedding of ArBB 
leads to duplication of effort. The low-level operations exported by the 
ArBB C interface already construct an AST within the ArBB system. One possible benefit 
of the deep embedding is that it buys independence from the ArBB backend. It is possible 
to compile the AST generated on the Haskell side to some other target language. 

When compiling EmbArBB to ArBB a sharing detection pass is performed. This pass detects 
sharing in the AST using the method of A. Gill~\citet{Gill}. One benefit of this pass 
is that it reduces the numbers of calls into the ArBB C library. The efficiency of code 
generated is probably not affected since ArBB would discover that sharing in a Common 
Subexpression Elimination (CSE) pass. Another potential benefit is that sharing detection 
on the Haskell side results in a simpler AST being built on the ArBB side; potentially making 
the job for the ArBB optimisation passes simpler. What effects these 
operations truly had on performance has not been investigated. Actually, applying a sharing 
detection technique was one of my personal motivations for working on EmbArBB. In Obsidian 
we have not seen the need for this technique.

One benefit of a deep embedding is that transformations can be performed on the AST. Applying 
any optimisations in EmbArBB would very likely duplicate effort already put into ArBB. 
Therefore we apply no further transformations after sharing detection. However, if EmbArBB 
would be used to generate code in some other target language a set of optimisations 
would be needed.  

\subsubsection{Evaluation} 

In paper H, section~\ref{sec:paperH}, we benchmark EmbArBB against ArBB in C++ and the Haskell 
Repa library. EmbArBB compares favourably to Repa and is often a lot faster. EmbArBB and the ArBB 
C++ performance is not distinguishable from EmbArBB's indicating that our Haskell embedding 
adds no extra overhead. Of course this is expected, since once the ArBB system has generated 
the code, that code is identical no matter if the AST was built via Haskell or C++ calls. 
Missing is a comparison of how well EmbArBB and C++ compares in regards to the time it actually 
takes to go through the process of making those ArBB C API calls. However, if the generated 
code is used many times the caching ArBB does of generated code means that the initial code 
generation cost can be amortised. 

\subsection{Current status of ArBB and EmbArBB} 

Since our work with ArBB, Intel has unfortunately retired the ArBB system. This 
leaves EmbArBB without a functioning code generating backend. This makes the choice of a
deep embedding (in hindsight) the correct one. EmbArBB could be resurrected by implementing 
a new code generating backend. 


% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
%
% Related work
%
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
\section{Related work} 
\label{sec:relatedwork}
\FloatBarrier


\subsection{Languages for data-parallel programming}

In figure~\ref{fig:researchmatrix}, our work is placed in relation to other languages,
embedded languages and libraries for parallel programming. The languages and libraries considered 
are data-parallel; languages for control parallelism or distributed message passing systems are 
not considered. The systems are roughly divided 
into three groups, based on their level of abstraction. There are low-level languages; here 
I place languages that are imperative and C-like. In the middle layer I place languages 
that have higher level abstractions and are more easily composable. 
at the highest level, I place languages that completely abstract away from details of the 
hardware they run on. The division is not free from personal bias and the placement of languages
in boxes was not always easy. 

The figure also divides the languages according to what hardware they support. There 
are CPU specific languages, GPU specific languages and languages that support or aspire 
to support CPU,GPU and accelerator (Larrabee, Xeon Phi) execution. 

% ---------------------------------------------------------------------------
% Obsidian, Accelerate, Nikola, Thrust, CUB 
% ---------------------------------------------------------------------------
With Obsidian, we try to target the sparsely occupied area of mid-level GPU programming. With 
CUDA~\citet{wwwcuda} and OpenCL~\citet{OpenCL}, the programmer has full control of how to divide 
the computation amongst threads and blocks. Using Accelerate~\citet{ACCELERATEDAMP11}, 
Nikola~\citet{NIKOLA} 
and Thrust~\citet{THRUST} the programmer would use 
high-level patterns without direct insight into how the application would be mapped onto 
threads and blocks of the GPU. Using Obsidian, we want to bridge that gap by both being 
more composable than CUDA and giving the programmer control of what the computation will 
look like on the GPU. This is done by allowing the programmer to specify and compose 
thread-level and block-level code. Recently, the CUB library arrived that is built on a 
similar idea, that GPU programmers need to have block/thread level control to get maximum 
performance, while maintaining a higher level and more Thrust-like programmer 
interface~\citet{CUB}. Before CUB, and as far as I am aware, we were alone at targeting 
this level.  

\begin{figure}
\begin{center}
\begin{tikzpicture}[align=center, scale=2.5]
% \draw[help lines] (0,0) grid (3,3);


\draw[->] (-0.5,0) -- (-0.5,3) node[anchor=east] {Abstraction level};

\draw (0,3) node[anchor=south west] {CPUs};
\draw (1,3) node[anchor=south west] {GPUs};
%\draw (2,3) node[anchor=south west] {Accelerator};
\draw (2,3) node[anchor=south west] {CPUs, GPUs\\ Accelerators};

\node[anchor=west] (acc) at (1,2.7) {Accelerate\\Nikola\\Thrust};
\node[anchor=west] (acc) at (1,1.7) {CUB};
\node[anchor=west] (repa) at (0,2.5) {DPH\\Nesl\\Repa\\Meta-repa\\Feldspar};
\node[anchor=west] (mixed) at (0,1.7) {Cilk+\\ TBB};
\node[anchor=west] (accelerator) at (2,1.7) {Accelerator};

\node[anchor=west] (cuda) at (1,0.3) {CUDA};
\node[anchor=west] (opencl) at (2,0.3) {OpenCL};

\node[anchor=west] (OpenMP) at (0,0.3) {OpenMP\\PThreads};

\foreach \x in {0,...,2} { 
  \foreach \y in {0,...,2} { 
    \draw (\x,\y) rectangle (\x+1,\y+1);
  }
}

\draw[red!75,rounded corners,ultra thick] (1,0.4) rectangle (2,2.4);
\node[red!75,rotate=45,thick] (obs) at (1.5,1.5) {Obsidian};

\draw[orange!90,rounded corners,ultra thick] (2,1.4) rectangle (3,2.6);
\node[orange!90,thick] (arbb) at (2.5,2) {EmbArBB\\ArBB};


\end{tikzpicture}
\end{center}
\caption{Placing our work in the landscape of languages and libraries for parallel 
general purpose programming of CPUs, GPUs or both. }
\label{fig:researchmatrix} 
\end{figure}


% ---------------------------------------------------------------------------
% ArBB EmbArBB
% ---------------------------------------------------------------------------
The ArBB and EmbArBB systems also provide a set of parallel primitives that the programmer
composes to build his application. I place the ArBB and EmbArBB in a box slightly lower than 
Accelerate since while ArBB/EmbArBB also have built in reduction primitives, just like Accelerate, 
they are not general primitives. Where Accelerate has a single higher order reduction 
operation, ArBB/EmbArBB has \\ {\tt reduce\_add, reduce\_mul} and so on. Benefits of 
having ArBB embedded in Haskell compared to C++ are discussed in section~\ref{sec:EmbArBB}.
%I will go into 
%what benefits there are to having ArBB embedded in Haskell compared to C++ in 

% ---------------------------------------------------------------------------
% Microsoft Accelerator
% ---------------------------------------------------------------------------
Microsoft Accelerator~\citet{ACCELERATOR}, is a language embedded in C\#. Accelerator has
data-parallel arrays and a set of aggregate operations (operations on entire arrays). The 
operations on data-parallel arrays are JIT-compiled to GPU code. Accelerator can also 
compile to multithreaded CPU code and make use of SIMD units of modern 
CPUs~\citet{ACCELERATORCPU}; this makes the Accelerator system related to ArBB. 

% ---------------------------------------------------------------------------
% Python systems
% ---------------------------------------------------------------------------
There are many embedded languages implemented in Python that makes use of GPUs 
to improve performance. One example is Copperhead~\citet{copperhead}. Copperhead 
supports flat and nested data-parallelism via primitives such as {\tt map} and 
{\tt reduce}.  

% ---------------------------------------------------------------------------
% Repa, Meta-Repa 
% --------------------------------------------------------------------------- 

Repa (Regular Shape-polymorphic Arrays)~\citet{REPA}, is a Haskell library for 
data-parallel programming. 
Repa uses the same array representation as Obsidian, in Repa called a delayed
array, which is a function from index to element. However, in Repa arrays have shape; 
they can be of any dimensionality and there are shape-polymorphic functions on these. 
For example, {\tt map} can be applied to an array of any shape. 

Meta-Repa~\citet{METAREPA} is a reimplementation of the Repa library using deep embedded language 
techniques. Meta-Repa programs build ASTs that are then compiled (using template 
Haskell) to Haskell code using low-level parts of the Repa library for its parallelism.
The Meta-Repa approach to implementing the Repa library 
gives inlining of operations for free, where Repa programs need to be annotated 
with pragmas to ensure inlining. On the other hand, in Meta-Repa the programmer 
needs to explicitly state when something should not be inlined (using a {\tt force} primitive).
This is related to our work on EmbArBB that also uses an existing library (ArBB) 
for its parallelism. In reference~\citet{FPCDSL}, a case is made for this style 
of embedding preexisting DSLs. 


% ---------------------------------------------------------------------------
% DPH, NESL 
% ---------------------------------------------------------------------------
Data-Parallel Haskell~\citet{DPH} and Nesl~\citet{NESL} are languages for 
nested data-parallelism. Both run on CPUs, but there is also work towards 
a GPU version of Nesl~\citet{NestedGPU}. 

% ---------------------------------------------------------------------------
% Delite, Vertigo 
% ---------------------------------------------------------------------------

Delite~\citet{DELITE} is an infrastructure for implementation of domain 
specific embedded languages. There is an extensible internal representation (IR,AST); 
the programmer extends some base IR with domain specific constructs. The Delite system 
has been used to implement domain specific embedded languages for machine learning, rendering, 
physics and other domains. Delite, being a framework and the languages implemented using it 
very domains specific, was impossible to place in the matrix (figure~\ref{fig:researchmatrix}). 

\subsection{Embedded domain specific languages} 

Feldspar is an embedded language for digital signal processors~\citet{FELDSPAR}. Apart 
from using the syntactic library~\citet{SyntacticICFP12}, it is quite similar to Obsidian. 
There is a deeply embedded core language (called Core) and vectors as a shallow embedding 
on top of that, just like in Obsidian.

When we first started working on Obsidian, we used Lava~\citet{LavaICFP} as inspiration. 
We wanted to mimic the Lava programming style but generate GPU code. At first this worked quite 
well and most Obsidian programs superficially looked quite like Lava programs. We have diverged 
from this aspiration since realising that GPUs are quite a different story and performance 
requires more control of GPU architecture specifics. 

\subsection{Compiling Embedded Languages}

In paper A~(section \ref{sec:paperA}) we describe our method for compiling monadic embedded 
languages. 
This means to take a monadic computation and observe its structure in order to generate
code. This work is closely related to a method described in ``The Constrained Monad Problem''
by Sculthorpe and Gill~\citet{sculthorpe2013constrained}. In that paper they solve a more 
general problem than just compilation, but our method is nice because it is easy to understand 
and solves a single problem and does it well. In ``Generic monad constructs'' Persson et al 
also solves the problem of compiling monads using a continuation monad. 

``Compiling Embedded Languages'' by Elliott and Finne has been a constant source of information 
and something we have referred to in almost all our work. This is, in my opinion, the paper to turn 
to for anyone starting out with compiled embedded languages. 

%\subsection{Parallelism} 

%\noindent\emph{DPH}\citet{DPH} \newline
%\noindent\emph{D.A.Accelerate}\citet{ACCELERATEDAMP11} \newline
%\noindent\emph{Nikola}\citet{NIKOLA}\newline
%\noindent\emph{Repa}\citet{REPA}\newline
%\noindent\emph{Meta-Repa}\citet{METAREPA}\newline
%\noindent\emph{ArBB}\citet{ARBB2011}\newline
%\noindent\emph{OpenCL}\citet{OpenCL}\newline
%\noindent\emph{CUDA}\citet{CUDA}\newline
%\noindent\emph{Microsoft Accelerator}\citet{ACCELERATOR}\newline
%\noindent\emph{Copperhead}\citet{copperhead}\newline
%\noindent\emph{Delite}\citet{DELITE}\newline
%\noindent\emph{A monad for deterministic parallelism (R. Newton)} \citet{MonadPar} \newline
%\noindent\emph{Nesl}\citet{NESL} \newline

%\subsection{Embedded domain specific languages} 



\FloatBarrier
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
%
% Future work
%
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
\section{Future work} 

Obsidian is a language for GPU kernel implementation, not complete applications. 
It is not as well integrated into Haskell as for example Accelerate. There are 
ways to execute Obsidian kernels from within Haskell (we have seen examples of this in 
section~\ref{sec:GPUPROGRAMMING}); the recommended work flow, 
however, is to generate the CUDA as text and write the host code by hand. This 
can be seen as both a shortcoming and a feature. It is hard to integrate 
Accelerate programs in larger applications written in other languages; when 
using Obsidian a kernel is generated and can be used as part of for example a library. 

Obsidian is also a quite low-level language. The programmer needs to know GPU details 
and is allowed to write programs at a level almost as low as CUDA, if it is desired. 
Paper E (section~\ref{sec:paperE}) contains examples of programming at this low level. 
One potential use of Obsidian could be as the code generating backend of high-level 
and very domain specific embedded languages. This could be a niche for Obsidian.  

Warp level computations is work in progress and that these are not as well integrated 
into the hierarchy as the block, thread and grid levels are. The warp abstractions 
offered also requires more work in the code generating backend than the others, 
that correspond directly to CUDA concepts. The problem is that CUDA does not have 
a warp abstraction. Warps needs to be better integrated with the rest of Obsidian 
and the code generation required for warps needs more testing before being 
trustworthy and useful. When operational, warp level abstractions will help 
the programmer make better use of the GPU. 

%\noindent\emph{Improve on Warp level capabilities} 

%\noindent\emph{Improve expressivity, code performance} 

% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
%
% Discussion
%
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------

\section{Discussion} 

%\vspace{5mm} 

% \emph{This section will revised.} 

%\vspace{5mm}

This thesis contains two approaches to embedding a language for data-parallel programming 
in Haskell. EmbArBB embeds a preexisting library for data-parallelism in Haskell 
and at little effort from us, very good performance is gained. The performance of 
EmbArBB is entirely due to all the optimisation expertise and effort that was invested in 
the ArBB system. We get all that expertise for free. What EmbArBB provides is a 
way for Haskellers to benefit from the ArBB system. ArBB and EmbArBB are based on a 
set of parallel primitives for the programmer to use and compose. Obsidian takes 
a different approach. Built in primitives are very few and very low level, such as 
sequential and parallel for loops. On top of the low-level capabilities, a library 
of high-level primitives can be built. The key is that we want the programmer to 
be able to specify how to implement these higher level abstractions. 

Looking at the related work, we see that almost all high level GPU programming 
languages provide abstractions of parallel primitives, aggregate operations, 
maps, folds and reductions, giving justification to our investigation of GPU 
programming without these. We believe that the GPU architecture is still volatile 
enough that there be no ``the right'' way of implementing these primitives. As of 
today, the programmer still needs to explore the design space and find a solution 
that performs well on a particular GPU. 

Performance of Obsidian kernels is often good and the Obsidian code that they 
are generated from is often shorter than corresponding CUDA programs. Obsidian 
programs differ from CUDA programs by describing whole programs. With this I mean 
that a CUDA programmer needs to take a leap from the single threaded program 
parameterised on thread ID, to what actually will happen when this program is 
run by hundreds or thousands threads at once. The extensive indexing arithmetic 
often present in CUDA programs can make this leap a hard one. Similar programs 
in Obsidian could use operations on whole arrays such as {\tt map} or explicitly 
bounded parallel loops. 

Push arrays were invented by Koen Claessen and first implemented in Obsidian 
to provide more control over where computation takes place (which thread computes 
which element). Push arrays has since been experimented on by others and show up 
in Nikola~\citet{NIKOLAPUSH} and Feldspar~\citet{FELDSPARPUSH}. Push arrays appear 
in StreamHS~\citet{FPCDSL} and in ~\citet{MOA} as part of a multi-dimensional array 
calculus in Standard ML. 


% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
%
% Thesis overview
%
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
\section{Thesis overview} 

%This thesis contains an introduction and a selection of papers. The introduction 
%gives background and context to the papers. 

The papers contained in this thesis fall in three categories. 
\begin{itemize} 
\item General EDSL methodology in chapter~\ref{chap:EDSLImplementation}. 
\item GPU Programming using an EDSL in chapter~\ref{chap:GPUProgramming}. 
\item Retargetable parallel programming in chapter~\ref{chap:ArBB}. 
\end{itemize} 
%In the following sections, each paper is given a short description. Hopefully 
%there is enough information to wet the appetite. 

\subsection{EDSL implementation papers} 

\subsubsection{\paperA: \\ \paperATitle} 

Authors: Josef Svenningsson and Bo Joel Svensson 

\vspace{5mm}

\noindent This paper describes a simple and compositional method for 
reification of monads. This is useful for compilation of 
monadic embedded languages. We use a simple robot control language 
as the example. Robot programs can be expressed using Haskell do notation 
and is compiled down to a simple first order program representation. 
There is also a graphical simulator available that runs the compiled 
code. Robot language and simulator is available at github: \url{github.com/svenssonjoel/Robot}.

\subsection{GPU programming papers} 

\subsubsection{\paperB: \\ \paperBTitle}

Authors: Bo Joel Svensson, Mary Sheeran and Koen Claessen \newline

\vspace{5mm}

The first real paper about Obsidian, our embedded language for GPU 
programming. In this paper we identify similarities between 
connection patterns in hardware and parallel computations on GPUs. 
We introduce a structured and compositional way to write data-parallel 
programs on a GPU.

Basic parallel building blocks are implemented in a monadic style and 
composed using a sequential composition operator {\tt ->-}. 

\subsubsection{\paperC: \\ \paperCTitle}

Authors: Bo Joel Svensson, Koen Claessen and Mary Sheeran \newline

\vspace{5mm}

In this paper we describe a version of Obsidian implemented with a 
a different internal representation of GPU programs. We realised that 
most of our kernels have similar shape, they are parallel operations 
composed in sequence interspersed with barrier synchronisations. The 
internal representation used here captures this case exactly. Programming 
is now done is a style resembling arrows. 


\subsubsection{\paperD: \\ \paperDTitle}

Authors: Koen Claessen, Mary Sheeran and  Bo Joel Svensson \newline

\vspace{5mm}

This paper introduces {\em Push} arrays, invented by Koen Claessen. This 
array representation target specific performance problems that we want 
to solve with Obsidian. Here we have also gone back to using monadic 
style for programming in Obsidian. 


\subsubsection{\paperE: \\ \paperETitle} 

Authors: Josef Svenningsson, Bo Joel Svensson and Mary Sheeran \newline

\vspace{5mm}

This paper contains a sorting case study performed in Obsidian. It 
is however the algorithms that are in focus here. We describe 
an interesting variation on Counting sort that removes duplicate 
elements and is a nice fit for GPUs because of its little need for 
synchronisation. 

In order to implement these sorting algorithms we added atomic operations 
to Obsidian. 

\subsubsection{\paperF: \\ \paperFTitle}

Authors: Bo Joel Svensson and Mary Sheeran \newline
\noindent \emph{This work has not yet been published.}
\vspace{5mm} 

\noindent 

In this paper the most recent additions and improvements of Obsidian are 
described. Obsidian Programs are now parameterised on Hierarchy level. This forces
the programmer into writing programs that we know how to compile to a GPU easily 
(by limiting how parallel operations can be nested). The paper also gives a detailed  
optimisation story using Obsidian for implementing reduction kernels. 


\subsection{Retargetable parallel programming} 

\subsubsection{\paperG: \\ \paperGTitle}

Authors: Bo Joel Svensson and Ryan R. Newton

\vspace{5mm}

\noindent This position paper describes the work I did while on a 3 month internship at 
Intel. The main part of this work was implementation of Haskell bindings 
for the now retired Intel ArBB system. 

The paper also describes work we did towards implementing a backend for the 
Accelerate system using ArBB. We reached a proof-of-concept implementation 
capable of running some simple Accelerate programs using the ArBB backend.
However, much work would remain to be able to run the full scope of Accelerate 
programs on ArBB.  

\subsubsection{\paperH: \\ \paperHTitle}

Authors: Bo Joel Svensson and Mary Sheeran 

\vspace{5mm}

\noindent Here we explore another way to provide the capabilities of ArBB to the 
Haskell programmer. Rather than trying to use ArBB as a backend to Accelerate 
we provide a more direct mapping of ArBB's functionality to Haskell idioms. 
We call this embedded language EmbArBB and it provides a more Haskell-programmer-friendly 
interface compared to the raw ArBB bindings (which are a collection of functions 
in the IO monad). 

% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
%
% BIBLIOGRAPHY
%
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------

\bibliographystylet{alpha}
\bibliographyt{thesis}
%\addcontentsline{toc}{section}{Bibliography}




\clearpage{}

% --------------------------------------------------------------------------- %
% --------------------------------------------------------------------------- %
%
%  PAPERS 
%
% --------------------------------------------------------------------------- %
% --------------------------------------------------------------------------- %


\chapter{EDSL Implementation Papers}
\label{chap:EDSLImplementation}
% ---------------------------------------------------------------------------
% 
% ---------------------------------------------------------------------------
\cleardoublepage 


\section[\paperATitle]{\paperA: \\ \paperATitle}
\label{sec:paperA}
%\addcontentsline{toc}{chapter}{Simple and Compositional Reification of Monadic Embedded Languages}

% \paperATitle

\begin{center} 
Josef Svenningsson, Bo Joel Svensson
\end{center}


\input{./bb/paperThesis}



\chapter{GPU Programming Papers}
\label{chap:GPUProgramming}

% ---------------------------------------------------------------------------
% 
% ---------------------------------------------------------------------------
\cleardoublepage 

\section[\paperBTitle]{\paperB: \\ \paperBTitle}
\label{sec:paperB}
%\addcontentsline{toc}{chapter}{Obsidian: A Domain Specific Embedded Language for
%Parallel Programming of Graphics Processors}

%\paperBTitle

\begin{center} 
Bo Joel Svensson, Mary Sheeran, Koen Claessen
\end{center}

\input{./ifl/paperThesis}

% ---------------------------------------------------------------------------
% 
% ---------------------------------------------------------------------------
\cleardoublepage 

\section[\paperCTitle]{\paperC: \\ \paperCTitle}
\label{sec:paperC}
%\addcontentsline{toc}{chapter}{Obsidian: GPU Computing Using Haskell}

%\paperCTitle

\begin{center} 
Bo Joel Svensson, Koen Claessen, Mary Sheeran
\end{center}

\input{./papp/paperThesis}


% ---------------------------------------------------------------------------
% 
% ---------------------------------------------------------------------------
\cleardoublepage 

\section[\paperDTitle]{\paperD: \\ \paperDTitle}
\label{sec:paperD}
%\addcontentsline{toc}{chapter}{Expressive Array Constructs in an Embedded GPU Kernel Programming Language}

%\paperDTitle

\begin{center} 
Koen Claessen, Mary Sheeran, Bo Joel Svensson
\end{center}

\input{./expressive/paperThesis}

% ---------------------------------------------------------------------------
% 
% ---------------------------------------------------------------------------
\cleardoublepage 

\section[\paperETitle]{\paperE: \\ \paperETitle}
\label{sec:paperE}
%\addcontentsline{toc}{chapter}{Counting and Occurrence Sort for GPUs using an Embedded Language}

% \paperETitle

\begin{center} 
Josef Svenningsson, Bo Joel Svensson, Mary Sheeran 
\end{center}


\input{./csort/paperThesis}


% ---------------------------------------------------------------------------
% 
% ---------------------------------------------------------------------------
\cleardoublepage 


\section[\paperFTitle]{\paperF: \\ \paperFTitle}
\label{sec:paperF}


%\addcontentsline{toc}{chapter}{Simple and Compositional Reification of Monadic Embedded Languages}

% \paperFTitle

\begin{center} 
Bo Joel Svensson, Mary Sheeran
\end{center}


%\begin{center}
%{\large \bf This paper will be revised before printing the Thesis} 
%\end{center}


\input{./hl/paperThesis}



% ---------------------------------------------------------------------------
% 
% ---------------------------------------------------------------------------

\chapter{Retargetable Parallel Programming Papers}
\label{chap:ArBB}
% ---------------------------------------------------------------------------
% 
% ---------------------------------------------------------------------------
\cleardoublepage 


\section[\paperGTitle]{\paperG: \\ \paperGTitle}
\label{sec:paperG}
%\addcontentsline{toc}{chapter}{Parallel Programming in Haskell Almost for Free}

% \paperGTitle 

\begin{center} 
Bo Joel Svensson, Ryan R. Newton
\end{center}


\input{./arbb/paperThesis}

% ---------------------------------------------------------------------------
% 
% ---------------------------------------------------------------------------
\cleardoublepage 


\section[\paperHTitle]{\paperH: \\ \paperHTitle}
\label{sec:paperH}
%\addcontentsline{toc}{chapter}{Parallel Programming in Haskell Almost for Free}

% \paperHTitle

\begin{center} 
Bo Joel Svensson, Mary Sheeran
\end{center}


\input{./embarbb/paperThesis}


\cleardoublepage


% ---------------------------------------------------------------------------
% nocites 
% ---------------------------------------------------------------------------
%% \nocite{*}



%% \makeatletter
%% \renewenvironment{thebibliography}[1]
%%      {\chapter*{\bibname}%
%%       \@mkboth{\MakeUppercase\bibname}{\MakeUppercase\bibname}%
%%       \list{\@biblabel{\@arabic\c@enumiv}}%
%%            {\settowidth\labelwidth{\@biblabel{#1}}%
%%             \leftmargin\labelwidth
%%             \advance\leftmargin\labelsep
%%             \@openbib@code
%%             \usecounter{enumiv}%
%%             \let\p@enumiv\@empty
%%             \renewcommand\theenumiv{\@arabic\c@enumiv}}%
%%       \sloppy
%%       \clubpenalty4000
%%       \@clubpenalty \clubpenalty
%%       \widowpenalty4000%
%%       \sfcode`\.\@m}
%%      {\def\@noitemerr
%%        {\@latex@warning{Empty `thebibliography' environment}}%
%%        \endlist}
%% \makeatother

%% \bibliographystyle{alpha}
%% \bibliography{thesis}
%% \addcontentsline{toc}{chapter}{Bibliography}

\end{document}
