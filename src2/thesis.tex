\documentclass[a4paper]{book}

%\usepackage[latin9]{inputenc}
\usepackage[utf8x]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{pdfpages}
\usepackage[british]{babel}
\usepackage{placeins}
\usepackage{dialogue}
\usepackage{fancyvrb}

\usepackage{t1enc}         
\usepackage{times}
\usepackage{url}
\usepackage{enumerate}

%\usepackage{code}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
%\usepackage{lingmacros}
\usepackage{subfig}
\usepackage{fancyhdr}


\pagestyle{plain}
%\pagenumbering{arabic}


%stuff
\usepackage{latexsym}
\usepackage{tikz,pgflibraryshapes}
\usetikzlibrary{shapes,snakes,positioning,matrix}
\usepackage{pgfplots}
\usepackage{booktabs}
\usepackage{enumerate}
%\usepackage{etoolbox}


\usepackage{multibib} 
\newcites{bb}{Bibliography} 
\newcites{ifl}{Bibliography}
\newcites{papp}{Bibliography}
\newcites{exp}{Bibliography}
\newcites{hl}{Bibliography}
\newcites{csort}{Bibliography}
\newcites{emb}{Bibliography}
\newcites{arbb}{Bibliography}
\newcites{t}{Bibliography}
%\newcites[ifl}{Bibliography}

%\patchcmd{\thebibliography}{\section*}{\section}{}{}

\makeatletter
\renewenvironment{thebibliography}[1]
     {\section*{\bibname}%
      %\@mkboth{\MakeUppercase\bibname}{\MakeUppercase\bibname}%
      \list{\@biblabel{\@arabic\c@enumiv}}%
           {\settowidth\labelwidth{\@biblabel{#1}}%
            \leftmargin\labelwidth
            \advance\leftmargin\labelsep
            \@openbib@code
            \usecounter{enumiv}%
            \let\p@enumiv\@empty
            \renewcommand\theenumiv{\@arabic\c@enumiv}}%
      \sloppy
      \clubpenalty4000
      \@clubpenalty \clubpenalty
      \widowpenalty4000%
      \sfcode`\.\@m}
     {\def\@noitemerr
       {\@latex@warning{Empty `thebibliography' environment}}%
      \endlist}
\makeatother


%%

%% \newcommand{\bequ}{\begin{quote}}
%% \newcommand{\enqu}{\end{quote}}
%% \newcommand{\eop}[1]{\mbox{\sl #1}}
%% \newcommand{\sube}[2]{#1_{#2}}
%% \newcommand{\id}[1]{{\it #1}}
%% \newcommand{\kwd}[1]    {\mbox{\textbf{#1}}}
%% \newcommand{\str}[1]{{\id "#1"}}
%% \newcommand{\lemon}{\emph{lemon}}
%% \newcommand{\textt}[1]{\texttt{#1}}
%% \newcommand{\txttt}[1]{\texttt{#1}}

%\renewcommand*{\thesection}{\arabic{section}}

\newcommand{\thesistitle}{Embedded Languages for Data-Parallel Programming}
\newcommand{\dept}{Department of Computer Science and Engineering}
\newcommand{\uni}{Chalmers University of Technology and G\"oteborg University}
\newcommand{\group}{Functional Programming Research Group}


% ---------------------------------------------------------------------------
% Papers 
% ---------------------------------------------------------------------------
\newcommand{\paperA}{Paper A}
\newcommand{\paperATitle}{Simple and Compositional Reification of Monadic Embedded Languages}
\newcommand{\paperB}{Paper B}
\newcommand{\paperBTitle}{Obsidian: A Domain Specific Embedded Language for Parallel Programming of Graphics Processors}
\newcommand{\paperC}{Paper C} 
\newcommand{\paperCTitle}{GPGPU Kernel Implementation and Refinement using Obsidian} 
\newcommand{\paperD}{Paper D}
\newcommand{\paperDTitle}{Expressive Array Constructs in an Embedded GPU Kernel Programming Language}
\newcommand{\paperE}{Paper E}
\newcommand{\paperETitle}{Counting and Occurrence Sort for GPUs using an Embedded Language}
\newcommand{\paperF}{Paper F}
\newcommand{\paperFTitle}{A High-Level Embedded Language for Low-Level GPU Kernel Programming}
\newcommand{\paperG}{Paper G}
\newcommand{\paperGTitle}{Programming Future Parallel Architectures with Haskell and Intel ArBB}
\newcommand{\paperH}{Paper H}
\newcommand{\paperHTitle}{Parallel Programming in Haskell Almost for Free}

% ---------------------------------------------------------------------------
% BEGIN ! 
% ---------------------------------------------------------------------------
\begin{document}

\begin{titlepage}
\begin{centering}
{\sc Thesis for the Degree of Doctor of Philosophy}
\vspace{30ex}

{\LARGE\bf\thesistitle}

\vspace{7ex}

\large Bo Joel Svensson
\vfill
\includegraphics[width=120mm]{./img/ChalmGUtextsvEng}\\[5mm]
\includegraphics[height=4cm]{./img/ChalmGUmarke}\\

\vspace{1cm}
\normalsize
{\sc \dept}\\
{\sc \uni}\\
G\"oteborg, Sweden 2013

\end{centering}
\end{titlepage}

% ---------------------------------------------------------------------------
% Tryckortssida
% ---------------------------------------------------------------------------

\quad \vfill

{\noindent\large\bf\thesistitle} \\
\noindent Bo Joel Svensson \\
\noindent ISBN \\

\vspace{1cm}

\noindent\copyright {\sc {Bo Joel Svensson}}, 2013 \\ 
\vspace{1cm} 

\noindent Technical Report 101D\\
\noindent ISSN                 \\
\noindent \dept \\
\noindent \group\\

\vspace{1cm} 

\noindent \uni \\
\noindent SE--412~96~~G\"oteborg, Sweden\\
\noindent Phone: +46 (0)31--772~1000 \\

\vspace{1cm} 

\noindent Printed in Sweden\\
\noindent Chalmers Reproservice\\
\noindent G\"oteborg, Sweden 2013


\thispagestyle{empty}

\clearpage
\pagenumbering{roman}

\section*{Abstract}
Abstract abstract 

\vspace{5mm}

\noindent

\textbf{Keywords:} Data-parallelism, Functional Programming, Embedded languages, GPU

\clearpage

% ---------------------------------------------------------------------------
%  ACKS !!! 
% ---------------------------------------------------------------------------
\section*{Acknowledgments}


\vspace{5mm}
\noindent This research has been funded by the Swedish Foundation for
Strategic Research (which funds the Resource Aware Functional 
Programming (RAW FP) Project) and by the Swedish Research Council.

\clearpage

% ---------------------------------------------------------------------------
%  Guide for Opponent, grading committee
% ---------------------------------------------------------------------------
\section*{Publications}

This thesis includes the following publications: 

\begin{enumerate}[A.] 
\item Josef Svenningsson and Bo Joel Svensson. Simple and Compositional Reification of Monadic Embedded Languages, 2013. \emph{18th ACM SIGPLAN International Conference of Functional Programming, ICFP 2013.}
\item Bo Joel Svensson, Mary Sheeran and Koen Claessen. Obsidian: A Domain Specific Embedded Language for General Purpose Parallel Programming of Graphics Processors. In \emph{Proc. of Implementation and Applications of Functional Languages (IFL)}, Lecture Notes in Computer Science, Springer Verlag, March 2009.
\item Bo Joel Svensson, Koen Claessen and Mary Sheeran. GPGPU kernel implementation and refinement using Obsidian. \emph{Practical Aspects of High-level Parallel Programming, PaPP 2010.} Procedia Computer Science.
\item Koen Claessen, Mary Sheeran and Bo Joel Svensson. Expressive Array Constructs in an Embedded GPU Kernel Programming Language. In \emph{Proceedings of the 7th workshop on Declarative aspects and applications of multicore programming}, DAMP '12. ACM. 
\item Josef Svenningsson, Bo Joel Svensson and Mary Sheeran. Efficient Counting Sort Implementations using an Embedded GPU Programming Language. \emph{2nd Workshop on Functional High-Performance Computing, FHPC '13.}
\item Bo Joel Svensson and Mary Sheeran. A High-Level Embedded Language for Low-Level GPU Kernel Programming. {\bf\emph{This work has not yet been published.}} 
\item Bo Joel Svensson and Ryan Newton. Programming Future Parallel Architectures with Haskell and ArBB. \emph{Future Architectural Support for Parallel Programming (FASPP), in conjunction with ISCA '11.}
\item Bo Joel Svensson and Mary Sheeran. Parallel Programming in Haskell Almost for Free: an embedding of Intel's Array Building Blocks. \emph{Proceedings of the 1st ACM SIGPLAN Workshop on Functional High-Performance computing, FHPC '12.}
\end{enumerate} 

\clearpage
\subsection*{Personal contributions} 

The following describes my personal contributions to the papers included 
in this thesis. 

\subsubsection{Paper A}
I intuitively applied the monad reification method as part of the implementation 
of Obsidian. Josef Svenningsson identified its importance and suggested we write 
about it. The paper has two distinct parts. Sections one and two describe the 
method as I used it. And section four makes the compositional aspect of the method 
explicit. 

The code and examples in section one and two are my work. The contents of section four was 
developed by Josef Svenningsson and me. For section four we also got very much and valuable help 
from Emil Axelsson. 

For the rest of the paper, just substituting my name for Benny and Josef's for Bj\"orn
will give a good indication of our contributions. 

\subsubsection{Papers B, C, D and E} 
The Obsidian language has grown out of discussions between Mary Sheeran, Koen Claessen and me. 
When it comes to implementation of the ideas we had in these discussions, I have been responsible. 
Push arrays, first appearing in paper D, were invented by Koen Claessen. However, the 
implementation of them in the setting of Obsidian was done by me. Paper D also has large 
contributions by Mary Sheeran when it comes to examples and implementing them. 

Paper E was the first I wrote with Josef Svenningsson, who had been thinking about an interesting
variation of counting sort and wanted to implement it. Together we added atomic operations to 
Obsidian and I modified the code generator to accommodate them. Sections 
1 and 2 in the paper are written by Josef. Sections 3 through 7 are mostly my work, including 
the benchmarking. Section 8 is joint work.
%Me and Josef are equal partners in this paper and got much valuable advice from 
%Mary Sheeran. 

\subsubsection{Papers G and H} 
The work behind paper G, work was performed by me 
under supervision of Ryan R. Newton who is also mostly responsible for the writing of the 
paper. Programming and running time experimentation was performed by me, while getting much 
valuable advice and guidance from Ryan. 

The work behind paper H was done by me as a hobby project and my aspirations were not 
high. I just wanted to experiment with some embedded languages techniques that we had 
been avoiding (had no need for) in Obsidian. Mary discovered what I was doing and applied 
the right amount of pressure, leading to us encompassing much more of ArBB functionality than 
I had in mind. The paper's text is joint work; most examples as well as benchmarking are my work,
but the sparse matrix vector multiplication example is entirely Mary's. 

%Intel has unfortunately retired the ArBB project. This leaves EmbArBB without a functioning 
%code generating backend u

% \subsection*{Guide} 

\tableofcontents


\cleardoublepage
\clearpage

\pagenumbering{arabic}
\pagestyle{fancy}
\fancyfoot{}
\fancyhead[LO]{}
\fancyhead[RO]{\leftmark}
\renewcommand{\headrulewidth}{0.0pt}
\fancyhead[LE,RO]{\thepage}

% --------------------------------------------------------------------------- %
% --------------------------------------------------------------------------- %
%
%  INTRODUCTION 
%
% --------------------------------------------------------------------------- %
% --------------------------------------------------------------------------- %
\chapter{\thesistitle}

\section{Introduction} 

This thesis is about applying embedded language techniques to programming 
multi- and many-core computers. Computers today are becoming more and more  
parallel. With each new generation of 
general purpose processor (CPU) or graphics processor (GPU), we get more 
parallel resources. To fully make use of modern CPUs and GPUs, we need to
exploit their parallelism. 

While it is sometimes possible to automatically parallelise programs originally 
written for serial execution, it is becoming clear that explicitly parallel 
programming models are needed for making use of all parallel resources~\cite{STRUCTURED}. 
This also means that it is important to think in parallel from the beginning. 

This thesis focuses on data-parallelism, as opposed to control- or 
task-parallelism. Data parallelism is applicable where the input data can 
be divided into chunks and operated upon independently. Data-parallelism is scalable,
meaning that the potential for parallelism increases with problem size. 

Modern CPUs are parallel in a number of different ways. They discover instruction
level parallelism (ILP), they have multiple processor cores and vector units for 
Single Instruction Multiple Data (SIMD)  parallelism. The exact configuration, number
of cores and number of vector units, varies over available processor models. 
To obtain maximum performance, with a specific target processor in mind, the programmer
can use low-level intrinsics in order to get SIMD parallelism and a threading library
for parallelism over the available cores. One problem with this is that the resulting program 
would be specialised to a specific processor model. The Intel Array Building Blocks (ArBB) 
system approaches this problem by providing a high level programming interface~\citet{ARBB2011}.
This interface is based on a set of parallel operations on arrays, such as map, folds 
and scans. ArBB was implemented as an embedded language in C++, but a low-level C interface to 
its capabilities was also provided. ArBB programs are JIT-ed (Just-In-Time compiled) to 
target the available CPU or accelerator (such as the Intel Larrabee~\citet{Larrabee}, 
knights Ferry, Xeon Phi). In chapter~\ref{chap:ArBB}, there are papers describing work 
towards embedding Intel ArBB in Haskell. 

Graphics Processors (GPUs) trade programmer convenience for increased parallelism. 
For example, where CPUs dedicate large portions of chip area to caches, branch prediction and 
ILP, GPUs put additional cores. This comes at a price of increased programmer effort.
GPUs have programmer managed shared memories, conditionals are troublesome from a performance 
perspective and memory access patterns can make or break the performance of an 
application.  

Intel ArBB is part of a movement towards thinking of structure and patterns in 
parallel programming. Other languages and libraries with similar aspirations 
are NVIDIA Thrust~\citet{THRUST} and Data.Array.Accelerate~\citet{ACCELERATEDAMP11}.
In the book ``Structured Parallel Programming'' by Michael McCool et al, a case is 
made for thinking in terms of building blocks for parallel programming~\citet{STRUCTURED}. 
They emphasise the importance of collective array operations such as map, fold, scans, 
permutations and gather/scatter operations. The book contains examples in ArBB, Cilk Plus, 
Intel Threading Building Blocks, OpenMP and OpenCL. Not all these languages include 
the patterns as abstractions, but most are possible to express in each of the languages. 
The patterns, while very powerful as abstractions, also provide tools for thinking about 
parallel algorithms. The ArBB library supports flat regular data-parallelism, but also 
irregular and to some degree nested data-parallelism. 

The patterns are powerful abstractions, but it is also important not to be locked into 
a single implementation of each pattern. This could have negative performance impacts. 
In ArBB, for example, the patterns used are specialised to the current hardware during JIT 
compilation. Languages like OpenCL and CUDA do not provide abstractions of these patterns. 
The programmer needs to implement them using lower level features, making CUDA and OpenCL 
code less composable and harder to reuse. As an example, look at the slides from NVIDIA in 
reference~\citet{reduction} where a reduction primitive is implemented in CUDA. First a 
naive reduction pattern is implemented and then refined over many steps, taking many 
characteristics of the GPU into mind. The end result is an implementation of the 
reduction pattern that performs very well on the chosen GPU. But as GPUs evolve, the 
design space may need to be explored again. 
%What is 
%is that says that we do not need to redo this exploration with each new GPU generation 
%and discover a new optimal solution?  

Obsidian is our embedded language for GPU programming. We raise the level of abstraction 
compared to CUDA and provide a more composable interface. But we also want to keep 
the possibility for the programmer to implement patterns in different ways. A balance 
must be struck between providing enough low-level control to get at the performance 
and still making building blocks implemented by the programmer reusable and composable. 
We want to provide more compositional tools for the programmer to experiment with while 
exploring the space of various ways to implement a given pattern or primitive. 
In chapter~\ref{chap:GPUProgramming}, there are papers describing work on Obsidian. 
Obsidian is focused on flat and regular data-parallelism. 

% \section{The problem}

% \emph{what is so interesting about Parallelism and GPUs}

% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
%
% GPU Programming 
%
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
\section{GPUs and CUDA} 

In 2006 NVIDIA released their first Compute Unified Device Architecture (CUDA) 
GPU. With CUDA, NVIDIA also made it a lot easier for programmers to use GPUs 
for general purpose programming. A dialect of C, called CUDA C, was released specifically 
to simplify implementation of general purpose algorithms on GPUs. 

A CUDA GPU is built around a single kind of processor (as opposed to the different 
kinds of processors found in earlier GPUs). The processors in the GPU (called MPs, 
MultiProcessors) all contain a number of function units called SPs (Streaming Processors). 
Each MP also contains local memory, called shared memory since it can be accessed 
by all of the SPs in that MP. The number of MPs varies over the available GPUs; cheaper 
GPUs have as few as one MP, and as you go up in price the number of MPs increases.

Each MP of the GPU can manage a large number of threads; on today's GPUs up to 
2048 threads can run on a single MP. The GPU schedules threads in groups of 32, called 
{\em Warps}, that are executed in lock-step (SIMD style execution). Threads are 
also divided into {\em Blocks}; the threads within a block can communicate using the 
shared memory. The maximum block size is currently 1024 threads. Since 
the number of threads in a block can be higher than the warp size, a barrier 
synchronisation mechanism exists to ensure that threads within a block 
can communicate safely via the shared memory. 


Papers B, C, E and F (sections~\ref{sec:paperB},~\ref{sec:paperC},~\ref{sec:paperE},
~\ref{sec:paperF}) contain more information about GPUs, and the assumptions we make 
about them in relation to Obsidian. 

\subsection{CUDA programming} 

The CUDA programming model reflects the hierarchical architecture of GPUs. The programmer 
writes what are called {\em kernels}; in CUDA jargon a kernel refers to a sequential C program 
that is parameterised on a thread's identity. The kernel program is executed in parallel across 
all the threads of a block and many such blocks of threads can execute in parallel on the 
available MPs. The programmer needs to decompose the algorithm and data in such a way that 
communication will be local to the blocks. 

Below is a very simple CUDA example to give some indication of what a kernel and 
the code to launch that kernel looks like. 

\begin{small}
\begin{Verbatim}[samepage=true]
__global__ void kernel(int32_t *i1, int32_t *i2, int32_t *r) {
  
  unsigned int gid = blockIdx.x * 
                     blockDim.x + 
                     threadIdx.x;

  r[gid] = i1[gid] + i2[gid]; 

} 

\end{Verbatim}
\end{small}

This kernel takes two input arrays, {\tt i1} and {\tt i2}, and computes 
element wise sums into the result array, {\tt r}. The {\tt threadIdx} variable identifies 
a thread within a block. There is also a {\tt blockIdx} variable that identifies 
the block a thread belongs to and {\tt blockDim.x} specifies the number of threads per block. 


In CUDA there is a notion of {\em host} and {\em device}, the device is the GPU and 
its memory (Global memory, usually a few Gigabytes). The host refers to the system housing 
the GPU. The host runs a control program that launches kernels on the GPU. First memory 
needs to be allocated in the device to hold the inputs and outputs. Then data is copied from 
the host's system memory into device memory. 

\begin{small}
\begin{Verbatim}[samepage=true]
int main(void) {
  
  int32_t h_i1[32], h_i2[32], h_r[32]; 

  int32_t *d_i1, *d_i2, *d_r;

  
  for (int i = 0; i < 32; ++i) { 
    h_i1[i] = i;
    h_i2[i] = 32 - i;
  }

  // Allocate device memory
  cudaMalloc((void**)&d_i1,32*sizeof(int32_t));
  cudaMalloc((void**)&d_i2,32*sizeof(int32_t));
  cudaMalloc((void**)&d_r,32*sizeof(int32_t));
  
  // Copy inputs to device
  cudaMemcpy(d_i1,
             h_i1,
             32*sizeof(int32_t),
             cudaMemcpyHostToDevice);
  cudaMemcpy(d_i2,
             h_i2,
             32*sizeof(int32_t),
             cudaMemcpyHostToDevice);
  
  // Launch the kernel
  kernel<<<1,32,0>>>(d_i1,d_i2,d_r); 

  // Copy result to host
  cudaMemcpy(h_r,
             d_r,
             32*sizeof(int32_t),
             cudaMemcpyDeviceToHost);

  for (int i = 0; i < 32; ++i) { 
    printf("%d ",h_r[i]);
  }

  return 0;
}
\end{Verbatim} 
\end{small}

The syntax for launching a kernel is:

\begin{small}
\begin{Verbatim}[samepage=true]
name<<<Blocks, Threads/Block, BytesShared>>>(arg1,..,argN);
\end{Verbatim}
\end{small}

\noindent In the example above one block of 32 threads, using no shared memory, is launched. 

A kernel that uses shared memory takes the form: 

\begin{small}
\begin{Verbatim}[samepage=true]
__global__ void kernel2(int32_t *i, int32_t *r) {

  extern __shared__ int32_t sm[]; 

  unsigned int tid = threadIdx.x; 
  unsigned int gid = blockIdx.x * 
                     blockDim.x + 
                     threadIdx.x;

  sm[tid] = i[gid]; 
  __syncthreads();

  /* Compute on sm */ 
     
  __syncthreads();
  r[gid] = sm[tid]; 
  
}
\end{Verbatim} 
\end{small} 

\noindent A portion of the input array is stored into shared memory. A 
barrier synchronisation is used to ensure that each threads within the block 
has stored its value into shared memory (only necessary if threads will 
communicate across warp boundaries). After computing on the data locally, 
it is written back into global memory. 


\subsection{Writing efficient GPU kernels} 
\label{sec:efficient}

Writing efficient GPU kernels is hard. There are many details that the CUDA 
programmer must be aware of in order to make optimal use of the GPU. 
The NVIDIA ``CUDA C Best Practices Guide''~\citet{BestPrac}, is a good source 
to learn more about these details. Here is a list, with explanations of some 
of the practices that the guide deems most important. 

\begin{itemize} 

\item {\bf Minimise data transfers between host and device:} The bandwidth between device memory 
and GPU is much higher than the bandwidth between host memory and the GPU device via the PCIe bus.
This means that, first of all one must evaluate if the computation to be performed is significant 
enough to warrant a transfer to the GPU. If the computation is not arithmetic intensive, it may be 
better to do the work on the CPU. However, if the data is already in device memory and 
the operation that is about to perform on it is more efficiently handled by the CPU, it may still 
be better to let the GPU do the work. In other words, compute near the data unless transferring it 
pays off. 

\item {\bf Ensure that global memory accesses are coalesced:} Certain access patterns into 
GPU device memory can be {\em coalesced} (combined into few memory transactions). Unfortunately 
what these access patterns are varies somewhat depending on the GPU. For a typical model of 
GPU, the rule is that concurrent memory accesses (by threads within a warp) will be coalesced 
to as many transactions as cache lines touched (128b l1 cache lines). This means that strided 
accesses from within a warp are not favoured. 

\item {\bf Minimise use of global memory:} Accessing global device memory can take as 
many as 400 to 600 clock cycles if the data is not cached. The programmer should 
use shared memory as much as possible to avoid redundant loading from global memory.
Shared memory can also be used to coalesce reads from global memory. Shared memory is 
a limited resource in the MP, so if a kernel uses very much shared memory it means 
fewer such blocks can be active on the MP. 

\item {\bf Use a multiple of 32 threads per block:} Groups of 32 threads, warps, are the 
scheduled unit on a GPU. If a warp accesses memory (and will be waiting for it arrive during 
600 cycles) it will be swapped out and another warp will take its place. This means that 
the number of available warps decides how well memory latency can be hidden. A block that is 
not a multiple of 32 leads to wasting GPU resources; some processing elements will stand 
idle during the execution of the not full warps. 

\item {\bf Avoid different execution paths within a block:} Any branching instruction that 
causes the execution paths to diverge within a warp will affect performance. When the execution 
paths within a warp diverge, the computation is serialised. 

\item {\bf Do not use {\tt \_\_synchthreads()} in diverging code:} Extreme care must be taken 
when synchronising within any conditionally executed code; every thread must reach this barrier.
Failing to ensure this will likely lead to the kernel producing incorrect results. Note that 
this is a major limitation to the composability of CUDA functions. Wheather or not it is safe to 
call some function in a certain place in your code can be decided only by inspecting that function's 
implementation. 
%f it is safe or 
%not to call some function at a certain place in your code, can only be decided by inspecting 
%that functions implementation. 
\end{itemize} 

The list above provides some very general guidelines on what to think about when 
writing GPU kernels. The lesson is that fast memory near the processing element 
should be preferred over slow memory far away. However, this comes at the effort 
of decomposing computation and data in a way that makes this possible. 
 

%\citet{FERMIOPT}
%\citet{BestPrac} 
%\citet{merrill}


% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
%
% Embedded languages
%
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
\section{Embedded languages}

This thesis applies embedded language methodology to data-parallel programming. An 
embedded language is implemented as a library in a host language, Haskell in this case. 
The embedded language approach is beneficial since we are not entirely sure how 
to program current and future highly parallel computers. Implementing embedded languages allows 
us more rapid prototyping of ideas. Other benefits of embedded languages often listed 
are include making use of the host language's type system to guarantee that correctly typed target code is generated. 
Already existing libraries in the host language (if they are general enough) 
can be used in the embedded language. The host language can also be used as a powerful macro 
language for your embedded language. 

A very good paper, and highly relevant to our approach, is ``Compiling Embedded Languages''
by Conal Elliott et al~\citet{COMPILEEDSL}. This paper describes how to embed a compiler 
backend and generate efficient code in some target language. One key aspect of this 
approach is that the embedded language is a library of functions that create and 
compose Abstract Syntax Trees (ASTs). Obsidian (chapter \ref{chap:GPUProgramming}) as 
well as EmbArBB (chapter \ref{chap:ArBB}) are implemented in a similar way. An 
embedding that builds ASTs is called a deep embedding. Shallow embeddings, on the 
other hand, do not build ASTs. In ``Combining Deep and Shallow Embedding for EDSL'' 
Josef Svenningsson and Emil Axelsson combine the two methods, shallow and deep, 
in order to simplify the AST data type (fewer constructors) and make more extensible 
embedded languages \citet{DEEPSHALLOW}. Obsidian also uses a combination of shallow 
and deep embeddings, while EmbArBB is a more traditional deep embedding. 

The embedded language approach has become popular to use as a way of raising the 
level of abstraction in fields where the default is to use low-level languages. 
For example, Lava and Wired~\citet{LAVA,Wired} raises the level of abstraction in the 
fields of hardware design and verification and Feldspar does the same in the area of  digital 
signal processing~\citet{FELDSPAR2010}. For GPU programming, there are numerous 
embedded approaches using various host languages. Two languages for GPU programming 
embedded in Haskell are  Accelerate and Nikola \citet{ACCELERATEDAMP11, NIKOLA}. 
And there is Intel ArBB, embedded in C++, with the goal of being parallel across 
platforms \citet{ARBB2011}. Related work and information about the tools, libraries 
and language that make the wider context surrounding Obsidian can be found in 
section~\ref{sec:relatedwork}. 

% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
%
% Obsidian
%
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
\section{Obsidian: An embedded language for GPU kernel implementation}

\vspace{5mm} 
\emph{This section will be revised. I want to add more code examples.} 
\vspace{5mm} 

Obsidian aspires to raise the level of abstraction of GPU kernel implementation, 
while maintaining enough control over details that influence performance on a GPU. 
We try to reach these goals by offering abstractions to the programmer that are 
compositional and parallel. We also ensure that these can easily be compiled 
to NVIDIA CUDA code. We have two different array representations, each with 
its strengths and weaknesses. We also feel that it is important that the programmer 
be able to use local shared memory in computations and have the ability to influence 
memory access patterns. What these array representations and abstractions are 
will be explained in the following sections, with references to the papers 
that introduce them or explain them the best. 


\subsection{Deep embedding: The {\tt Program} data type}

Obsidian uses a deeply embedded {\tt Program} data type that represents GPU Kernels. 
A CUDA GPU has a hierarchy of parallel resources. At the bottom there are threads, each 
executing sequential programs. There are groups of threads (of 32) called {\em Warps} that 
execute in lock-step. Warps are the scheduled unit of work on a GPU. There are {\em Blocks} 
of threads, a set of threads that run as a group and share local (shared) memory. And 
lastly there is a {\em Grid} of blocks that specifies the total number of threads involved 
in a computation (Number\_of\_Blocks * Number\_of\_Threads). The deeply embedded program 
data type of Obsidian models this hierarchy by parameterising programs on a hierarchy 
level type parameter (the  {\tt t} parameter below).

\begin{verbatim} 
data Program t a where
\end{verbatim}

The {\tt t} parameter can be either {\tt Thread}, {\tt Warp}, {\tt Block} or {\tt Grid}.
These types are related to each other via a {\tt Step} type constructor that represents 
going upward one step in the hierarchy. 

\begin{verbatim} 
data Step a -- A step in the hierarchy
data Zero
\end{verbatim} 

The {\tt Thread} type is level {\tt Zero} ({\tt type Thread = Zero}). Then {\tt Block} is 
{\tt Step Thread} and {\tt Grid} is {\tt Step Block}. Currently {\tt Warp} is not included 
in the hierarchy leading to warp programming being a special case. One benefit a warp has is 
that threads can communicate via shared memory without using synchronisation primitives. There 
is no programmer support for warp level programming in CUDA (as there is for block level 
programs). The programmer needs to take care to ensure that the communication patterns are 
within a warp and is then free to remove synchronisations. If warps were to be put in the 
program level hierarchy of Obsidian, its place would be between the thread and block level. 
This would force the programmer to go via warps when putting together a block computation even 
if the communication pattern is not suited for that, leading to inconvenience with no gain. 

As an example of how the hierarchy level parameter is used, I show the {\tt ForAll} constructor 
from the {\tt Program} data type.  This constructor represents parallelism either over threads 
or blocks. 

\begin{verbatim}
ForAll :: EWord32 
            -> (EWord32 -> Program t ())
            -> Program (Step t) ()
\end{verbatim}

{\tt ForAll} takes a number of parallel iterations, and a body represented by a function 
from an index to a program. The body is a {\tt Program} at some level {\tt t} while the 
resulting program is a step above. This means that a thread program can be turned into 
a block program by using {\tt ForAll}, or that a block program can be turned into a grid program. 

Information about the {\tt Program} data type can be found in paper F, \paperFTitle, in 
section \ref{sec:paperF}. 

\subsection{Scalars} 

Scalars and operations on scalars are represented by an expression data type ({\tt Exp a}) in 
Obsidian. This is another example of a deep embedding; the expression data type contains 
constructors for literal values, arithmetic operations, and conditionals. 

Haskell's type class system allows us to overload arithmetic operations such as {\tt (+)}, 
{\tt (-)} and {\tt (*)} by making expressions an instance of {\tt Num}. This allows the 
Obsidian arithmetic expressions to look exactly like corresponding native Haskell arithmetic. 

The approach taken to embed the scalar language is very similar to what is described 
in ``Compiling Embedded Languages''~\citet{COMPILEEDSL}; but we use a Generalised Algebraic 
Datatype (GADT) to obtain typed expressions. An alternative to the GADT, is to use phantom 
types. Earlier versions of Obsidian (section~\ref{sec:paperB}) used phantom types. 


\subsection{Arrays} 

Obsidian has two array representations. These array representations are implemented as 
shallow embeddings on top of the expression and program data types. This means that these 
arrays will disappear during Haskell evaluation of the Obsidian program.
 
\subsubsection{Pull arrays}

Pull arrays have been part of Obsidian from the very beginning~\citet{JMT} but called either 
{\tt Array} or just {\tt Arr}. A pull array represents an array as an indexing function; given 
an index it provides an element: 

\begin{verbatim} 
data Pull s a = Pull {pullLen :: s, 
                      pullFun :: EWord32 -> a}
\end{verbatim} 

Pull arrays are parameterised on both element ({\tt a}) and length ({\tt s}) type. The length 
parameter is used to be able to represent arrays with both static (known at Haskell runtime) 
or dynamic (length represented by an expression, an unknown length). When creating block and 
warp level computations, we want to know the exact lengths in order to avoid generating code 
riddled with conditionals concerning array lengths. It is possible to write kernels that 
are not specialised for a certain size. This is accomplished by adding bounds checks. Since 
conditionals  are problematic from a performance point of view, fixed size kernels are recommended 
when performance is crucial. 
%Conditionals are problematic on GPUs and 
%while general programs that run for any size is possible when performance is crucial a fixed 
%size is recommended. 

One benefit of pull arrays is that they give fusion of all operations on them for free. 
For example, {\tt map f} is implemented on pull arrays by composing {\tt f} with the indexing function: 

\begin{verbatim} 
map f (Pull n ixf) = Pull n (f . ixf)
\end{verbatim}

Now, we see that \verb!map f . map g! is the same as \verb!map (f . g)! since both result in {\tt f}
and {\tt g} becoming composed onto the indexing function. No intermediate array is built in 
memory.  

Another benefit of pull arrays are that they are parallel. Simply put, early Obsidian was pull 
arrays in addition to a way to compute the array and store its elements to memory 
(called {\tt sync} in early versions of Obsidian and later {\tt force}). A very 
direct way to compile a pull array to a GPU is to launch as many threads as there are 
elements in the array and apply the indexing function to the thread id. Now each thread 
will have computed an element of the array and can store that to memory. 

 
\subsubsection{Push arrays}

Push arrays were added to Obsidian as a complement to pull arrays. Pull arrays are 
good because they are so simple to understand and easy to parallelise. However, certain
operations on pull arrays yield code that is not suitable for GPU execution. Concatenation 
or interleaving of pull arrays result in conditionals in the indexing function. The 
interleaving case, which is worst, leads to diverging branching in every warp. If branches 
diverge, the GPU will turn off the processing elements taking one path and 
allow the others to progress; then turning to the other branch. The computation of the 
branches is serialised and compute resources are wasted. 

Push arrays, like pull arrays, are functions. A Push array is a higher order 
function, taking a function from value and index to a thread program and giving 
as result a program at some level in the hierarchy. The function passed to the 
push array function we call a write-function. Below is the definition of 
push arrays that we use in Obsidian. Push arrays are parameterised on element 
type ({\tt a}), size type ({\tt s}) and program hierarchy level ({\tt p}). 

\begin{verbatim}
data Push p s a =
  Push s ((a -> EWord32 -> TProgram ()) -> Program p ())
\end{verbatim} 

When working with pull arrays, the consumer of such an array decides how to 
iterate over the array and what elements to compute. This is done applying the 
push array function to those indices that are interesting to the consumer. Push 
arrays work in the opposite way. The Push array function (the result program) contain 
the iteration schema; the consumer of the push only decides what write function 
to supply. The iteration schema used is decided upon when creating a push arrays. 
The code below is an example of how to create a push array from a pull array.

\begin{verbatim} 
convertToPush :: Pull Word32 e 
               -> Push Block Word32 e 
convertToPush (Pull n ixf) =
    Push n $ \wf -> 
      ForAll (fromIntegral n) $ \i -> wf (ixf i) i
\end{verbatim} 

The function above takes a pull array and creates a push array with a parallel 
iteration schema over threads within a block. There are other iteration schemas to 
chose from such as sequential, combinbations of sequential and parallel
across both threads and blocks (two level parallelism). 

Push arrays give many of the same benefits that pull arrays have. Using push 
arrays we also get fusion of operations as default. The implementation of map 
(shown below) illustrates how maping a function {\tt f} over a push array results 
in an application of {\tt f} in the write-function. 

\begin{verbatim} 
map f (Push s p) = Push s $ \wf -> p (\e ix -> wf (f e) ix)
\end{verbatim}

\subsection{Compilation to CUDA}

When compiling Obsidian programs to CUDA, we only need to be concerned with the 
{\tt Program} datatype. Given the hierarchy level type parameters to the program datatype, 
we know that the AST can be quite directly translated to CUDA. We know that there will 
be at most one level of nestedness in the parallelism (several blocks, each of several threads). 
This means that we do not need to do any transformations on the AST before generating CUDA code. 
Paper F (section~\ref{sec:paperF}) contains technical details of the compilation process. 

\subsection{Results}
\FloatBarrier 

We have implemented basic algorithms such as sorting, merging, fractal generation, reductions 
and parallel prefix (scans). Often we have managed to get very good performance, close to that 
of hand optimised CUDA code. However, there are also cases where we just cannot seem to reach 
the level of performance that we would want. One such example is parallel prefix, an 
application that has been haunting us from the very beginning. Scan is problematic to us because 
it is best implemented as an in-place algorithm, while our array representations do 
not allow for in-place update; there is no memory associated with them to update. 
We are trying to improve this situation by adding a third array representation to Obsidian,
mutable arrays.   

Paper F (section~\ref{sec:paperF}) tells a story about how to implement and tune 
a reduction kernel in Obsidian. At the end, the implemented reduction kernel is  
faster than the reduction skeleton used in Accelerate. This is quite a feat, since 
reduction in Accelerate corresponds to just one single hand optimised skeleton. We would have 
been happy just getting close.   

In paper E (section~\ref{sec:paperE}), we implement counting sort and occurrance sort 
using Obsidian. We use very low-level features of Obsidian and end up producing 
code that performs very well. 

Paper D (section~\ref{sec:paperD}) introduces push arrays and uses them in the 
implementation of sorting networks to quite good results. With push arrays we 
were able to experiment with more detailed decompositions (what thread computes 
what value) of the algorithms. 

In section \ref{sec:efficient}, guidelines for writing efficient CUDA code is listed. 
Here we revisit these guidelines and explain how they are approached by Obsidian and 
the Obsidian programmer. 

\vspace{5mm}

\emph{This list below will be revised. More details or code will be added.}

\vspace{5mm}

\begin{itemize} 

\item {\bf Minimise data transfers between host and device:} Data transfers
between host and device are out of Obsidian's scope. If Obsidian is used to create 
a kernel for a computation, the programmer has already decided that a GPU should be used. 

\item {\bf Ensure that global memory accesses are coalesced:} Inputs to an Obsidian 
program are pull arrays. Pull arrays can be arbitrarily permuted; the permutation 
used will decide the access pattern. If this method can be applied or not partly also 
depends on the algorithm being implemented. 

\item {\bf Minimise use of global memory:} Intermediate results can be computed into 
shared memory. Arrays are stored into shared memory when the programmer uses a 
{\tt force} primitive.   

\item {\bf Use a multiple of 32 threads per block:} Obsidian requires regularity even more 
than CUDA does. Since we focus on generating high performance kernels, we rule out generation 
of kernels that work on different input sizes. The choise of how many threads to use per block 
is in the hands of the Obsidian program. The method for making that choise has changed over time. 
Earlier, the number of threads needed by kernel was decided by the size of the arrays it 
computed. This has recently changed is now set by parameters to the code generation process. 

\item {\bf Avoid different execution paths within a block:} Sometimes divergent code cannot 
be avoided. However, It is important not to have threads diverge unnecessarily. When using pull 
arrays alone this was often a problem; with the addition of push arrays the situation improved. 
Using push arrays a divergent computation can be explicitly split up into phases, each with all 
threads following the same execution path. This means that fewer threads are required to do the 
same work. 

\item {\bf Do not use {\tt \_\_synchthreads()} in diverging code:} In Obsidian it is impossible 
for a barrier synchronisation to appear within conditional code. Conditionals can be introduced 
either by a conditional expression or a {\tt Cond} operation in the program datatype. The Cond 
operation chooses between two thread programs and the synchronisation operation is at the block 
level in the hierarchy. So a synchronisation cannot occur in conditional code. 
%f it is safe or 
%not to call some function at a certain place in your code, can only be decided by inspecting 
%that functions implementation. 
\end{itemize} 

%% In the book ``Structured Parallel Programming''~\citet{STRUCTURED}, there is a list showing language 
%% support for various parallel patterns. In figure~\ref{fig:caps} this list augmented with a column for Obsidian. 
%% This figure is added to indicate relative expressive power of the different languages. The most 
%% interesting comparison is between Obsidian and OpenCL that both have a programming model geared towards
%% GPUs; even though OpenCL can compile for both CPU and GPU execution. The paragraphs below 
%% take a closer look at well Obsidian performes in the patterns. 

%% \subsubsection{Map} 

%% Map, to a  functional programmer is a higher order function that takes a pure 
%% function and applies it to all elements of a collection (for example a list). 
%% That meaning of map is easily implementable in Obsidian on both pull and push arrays. 
%% Obsidian is also capable, via its parallel for loops, to map sequential computations 
%% over threads in a block and block computation over many blocks. This is probably 
%% more in the spirit of what the authors refer to in~\citet{STRUCTURED}. 

%% \subsubsection{Stencil} 

%% Stencil computations can be expressed in Obsidian, with some manual labour. 
%% A stencil computation would have the form of a pull array to pull array function. 
%% Pull arrays allow reading at position. 
%% Stencil operations where more than one output position is touched could be 
%% implemented as a pull array to push array function.
%% One problem with stencil computations is the borders. If every thread needs to 
%% execute a conditional, checking if it is at the border, performance would suffer. 
%% Using Obsidian the programmer would likely generate more than one kernel; a fast 
%% kernel for the internal points and one or more border kernel. 

%% \subsubsection{Reduction and scan} 
%% Reductions can easily be implemented in Obsidian and with very good performance. 
%% Obsidian also seems particularly well suited for experimenting with the kind 
%% of trade offs that are interesting when implementing reduction kernels. 

%% Scans can also be implemented and the code is elegant. 

%% \subsubsection{Pack and expand} 




%% \begin{figure}[!ht]
%% \begin{small} 
%% \begin{tabular}{|l|l|l|l|l|l|l|}
%% Parallel Pattern & TBB & Cilk Plus & OpenMP & ArBB & OpenCL & Obsidian \\
%% \hline 
%% Parallel Nesting & F   & F         &        &      &        &          \\
%% Map              & F   & F         & F      & F    & F      & I        \\
%% Stencil          & I   & I         & I      & F    & I      & I        \\ 
%% Workpile         & F   &           &        &      & I      &          \\
%% Reduction        & F   & F         & F      & F    & I      & I        \\ 
%% Scan             & F   & I         & I      & F    & I      & I        \\ 
%% Fork-Join        & F   & F         & I      &      &        &          \\ 
%% Recurrance       &     & P         &        &      &        &          \\ 
%% Superscalar seq. &     &           &        &      & F      &          \\

%% Pack             & I   &  I        &  I     & F    & I      & I        \\ 
%% Expand           & I   &  I        &  I     & I    & I      & *        \\
%% Pipeline         & F   &  I        &  I     &      &        &          \\
%% Geometric decomp. & I  &  I        &  I     & I    & I      & I        \\
%% Search           &  I  &  I        &  I     & I    & I      & I        \\
%% Category reduction & I &  I        &  I     & I    & I      & I        \\
%% Gather           & I   &  F        &  I     & F    & I      & I        \\
%% Atomic scatter   & F   &  I        &  I     &      & I      & I        \\
%% Permutation scatter & F & F        &  F     & F    & F      & I        \\ 
%% Merge scatter    &  I  &  I        &  I     & F    & I      & *        \\
%% Priority scatter &     &           &        &      &        & *        \\

%% \end{tabular}
%% \end{small}
%% \label{fig:caps}
%% \caption{This figure lists what parallel patterns a number of languages support. The list 
%% is copied from ``Structured Parallel Programming''~{\protect\citet{STRUCTURED}} and augmented with a 
%% column for Obsidian. \\
%% {\bf Key:} F - means the pattern is supported directly with an abstraction. P - indicates 
%% that the pattern can be implemented in terms of other patterns appearing below. I - means 
%% that the pattern can be easily implemented. Lastly, (*) indicates some patterns 
%% that can be implemented in Obsidian but currently not efficiently so.} 
%% \end{figure}


%% In the technical report ``The Landscape of Parallel Computing Research: A View 
%% from Berkeley''~\citet{dwarfs}, 13 computational kernels (called dwarfs) of importance is identified. 
%% The paper also proposes that we use these 13 dwarfs to evaluate parallel programming 
%% models and languages. The dwarfs are listed below, together with thoughts on how 
%% well they can be tackled by Obsdian. 

%% \begin{itemize} 
%% \item {\bf Dense Linear Algebra:} Obsidian works well for expressing vector-vector 
%% operations. It is also possible to express operations such as matrix multiplication, 
%% even though Obsidians support for multidimensional arrays is limited. It is possible 
%% to use pull arrays of pull arrays, but not push arrays of push arrays. 

%% \item {\bf Sparse Linear Algebra:} 

%% \item {\bf Spectral Methods (FFT):} FFTs are implemented in a way that is 
%% similar to sorting networks. A so-called butterfly network is used. We know 
%% that we can express these network structures in Obsidian. 

%% \item {\bf N-Body Methods:}  

%% \item {\bf Structured Grids:} Regular grids should not be a problem for Obsidian. 
%% Obsidian has no built in support for stencils, the combination of pull and push array 
%% still provides the capability to express such computations. 

%% \item {\bf Unstructured Grids:} May be hard problem for GPUs in general. The paper 
%% states that these problems have the property that ``Updates typically involve multiple 
%% levels of memory reference indirection''. We know that the GPU thrives when memory access 
%% patters are predictable and regular, which is the opposite scenario. 



%% \end{itemize} 

\FloatBarrier 
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
%
% ArBB work
%
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
\section{Embedding Intel Array Building Blocks} 

Intel ArBB~\citet{ARBB2011} is a system for high-level data-parallel programming with the 
ability to generate code for a variety of different hardware configurations. It is implemented 
as an embedded language in C++. One motivation for ArBB that is mentioned in~\citet{ARBB2011} is 
that while high-performance computing specialists are highly competent at implementing 
kernels using low-level programming techniques, mainstream developers are not. ArBB offers 
a more composable way to write programs that make use of core and vector (SIMD) parallelism and 
doing this while using a familiar language, C++.  

One of the greatest strengths of ArBB, as I see it, is that it also comes with a low-level C 
interface. The main purpose of this C interface is to make it easier to use the ArBB system 
from other languages. Many languages have foreign function interfaces that are geared towards C.
This gives ArBB a level of language independence on top of its cross platform abilities.  


ArBB is based on a set of parallel primitives (or structures) on dense one, two and three dimensional vectors and nested vectors: 
\begin{itemize} 
\item {\bf Reductions:}   {\tt add\_reduce}, {\tt mul\_reduce}, {\tt max\_reduce} ... 
\item {\bf Scans:}        {\tt add\_scan}, {\tt mul\_scan}, {\tt max\_scan} ... 
\item {\bf Sorting:}      {\tt sort} 
\item {\bf Permutations:} {\tt gather}, {\tt scatter}, {\tt reverse} ...
\item {\bf Sequential loops:} {\tt \_for}, {\tt \_while} 
\item {\bf Map, zipWith, stencil:}  {\tt map} 
\end{itemize}

These operations (and many more, there are about 120 operations in total) can be used 
by the programmer when writing programs using ArBB. Note that there are sequential looping 
constructs with special names, {\tt \_for} and {\tt \_while}. ArBB is a deeply embedded 
language and calling the ArBB functions does not actually execute them, it just ads a node to 
some internal representation. Therefore, the use of normal C++ for loops leads to unrolled 
ArBB programs and a special {\tt \_for} loop is needed to get a loop in the generated program. 
This is exactly the way these things work in Haskell embedded languages, but it may be even 
more surprising to a C++ programmer. Before an ArBB function can be computed it needs to be 
captured. For this, ArBB has a {\tt call} function that is used to run functions using ArBB 
functionality. The first time {\tt call} is used on a function pointer that function is 
executed and the ArBB functions used in it build an AST (we say the function has been 
captured). The AST is then compiled and specialised for the hardware available. Any 
subsequent calls of a function using ArBB functionality do not lead to a recompilation, 
the first call caches this compiled code. 

The first step of our work with embedding Intel ArBB functionality was to implement very direct 
and low-level bindings to the C interface. This provides a Haskell function (in the IO Monad) 
for each of the functions in the ArBB C interface. More details and results of this work 
can be found in paper G in section~\ref{sec:paperG}.

%\noindent\emph{What does ArBB do, what are the operations and datastructures it support} 

%\noindent\emph{What does EmbArBB do} 

\subsection{EmbArBB: Embedding ArBB in Haskell}
\label{sec:EmbArBB} 

The ArBB Haskell bindings provide a very rudimentary interface to ArBB functionality and 
is not useful for application implementation. A higher-level interface to ArBB to offer 
the Haskell programmer is needed. Our first attempt was to implement an Accelerate backend 
using the ArBB bindings. However, this had some hard problems to solve. Accelerate is a 
more richer language and there was an API mismatch between Accelerate and ArBB. These problems 
are mentioned in in paper G, section~\ref{sec:paperG}. As a way to offer ArBB functionality 
to the Haskell in a way that is useful and simpler to implement, we start working on EmbArBB. 

With EmbArBB we improve the interfacing with Haskell by implementing a deeply embedded language. 
Now, programs in EmbArBB create ASTs. These ASTs are compiled via the ArBB bindings creating 
function objects that can then be run. 

\subsubsection{Dense and nested arrays}

ArBB supports one, two and three dimensional arrays, called dense vectors. In EmbArBB we 
represent these with a data type, {\tt DVector}, parameterised by dimensionality and 
element type. The dimension parameter can be either {\tt Dim1}, {\tt Dim2} or {\tt Dim3}. 
There are also nested vectors, in EmbArBB called {\tt NVector}, parameterised on element type. 

\subsubsection{Operations on arrays} 

EmbArBB provides many of the ArBB operations on vectors but provides a 
shape-polymorphic interface to them. For example, a reduction operation can take a two  
dimensional vector and reduce all rows or columns and provide a one dimensional result. Or 
it takes a one dimensional vector and returns a single value (zero dimensional vector). 

\begin{verbatim} 
addReduce :: Num a 
           => Exp USize 
           -> Exp (DVector (t:.Int) a) 
           -> Exp (DVector t a) 
\end{verbatim} 

The {\tt addReduce} function takes an input vector that is at least one dimensional 
({\tt t:.Int}) and returns a vector that has one less dimension ({\tt t}).
 
\subsubsection{Deep embedding}

EmbArBB is a traditional deep embedding; there is an AST data type with constructors 
representing each ArBB language feature. It is true that a deep embedding of ArBB 
leads to duplication of effort. The low-level operations exported by the 
ArBB C interface already construct an AST within the ArBB system. One possible benefit 
of the deep embedding is that it buys independence from the ArBB backend. It is possible 
to compile the AST generated on the Haskell side to some other target language. 

When compiling EmbArBB to ArBB a sharing detection pass is performed. This pass detects 
sharing in the AST using the method of A. Gill~\citet{Gill}. One benefit of this pass 
is that it reduces the numbers of calls into the ArBB C library. The efficiency of code 
generated is probably not affected since ArBB would discover that sharing in a Common 
Subexpression Elimination (CSE) pass. Another potential benefit is that sharing detection 
on the Haskell side results in a simpler AST being built on the ArBB side; potentially making 
the job for the ArBB optimisation passes simpler. What effects these 
operations truly had on performance has not been investigated. Actually, applying a sharing 
detection technique was one of my personal motivations for working on EmbArBB. In Obsidian 
we have not seen the need for this technique.

One benefit of a deep embedding is that transformations can be performed on the AST. Applying 
any optimisations in EmbArBB would very likely duplicate effort already put into ArBB. 
Therefore we apply no further transformations after sharing detection. However, if EmbArBB 
would be used to generate code in some other target language a set of optimisations 
would be needed.  

\subsubsection{Evaluation} 

In paper H, section~\ref{sec:paperH}, we benchmark EmbArBB against ArBB in C++ and the Haskell 
Repa library. EmbArBB compares favourably to Repa and is often a lot faster. EmbArBB and the ArBB 
C++ performance is not distinguishable from EmbArBB's indicating that our Haskell embedding 
adds no extra overhead. Of course this is expected, since once the ArBB system has generated 
the code, that code is identical no matter if the AST was built via Haskell or C++ calls. 
Missing is a comparison of how well EmbArBB and C++ compares in regards to the time it actually 
takes to go through the process of making those ArBB C API calls. However, if the generated 
code is used many times the caching ArBB does of generated code means that the initial code 
generation cost can be amortised. 

\subsection{Current status of ArBB and EmbArBB} 

Since our work with ArBB, Intel has unfortunately retired the ArBB system. This 
leaves EmbArBB without a functioning code generating backend. This makes the choice of a
deep embedding (in hindsight) the correct one. EmbArBB could be resurrected by implementing 
a new code generating backend. 


% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
%
% Related work
%
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
\section{Related work} 
\label{sec:relatedwork}
\FloatBarrier


\subsection{Languages for data-parallel programming}

In figure~\ref{fig:researchmatrix}, our work is placed in relation to other languages,
embedded languages and libraries for parallel programming. The languages and libraries considered 
are data-parallel; languages for control parallelism or distributed message passing systems are 
not considered. The systems are roughly divided 
into three groups, based on their level of abstraction. There are low-level languages; here 
I place languages that are imperative and C-like. In the middle layer I place languages 
that have higher level abstractions and are more easily composable. 
at the highest level, I place languages that completely abstract away from details of the 
hardware they run on. The division is not free from personal bias and the placement of languages
in boxes was not always easy. 

The figure also divides the languages according to what hardware they support. There 
are CPU specific languages, GPU specific languages and languages that support or aspire 
to support CPU,GPU and accelerator (Larrabee, Xeon Phi) execution. 

% ---------------------------------------------------------------------------
% Obsidian, Accelerate, Nikola, Thrust, CUB 
% ---------------------------------------------------------------------------
With Obsidian, we try to target the sparsely occupied area of mid-level GPU programming. With 
CUDA~\citet{wwwcuda} and OpenCL~\citet{OpenCL}, the programmer has full control of how to divide 
the computation amongst threads and blocks. Using Accelerate~\citet{ACCELERATEDAMP11}, 
Nikola~\citet{NIKOLA} 
and Thrust~\citet{THRUST} the programmer would use 
high-level patterns without direct insight into how the application would be mapped onto 
threads and blocks of the GPU. Using Obsidian, we want to bridge that gap by both being 
more composable than CUDA and giving the programmer control of what the computation will 
look like on the GPU. This is done by allowing the programmer to specify and compose 
thread-level and block-level code. Recently, the CUB library arrived that is built on a 
similar idea, that GPU programmers need to have block/thread level control to get maximum 
performance, while maintaining a higher level and more Thrust-like programmer 
interface~\citet{CUB}. Before CUB, and as far as I am aware, we were alone at targeting 
this level.  

% ---------------------------------------------------------------------------
% ArBB EmbArBB
% ---------------------------------------------------------------------------
The ArBB and EmbArBB systems also provide a set of parallel primitives that the programmer
composes to build his application. I place the ArBB and EmbArBB in a box slightly lower than 
Accelerate since while ArBB/EmbArBB also have built in reduction primitives, just like Accelerate, 
they are not general primitives. Where Accelerate has a single higher order reduction 
operation, ArBB/EmbArBB has {\tt reduce\_add, reduce\_mul} and so on. Benefits of 
having ArBB embedded in Haskell compared to C++ are discussed in section~\ref{sec:EmbArBB}.
%I will go into 
%what benefits there are to having ArBB embedded in Haskell compared to C++ in 

% ---------------------------------------------------------------------------
% Microsoft Accelerator
% ---------------------------------------------------------------------------
Microsoft Accelerator~\citet{ACCELERATOR}, is a language embedded in C\#. Accelerator has
data-parallel arrays and a set of aggregate operations (operations on entire arrays). The 
operations on data-parallel arrays are JIT-compiled to GPU code. Accelerator can also 
compile to multithreaded CPU code and make use of SIMD units of modern 
CPUs~\citet{ACCELERATORCPU}; this makes the Accelerator system related to ArBB. 

% ---------------------------------------------------------------------------
% Python systems
% ---------------------------------------------------------------------------
There are many embedded languages implemented in Python that makes use of GPUs 
to improve performance. One example is Copperhead~\citet{copperhead}. Copperhead 
supports flat and nested data-parallelism via primitives such as {\tt map} and 
{\tt reduce}.  

% ---------------------------------------------------------------------------
% Repa, Meta-Repa 
% --------------------------------------------------------------------------- 

Repa (Regular Shape-polymorphic Arrays)~\citet{REPA}, is a Haskell library for 
data-parallel programming. 
Repa uses the same array representation as Obsidian, in Repa called a delayed
array, which is a function from index to element. However, in Repa arrays have shape; 
they can be of any dimensionality and there are shape-polymorphic functions on these. 
For example, {\tt map} can be applied to an array of any shape. 

Meta-Repa~\citet{METAREPA} is a reimplementation of the Repa library using deep embedded language 
techniques. Meta-Repa programs build ASTs that are then compiled (using template 
Haskell) to Haskell code using low-level parts of the Repa library for its parallelism.
The Meta-Repa approach to implementing the Repa library 
gives inlining of operations for free, where Repa programs need to be annotated 
with pragmas to ensure inlining. On the other hand, in Meta-Repa the programmer 
needs to explicitly state when something should not be inlined (using a {\tt force} primitive).
This is related to our work on EmbArBB that also uses an existing library (ArBB) 
for its parallelism. In reference~\citet{FPCDSL}, a case is made for this style 
of embedding prexisting DSLs. 


% ---------------------------------------------------------------------------
% DPH, NESL 
% ---------------------------------------------------------------------------
Data-Parallel Haskell~\citet{DPH} and Nesl~\citet{NESL} are languages for 
nested data-parallelism. Both run on CPUs, but there is also work towards 
a GPU version of Nesl~\citet{NestedGPU}. 

% ---------------------------------------------------------------------------
% Delite, Vertigo 
% ---------------------------------------------------------------------------

Delite~\citet{DELITE} is an infrastructure for implementation of domain 
specific embedded languages. There is an extensible internal representation (IR,AST); 
the programmer extends some base IR with domain specific constructs. The Delite system 
has been used to implement domain specific embedded languages for machine learning, rendering, 
physics and other domains. Delite, being a framework and the languages implemented using it 
very domains specific, was impossible to place in the matrix (figure~\ref{fig:researchmatrix}). 

\subsection{Embedded domain specific languages} 

Feldspar is an embedded language for digital signal processors~\citet{FELDSPAR}. Apart 
from using the syntactic library~\citet{SyntacticICFP12}, it is quite similar to Obsidian. 
There is a deeply embedded core language (called Core) and vectors as a shallow embedding 
on top of that, just like in Obsidian.

When we first started working on Obsidian, we used Lava~\citet{LavaICFP} as inspiration. 
We wanted to mimic the Lava programming style but generate GPU code. At first this worked quite 
well and most Obsidian programs superficially looked quite like Lava programs. We have diverged 
from this aspiration since realising that GPUs are quite a different story and performance 
requires more control of GPU architecture specifics. 

\subsection{Compiling Embedded Languages}

In paper A~(section \ref{sec:paperA}) we describe our method for compiling monadic embedded 
languages. 
This means to take a monadic computation and observe its structure in order to generate
code. This work is closely related to a method described in ``The Constrained Monad Problem''
by Sculthorpe and Gill~\citet{sculthorpe2013constrained}. In that paper they solve a more 
general problem than just compilation, but our method is nice because it is easy to understand 
and solves a single problem and does it well. In ``Generic monad constructs'' Persson et al 
also solves the problem of compiling monads using a continuation monad. 

``Compiling Embedded Languages'' by Elliott and Finne has been a constant source of information 
and something we have refered to in almost all our work. This is, in my opinion, the paper to turn 
to for anyone starting out with compiled embedded languages. 

\begin{figure}
\begin{center}
\begin{tikzpicture}[align=center, scale=2.5]
% \draw[help lines] (0,0) grid (3,3);


\draw[->] (-0.5,0) -- (-0.5,3) node[anchor=east] {Abstraction level};

\draw (0,3) node[anchor=south west] {CPUs};
\draw (1,3) node[anchor=south west] {GPUs};
%\draw (2,3) node[anchor=south west] {Accelerator};
\draw (2,3) node[anchor=south west] {CPUs, GPUs\\ Accelerators};

\node[anchor=west] (acc) at (1,2.7) {Accelerate\\Nikola\\Thrust};
\node[anchor=west] (acc) at (1,1.7) {CUB};
\node[anchor=west] (repa) at (0,2.6) {DPH\\Nesl\\Repa\\Meta-repa};
\node[anchor=west] (mixed) at (0,1.7) {Cilk+\\ TBB};
\node[anchor=west] (accelerator) at (2,1.7) {Accelerator};

\node[anchor=west] (cuda) at (1,0.3) {CUDA};
\node[anchor=west] (opencl) at (2,0.3) {OpenCL};

\node[anchor=west] (OpenMP) at (0,0.3) {OpenMP\\PThreads};

\foreach \x in {0,...,2} { 
  \foreach \y in {0,...,2} { 
    \draw (\x,\y) rectangle (\x+1,\y+1);
  }
}

\draw[red!75,rounded corners,ultra thick] (1,0.4) rectangle (2,2.4);
\node[red!75,rotate=45,thick] (obs) at (1.5,1.5) {Obsidian};

\draw[orange!90,rounded corners,ultra thick] (2,1.4) rectangle (3,2.6);
\node[orange!90,thick] (arbb) at (2.5,2) {EmbArBB\\ArBB};


\end{tikzpicture}
\end{center}
\caption{Placing our work in the landscape of languages and libraries for parallel 
general purpose programming of CPUs, GPUs or both. }
\label{fig:researchmatrix} 
\end{figure}



%\subsection{Parallelism} 

%\noindent\emph{DPH}\citet{DPH} \newline
%\noindent\emph{D.A.Accelerate}\citet{ACCELERATEDAMP11} \newline
%\noindent\emph{Nikola}\citet{NIKOLA}\newline
%\noindent\emph{Repa}\citet{REPA}\newline
%\noindent\emph{Meta-Repa}\citet{METAREPA}\newline
%\noindent\emph{ArBB}\citet{ARBB2011}\newline
%\noindent\emph{OpenCL}\citet{OpenCL}\newline
%\noindent\emph{CUDA}\citet{CUDA}\newline
%\noindent\emph{Microsoft Accelerator}\citet{ACCELERATOR}\newline
%\noindent\emph{Copperhead}\citet{copperhead}\newline
%\noindent\emph{Delite}\citet{DELITE}\newline
%\noindent\emph{A monad for deterministic parallelism (R. Newton)} \citet{MonadPar} \newline
%\noindent\emph{Nesl}\citet{NESL} \newline

%\subsection{Embedded domain specific languages} 



\FloatBarrier
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
%
% Future work
%
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
\section{Future work} 

Obsidian is a language for GPU kernel implementation. It is not as well integrated 
into Haskell as for example Accelerate. There are ways to execute Obsidian kernels 
from within Haskell; the recommended workflow, however, is to generate the CUDA  
as text and write the host code by hand. This can be seen as both a shortcoming 
and a feature. It is hard to integrate Accelerate programs in larger applications 
written in other languages; when using Obsidian a kernel is generated and 
can be used as part of for example a library. 

Obsidian is also a quite low-level language. The programmer needs to know GPU details 
and is allowed to write programs at a level almost as low as CUDA, if it is desired. 
Paper E (section~\ref{sec:paperE}) contains examples of programming at this low level. 
One potential use of Obsidian could be as the code generating backend of high-level 
and very domain specific embedded languages. This could be a niche for Obsidian.  

Here and there in the thesis it has been hinted that Obsidian has warp level 
capabilites and that these are not as integrated into the hierarchy as the block, 
thread and grid levels are. The warp abstractions offered also requires more work in 
the code generating backend than the others, that correspond directly to CUDA concepts. 
The problem is that CUDA does not have a warp abstraction. Warps needs to be 
better integrated with the rest of Obsidian and the code generation required 
for warps needs more testing before being trustworthy and useful. 

%\noindent\emph{Improve on Warp level capabilities} 

%\noindent\emph{Improve expressivity, code performance} 

% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
%
% Discussion
%
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------

\section{Discussion} 

\vspace{5mm} 

\emph{This section will revised.} 

\vspace{5mm}

This thesis contains two approaches to embedding a language for data-parallel programming 
in Haskell. EmbArBB embeds a preexisting library for data-parallelism in Haskell 
and at little effort from us, very good performance is gained. The performance of 
EmbArBB is entirely due to all the optimisation expertice and effort that was invested in 
the ArBB system. We get all that expertice for free. What EmbArBB provides is a 
way for Haskellers to benefit from the ArBB system. ArBB and EmbArBB are based on a 
set of parallel primitives for the programmer to use and compose. Obsidian takes 
a different approach. Built in primitives are very few and very low level, such as 
sequential and parallel for loops. On top of the low-level capabilities, a library 
of high-level primitives can be built. The key is that we want the programmer to 
be able to specify how to implement these higher level abstractions. 

Looking at the related work, we see that almost all high level GPU programming 
languages provide abstractions of parallel primitives, aggregate operations, 
maps, folds and reductions, giving justification to our investigation of GPU 
programming without these. We believe that the GPU architecture is still volatile 
enough that there be no ``the right'' way of implementing these primitives. As of 
today, the programmer still needs to explore the design space and find a solution 
that performs well on a particular GPU. 

Performance of Obsidian kernels is often good and the Obsidian code that they 
are generated from is often shorter than corresponding CUDA programs. Obsidian 
programs differ from CUDA programs by describing whole programs. With this I mean 
that a CUDA programmer needs to take a leap from the single threaded program 
parameterised on thread Id, to what actually will happen when this program is 
run by hundreds or thousands threads at once. The extensive indexing arithmetic 
often present in CUDA programs can make this leap a hard one. Similar programs 
in Obsidian could use operations on whole arrays such as {\tt map} or explicitly 
bounded parallel loops. 

Push arrays were invented by Koen Claessen and first implemented in Obsidian 
to provide more control over where computation takes place (which thread computes 
which element). Push arrays has since been experimented on by others and show up 
in Nikola~\citet{NIKOLAPUSH} and Feldspar. Push arrays appear in StreamHS~\citet{FPCDSL} 
and ~\citet{MOA} as part of a multi-dimensional array calculus in Standard ML. 


% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
%
% Thesis overview
%
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
\section{Thesis overview} 

%This thesis contains an introduction and a selection of papers. The introduction 
%gives background and context to the papers. 

The papers contained in this thesis fall in three categories. 
\begin{itemize} 
\item General EDSL methodology in chapter~\ref{chap:EDSLImplementation}. 
\item GPU Programming using an EDSL in chapter~\ref{chap:GPUProgramming}. 
\item Retargetable parallel programming in chapter~\ref{chap:ArBB}. 
\end{itemize} 
%In the following sections, each paper is given a short description. Hopefully 
%there is enough information to wet the appetite. 

\subsection{EDSL implementation papers} 

\subsubsection{\paperA: \\ \paperATitle} 

Authors: Josef Svenningsson and Bo Joel Svensson 

\vspace{5mm}

\noindent This paper describes a simple and compositional method for 
reification of monads. This is useful for compilation of 
monadic embedded languages. We use a simple robot control language 
as the example. Robot programs can be expressed using Haskell do notation 
and is compiled down to a simple first order program representation. 
There is also a graphical simulator available that runs the compiled 
code. Robot language and simulator is available at github: \url{github.com/svenssonjoel/Robot}.

\subsection{GPU programming papers} 

\subsubsection{\paperB: \\ \paperBTitle}

Authors: Bo Joel Svensson, Mary Sheeran and Koen Claessen \newline

\vspace{5mm}

The first real paper about Obsidian, our embedded language for GPU 
programming. In this paper we identify similarities between 
connection patterns in hardware and parallel computations on GPUs. 
We introduce a structured and compositional way to write data-parallel 
programs on a GPU.

Basic parallel building blocks are implemented in a monadic style and 
composed using a sequential composition operator {\tt ->-}. 

\subsubsection{\paperC: \\ \paperCTitle}

Authors: Bo Joel Svensson, Koen Claessen and Mary Sheeran \newline

\vspace{5mm}

In this paper we describe a version of Obsidian implemented with a 
a different internal representation of GPU programs. We realised that 
most of our kernels have similar shape, they are parallel operations 
composed in sequence interspersed with barrier synchronisations. The 
internal representation used here captures this case exactly. Programming 
is now done is a style resembling arrows. 


\subsubsection{\paperD: \\ \paperDTitle}

Authors: Koen Claessen, Mary Sheeran and  Bo Joel Svensson \newline

\vspace{5mm}

This paper introduces {\em Push} arrays, invented by Koen Claessen. This 
array representation target specific performance problems that we want 
to solve with Obsidian. Here we have also gone back to using monadic 
style for programming in Obsidian. 


\subsubsection{\paperE: \\ \paperETitle} 

Authors: Josef Svenningsson, Bo Joel Svensson and Mary Sheeran \newline

\vspace{5mm}

This paper contains a sorting case study performed in Obsidian. It 
is however the algorithms that are in focus here. We describe 
an interesting variation on Counting sort that removes duplicate 
elements and is a nice fit for GPUs because of its little need for 
synchronisation. 

In order to implement these sorting algorithms we added atomic operations 
to Obsidian. 

\subsubsection{\paperF: \\ \paperFTitle}

Authors: Bo Joel Svensson and Mary Sheeran \newline
\noindent \emph{This work has not yet been published.}
\vspace{5mm} 

\noindent 

In this paper the most recent additions and improvements of Obsidian are 
described. Obsidian Programs are now parameterised on Hierarchy level. This forces
the programmer into writing programs that we know how to compile to a GPU easily 
(by limiting how parallel operations can be nested). The paper also gives a detailed  
optimisation story using Obsidian for implementing reduction kernels. 


\subsection{Retargetable parallel programming} 

\subsubsection{\paperG: \\ \paperGTitle}

Authors: Bo Joel Svensson and Ryan R. Newton

\vspace{5mm}

\noindent This position paper describes the work I did while on a 3 month internship at 
Intel. The main part of this work was implementation of Haskell bindings 
for the now retired Intel ArBB system. 

The paper also describes work we did towards implementing a backend for the 
Accelerate system using ArBB. We reached a proof-of-concept implementation 
capable of running some simple Accelerate programs using the ArBB backend.
However, much work would remain to be able to run the full scope of Accelerate 
programs on ArBB.  

\subsubsection{\paperH: \\ \paperHTitle}

Authors: Bo Joel Svensson and Mary Sheeran 

\vspace{5mm}

\noindent Here we explore another way to provide the capabilities of ArBB to the 
Haskell programmer. Rather than trying to use ArBB as a backend to Accelerate 
we provide a more direct mapping of ArBB's functionality to Haskell idioms. 
We call this embedded language EmbArBB and it provides a more Haskell-programmer-friendly 
interface compared to the raw ArBB bindings (which are a collection of functions 
in the IO monad). 

% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
%
% BIBLIOGRAPHY
%
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------

\bibliographystylet{alpha}
\bibliographyt{thesis}
%\addcontentsline{toc}{section}{Bibliography}




\clearpage{}

% --------------------------------------------------------------------------- %
% --------------------------------------------------------------------------- %
%
%  PAPERS 
%
% --------------------------------------------------------------------------- %
% --------------------------------------------------------------------------- %


\chapter{EDSL Implementation Papers}
\label{chap:EDSLImplementation}
% ---------------------------------------------------------------------------
% 
% ---------------------------------------------------------------------------
\cleardoublepage 


\section[\paperATitle]{\paperA: \\ \paperATitle}
\label{sec:paperA}
%\addcontentsline{toc}{chapter}{Simple and Compositional Reification of Monadic Embedded Languages}

% \paperATitle

\begin{center} 
Josef Svenningsson, Bo Joel Svensson
\end{center}


\input{./bb/paperThesis}



\chapter{GPU Programming Papers}
\label{chap:GPUProgramming}

% ---------------------------------------------------------------------------
% 
% ---------------------------------------------------------------------------
\cleardoublepage 

\section[\paperBTitle]{\paperB: \\ \paperBTitle}
\label{sec:paperB}
%\addcontentsline{toc}{chapter}{Obsidian: A Domain Specific Embedded Language for
%Parallel Programming of Graphics Processors}

%\paperBTitle

\begin{center} 
Bo Joel Svensson, Mary Sheeran, Koen Claessen
\end{center}

\input{./ifl/paperThesis}

% ---------------------------------------------------------------------------
% 
% ---------------------------------------------------------------------------
\cleardoublepage 

\section[\paperCTitle]{\paperC: \\ \paperCTitle}
\label{sec:paperC}
%\addcontentsline{toc}{chapter}{Obsidian: GPU Computing Using Haskell}

%\paperCTitle

\begin{center} 
Bo Joel Svensson, Koen Claessen, Mary Sheeran
\end{center}

\input{./papp/paperThesis}


% ---------------------------------------------------------------------------
% 
% ---------------------------------------------------------------------------
\cleardoublepage 

\section[\paperDTitle]{\paperD: \\ \paperDTitle}
\label{sec:paperD}
%\addcontentsline{toc}{chapter}{Expressive Array Constructs in an Embedded GPU Kernel Programming Language}

%\paperDTitle

\begin{center} 
Koen Claessen, Mary Sheeran, Bo Joel Svensson
\end{center}

\input{./expressive/paperThesis}

% ---------------------------------------------------------------------------
% 
% ---------------------------------------------------------------------------
\cleardoublepage 

\section[\paperETitle]{\paperE: \\ \paperETitle}
\label{sec:paperE}
%\addcontentsline{toc}{chapter}{Counting and Occurrence Sort for GPUs using an Embedded Language}

% \paperETitle

\begin{center} 
Josef Svenningsson, Bo Joel Svensson, Mary Sheeran 
\end{center}


\input{./csort/paperThesis}


% ---------------------------------------------------------------------------
% 
% ---------------------------------------------------------------------------
\cleardoublepage 


\section[\paperFTitle]{\paperF: \\ \paperFTitle}
\label{sec:paperF}


%\addcontentsline{toc}{chapter}{Simple and Compositional Reification of Monadic Embedded Languages}

% \paperFTitle

\begin{center} 
Bo Joel Svensson, Mary Sheeran
\end{center}


\begin{center}
{\large \bf This paper will be revised before printing the Thesis} 
\end{center}


\input{./hl/paperThesis}



% ---------------------------------------------------------------------------
% 
% ---------------------------------------------------------------------------

\chapter{Retargetable Parallel Programming Papers}
\label{chap:ArBB}
% ---------------------------------------------------------------------------
% 
% ---------------------------------------------------------------------------
\cleardoublepage 


\section[\paperGTitle]{\paperG: \\ \paperGTitle}
\label{sec:paperG}
%\addcontentsline{toc}{chapter}{Parallel Programming in Haskell Almost for Free}

% \paperGTitle 

\begin{center} 
Bo Joel Svensson, Ryan R. Newton
\end{center}


\input{./arbb/paperThesis}

% ---------------------------------------------------------------------------
% 
% ---------------------------------------------------------------------------
\cleardoublepage 


\section[\paperHTitle]{\paperH: \\ \paperHTitle}
\label{sec:paperH}
%\addcontentsline{toc}{chapter}{Parallel Programming in Haskell Almost for Free}

% \paperHTitle

\begin{center} 
Bo Joel Svensson, Mary Sheeran
\end{center}


\input{./embarbb/paperThesis}


\cleardoublepage


% ---------------------------------------------------------------------------
% nocites 
% ---------------------------------------------------------------------------
\nocite{*}



\makeatletter
\renewenvironment{thebibliography}[1]
     {\chapter*{\bibname}%
      \@mkboth{\MakeUppercase\bibname}{\MakeUppercase\bibname}%
      \list{\@biblabel{\@arabic\c@enumiv}}%
           {\settowidth\labelwidth{\@biblabel{#1}}%
            \leftmargin\labelwidth
            \advance\leftmargin\labelsep
            \@openbib@code
            \usecounter{enumiv}%
            \let\p@enumiv\@empty
            \renewcommand\theenumiv{\@arabic\c@enumiv}}%
      \sloppy
      \clubpenalty4000
      \@clubpenalty \clubpenalty
      \widowpenalty4000%
      \sfcode`\.\@m}
     {\def\@noitemerr
       {\@latex@warning{Empty `thebibliography' environment}}%
       \endlist}
\makeatother

\bibliographystyle{alpha}
\bibliography{thesis}
\addcontentsline{toc}{chapter}{Bibliography}

\end{document}
