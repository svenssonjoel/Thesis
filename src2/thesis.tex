\documentclass[a4paper]{book}

%\usepackage[latin9]{inputenc}
\usepackage[utf8x]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{pdfpages}
\usepackage[british]{babel}
\usepackage{placeins}
\usepackage{dialogue}
\usepackage{fancyvrb}

\usepackage{t1enc}         
\usepackage{times}
\usepackage{url}
\usepackage{enumerate}

%\usepackage{code}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
%\usepackage{lingmacros}
\usepackage{subfig}
\usepackage{fancyhdr}


\pagestyle{plain}
%\pagenumbering{arabic}


%stuff
\usepackage{latexsym}
\usepackage{tikz,pgflibraryshapes}
\usetikzlibrary{shapes,snakes,positioning,matrix}
\usepackage{pgfplots}
\usepackage{booktabs}
\usepackage{enumerate}
%\usepackage{etoolbox}


\usepackage{multibib} 
\newcites{bb}{Bibliography} 
\newcites{ifl}{Bibliography}
\newcites{papp}{Bibliography}
\newcites{exp}{Bibliography}
\newcites{hl}{Bibliography}
\newcites{csort}{Bibliography}
\newcites{emb}{Bibliography}
\newcites{arbb}{Bibliography}
\newcites{t}{Bibliography}
%\newcites[ifl}{Bibliography}

%\patchcmd{\thebibliography}{\section*}{\section}{}{}

\makeatletter
\renewenvironment{thebibliography}[1]
     {\section*{\bibname}%
      %\@mkboth{\MakeUppercase\bibname}{\MakeUppercase\bibname}%
      \list{\@biblabel{\@arabic\c@enumiv}}%
           {\settowidth\labelwidth{\@biblabel{#1}}%
            \leftmargin\labelwidth
            \advance\leftmargin\labelsep
            \@openbib@code
            \usecounter{enumiv}%
            \let\p@enumiv\@empty
            \renewcommand\theenumiv{\@arabic\c@enumiv}}%
      \sloppy
      \clubpenalty4000
      \@clubpenalty \clubpenalty
      \widowpenalty4000%
      \sfcode`\.\@m}
     {\def\@noitemerr
       {\@latex@warning{Empty `thebibliography' environment}}%
      \endlist}
\makeatother


%%

%% \newcommand{\bequ}{\begin{quote}}
%% \newcommand{\enqu}{\end{quote}}
%% \newcommand{\eop}[1]{\mbox{\sl #1}}
%% \newcommand{\sube}[2]{#1_{#2}}
%% \newcommand{\id}[1]{{\it #1}}
%% \newcommand{\kwd}[1]    {\mbox{\textbf{#1}}}
%% \newcommand{\str}[1]{{\id "#1"}}
%% \newcommand{\lemon}{\emph{lemon}}
%% \newcommand{\textt}[1]{\texttt{#1}}
%% \newcommand{\txttt}[1]{\texttt{#1}}

%\renewcommand*{\thesection}{\arabic{section}}

\newcommand{\thesistitle}{Embedded Languages for Data-Parallel Programming}
\newcommand{\dept}{Department of Computer Science and Engineering}
\newcommand{\uni}{Chalmers University of Technology and G\"oteborg University}
\newcommand{\group}{Functional Programming Research Group}


% ---------------------------------------------------------------------------
% Papers 
% ---------------------------------------------------------------------------
\newcommand{\paperA}{Paper A}
\newcommand{\paperATitle}{Simple and Compositional Reification of Monadic Embedded Languages}
\newcommand{\paperB}{Paper B}
\newcommand{\paperBTitle}{Obsidian: A Domain Specific Embedded Language for Parallel Programming of Graphics Processors}
\newcommand{\paperC}{Paper C} 
\newcommand{\paperCTitle}{GPGPU Kernel Implementation and Refinement using Obsidian} 
\newcommand{\paperD}{Paper D}
\newcommand{\paperDTitle}{Expressive Array Constructs in an Embedded GPU Kernel Programming Language}
\newcommand{\paperE}{Paper E}
\newcommand{\paperETitle}{Counting and Occurrence Sort for GPUs using an Embedded Language}
\newcommand{\paperF}{Paper F}
\newcommand{\paperFTitle}{A High-Level Embedded Language for Low-Level GPU Kernel Programming}
\newcommand{\paperG}{Paper G}
\newcommand{\paperGTitle}{Programming Future Parallel Architectures with Haskell and Intel ArBB}
\newcommand{\paperH}{Paper H}
\newcommand{\paperHTitle}{Parallel Programming in Haskell Almost for Free}

% ---------------------------------------------------------------------------
% BEGIN ! 
% ---------------------------------------------------------------------------
\begin{document}

\begin{titlepage}
\begin{centering}
{\sc Thesis for the Degree of Doctor of Philosophy}
\vspace{30ex}

{\LARGE\bf\thesistitle}

\vspace{7ex}

\large Bo Joel Svensson
\vfill
\includegraphics[width=120mm]{./img/ChalmGUtextsvEng}\\[5mm]
\includegraphics[height=4cm]{./img/ChalmGUmarke}\\

\vspace{1cm}
\normalsize
{\sc \dept}\\
{\sc \uni}\\
G\"oteborg, Sweden 2013

\end{centering}
\end{titlepage}

% ---------------------------------------------------------------------------
% Tryckortssida
% ---------------------------------------------------------------------------

\quad \vfill

{\noindent\large\bf\thesistitle} \\
\noindent Bo Joel Svensson \\
\noindent ISBN \\

\vspace{1cm}

\noindent\copyright {\sc {Bo Joel Svensson}}, 2013 \\ 
\vspace{1cm} 

\noindent Technical Report 101D\\
\noindent ISSN                 \\
\noindent \dept \\
\noindent \group\\

\vspace{1cm} 

\noindent \uni \\
\noindent SE--412~96~~G\"oteborg, Sweden\\
\noindent Phone: +46 (0)31--772~1000 \\

\vspace{1cm} 

\noindent Printed in Sweden\\
\noindent Chalmers Reproservice\\
\noindent G\"oteborg, Sweden 2013


\thispagestyle{empty}

\clearpage
\pagenumbering{roman}

\section*{Abstract}
Abstract abstract 

\vspace{5mm}

\noindent

\textbf{Keywords:} Data-parallelism, Functional Programming, Embedded languages, GPU

\clearpage

% ---------------------------------------------------------------------------
%  ACKS !!! 
% ---------------------------------------------------------------------------
\section*{Acknowledgments}


\vspace{5mm}
\noindent This research has been funded by the Swedish Foundation for
Strategic Research (which funds the Resource Aware Functional 
Programming (RAW FP) Project) and by the Swedish Research Council.

\clearpage

% ---------------------------------------------------------------------------
%  Guide for Opponent, grading committee
% ---------------------------------------------------------------------------
\section*{Publications}

This thesis includes the following publications: 

\begin{enumerate}[A.] 
\item Josef Svenningsson and Bo Joel Svensson. Simple and Compositional Reification of Monadic Embedded Languages, 2013. \emph{18th ACM SIGPLAN International Conference of Functional Programming, ICFP 2013.}
\item Bo Joel Svensson, Mary Sheeran and Koen Claessen. Obsidian: A Domain Specific Embedded Language for General Purpose Parallel Programming of Graphics Processors. In \emph{Proc. of Implementation and Applications of Functional Languages (IFL)}, Lecture Notes in Computer Science, Springer Verlag, March 2009.
\item Bo Joel Svensson, Koen Claessen and Mary Sheeran. GPGPU kernel implementation and refinement using Obsidian. \emph{Practical Aspects of High-level Parallel Programming, PaPP 2010.} Procedia Computer Science.
\item Koen Claessen, Mary Sheeran and Bo Joel Svensson. Expressive Array Constructs in an Embedded GPU Kernel Programming Language. In \emph{Proceedings of the 7th workshop on Declarative aspects and applications of multicore programming}, DAMP '12. ACM. 
\item Josef Svenningsson, Bo Joel Svensson and Mary Sheeran. Efficient Counting Sort Implementations using an Embedded GPU Programming Language. \emph{2nd Workshop on Functional High-Performance Computing, FHPC '13.}
\item Bo Joel Svensson and Mary Sheeran. A High-Level Embedded Language for Low-Level GPU Kernel Programming. {\bf\emph{This work has not yet been published.}} 
\item Bo Joel Svensson and Ryan Newton. Programming Future Parallel Architectures with Haskell and ArBB. \emph{Future Architectural Support for Parallel Programming (FASPP), in conjunction with ISCA '11.}
\item Bo Joel Svensson and Mary Sheeran. Parallel Programming in Haskell Almost for Free: an embedding of Intel's Array Building Blocks. \emph{Proceedings of the 1st ACM SIGPLAN Workshop on Functional High-Performance computing, FHPC '12.}
\end{enumerate} 

\clearpage
\subsection*{Personal contributions} 

The following describes my personal contributions to the papers included 
in this thesis. 

\subsubsection{Paper A}
I intuitively applied the monad reification method as part of the implementation 
of Obsidian. Josef Svenningsson identified its importance and suggested we write 
about it. The paper has two distinct parts. Sections one and two describe the 
method as I used it. And section four makes the compositional aspect of the method 
explicit. 

The code and examples in section one and two are my work. The contents of section four was 
developed by Josef Svenningsson and me. For section four we also got very much and valuable help 
from Emil Axelsson. 

For the rest of the paper, just substituting my name for Benny and Josef's for Bj\"orn
will give a good indication of our contributions. 

\subsubsection{Papers B, C, D and E} 
The Obsidian language has grown out of discussions between Mary Sheeran, Koen Claessen and me. 
When it comes to implementation of the ideas we had in these discussions, I have been responsible. 
Push arrays, first appearing in paper D, were invented by Koen Claessen. However, the 
implementation of them in the setting of Obsidian was done by me. Paper D also has large 
contributions by Mary Sheeran when it comes to examples and implementing them. 

Paper E was the first I wrote with Josef Svenningsson, who had been thinking about an interesting
variation of counting sort and wanted to implement it. Together we added atomic operations to 
Obsidian and I modified the code generator to accommodate them. Sections 
1 and 2 in the paper are written by Josef. Sections 3 through 7 are mostly my work, including 
the benchmarking. Section 8 is joint work.
%Me and Josef are equal partners in this paper and got much valuable advice from 
%Mary Sheeran. 

\subsubsection{Papers G and H} 
The work behind paper G, work was performed by me 
under supervision of Ryan R. Newton who is also mostly responsible for the writing of the 
paper. Programming and running time experimentation was performed by me, while getting much 
valuable advice and guidance from Ryan. 

The work behind paper H was done by me as a hobby project and my aspirations were not 
high. I just wanted to experiment with some embedded languages techniques that we had 
been avoiding (had no need for) in Obsidian. Mary discovered what I was doing and applied 
the right amount of pressure, leading to us encompassing much more of ArBB functionality than 
I had in mind. The paper's text is joint work; most examples as well as benchmarking are my work,
but the sparse matrix vector multiplication example is entirely Mary's. 

%Intel has unfortunately retired the ArBB project. This leaves EmbArBB without a functioning 
%code generating backend u

% \subsection*{Guide} 

\tableofcontents


\cleardoublepage
\clearpage

\pagenumbering{arabic}
\pagestyle{fancy}
\fancyfoot{}
\fancyhead[LO]{}
\fancyhead[RO]{\leftmark}
\renewcommand{\headrulewidth}{0.0pt}
\fancyhead[LE,RO]{\thepage}

% --------------------------------------------------------------------------- %
% --------------------------------------------------------------------------- %
%
%  INTRODUCTION 
%
% --------------------------------------------------------------------------- %
% --------------------------------------------------------------------------- %
\chapter{\thesistitle}

\section{Introduction} 

This thesis is about applying embedded language techniques to programming 
multi- and many-core computers. Computers today are becoming more and more  
parallel. With each new generation of 
general purpose processor (CPU) or graphics processor (GPU), we get more 
parallel resources. To fully make use of modern CPUs and GPUs, we need to
exploit their parallelism. 

While it is sometimes possible to automatically parallelise programs originally 
written for serial execution, it is becoming clear that explicitly parallel 
programming models are needed for making use of all parallel resources~\cite{STRUCTURED}. 
This also means that it is important to think in parallel from the beginning. 

In this thesis the focus is on data-parallelism, as opposed to task- or 
control-parallelism. Data parallelism is applicable where the input data can 
be divided into chunks and operated upon independently. \emph{more, and maybe not here}

Modern CPUs are parallel in a number of different ways. They discover instruction
level parallelism (ILP), they have multiple processor cores and vector units for 
Single Instruction Multiple Data (SIMD)  parallelism. The exact configuration, number
of cores and number of vector units, varies over available processor models. 
To obtain maximum performance, with a specific target processor in mind, the programmer
can use low-level intrinsics in order to get SIMD parallelism and a threading library
for parallelism over the available cores. One problem with this is that the resulting program 
would be specialised to a specific processor model. The Intel Array Building Blocks (ArBB) 
system approaches this problem by providing a high level programming interface~\citet{ARBB2011}.
This interface is based on a set of parallel operations on arrays, such as map, folds 
and scans. ArBB was implemented as an embedded language in C++, but a low-level C interface to 
its capabilities was also provided. ArBB programs are JIT-ed (Just-In-Time compiled) to 
target the available CPU or accelerator (such as the Intel Larrabee~\citet{Larrabee}, 
knights Ferry, Xeon Phi). In chapter~\ref{chap:ArBB}, there are papers describing work 
towards embedding Intel ArBB in Haskell. 

Graphics Processors (GPUs) trade programmer convenience for increased parallelism. 
For example, where CPUs dedicate large portions of chip area to caches, branch prediction and 
ILP, GPUs put additional cores. This comes at a price of increased programmer effort.
GPUs have programmer managed shared memories, conditionals are troublesome from a performance 
perspective and memory access patterns can make or break the performance of an 
application.  

Intel ArBB is part of a movement towards thinking of structure and patterns in 
parallel programming. Other languages and libraries with similar aspirations 
are NVIDIA Thrust~\citet{THRUST} and Data.Array.Accelerate~\citet{ACCELERATEDAMP11}.
In the book ``Structured Parallel Programming'' by Michael McCool et al, a case is 
made for thinking in terms of building blocks for parallel programming~\citet{STRUCTURED}. 
They emphasise the importance of collective array operations such as map, fold, scans, 
permutations and gather/scatter operations. The book contains examples in ArBB, Cilk Plus, 
Intel Threading Building Blocks, OpenMP and OpenCL. Not all these languages include 
the patterns as abstractions, but most are possible to express in each of the languages. 
The patterns, while very powerful as abstractions, also provide tools for thinking about 
parallel algorithms. 

The patterns are powerful abstractions, but it is also important not to be locked into 
a single implementation of each pattern. This could have negative performance impacts. 
In ArBB, for example, the patterns used are specialised to the current hardware during JIT 
compilation. Languages like OpenCL and CUDA do not provide abstractions of these patterns. 
The programmer needs to implement them using lower level features, making CUDA and OpenCL 
code less composable and harder to reuse. As an example, look at the slides from NVIDIA in 
reference~\citet{reduction} where a reduction primitive is implemented in CUDA. First a 
naive reduction pattern is implemented and then refined over many steps, taking many 
characteristics of the GPU into mind. The end result is an implementation of the 
reduction pattern that performs very well on the chosen GPU. But as GPUs evolve, the 
design space may need to be explored again. 
%What is 
%is that says that we do not need to redo this exploration with each new GPU generation 
%and discover a new optimal solution?  

Obsidian is our embedded language for GPU programming. We raise the level of abstraction 
compared to CUDA and provide a more composable interface. But we also want to keep 
the possibility for the programmer to implement patterns in different ways. A balance 
must be struck between providing enough low-level control to get at the performance 
and still making building blocks implemented by the programmer reusable and composable. 
We want to provide more compositional tools for the programmer to experiment with while 
exploring the space of various ways to implement a given pattern or primitive. 
In chapter~\ref{chap:GPUProgramming}, there are papers describing work on Obsidian. 

% \section{The problem}

% \emph{what is so interesting about Parallelism and GPUs}

% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
%
% GPU Programming 
%
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
\section{GPUs and CUDA} 

In 2006 NVIDIA released their first Compute Unified Device Architecture (CUDA) 
GPU. With CUDA, NVIDIA also made it a lot easier for programmers to use GPUs 
for general purpose programming. A dialect of C, called CUDA C, was released specifically 
to simplify implementation of general purpose algorithms on GPUs. 

A CUDA GPU is built around a single kind of processor (as opposed to the different 
kinds of processors found in earlier GPUs). The processors in the GPU (called MPs, 
MultiProcessors) all contain a number of function units called SPs (Streaming Processors). 
Each MP also contains local memory, called shared memory since it can be accessed 
by all of the SPs in that MP. The number of MPs varies over the available GPUs; cheaper 
GPUs have as few as one MP, and as you go up in price the number of MPs increases.

Each MP of the GPU can manage a large number of threads; on today's GPUs up to 
2048 threads can run on a single MP. The GPU schedules threads in groups of 32, called 
{\em Warps}, that are executed in lock-step (SIMD style execution). Threads are 
also divided into {\em Blocks}; the threads within a block can communicate using the 
shared memory. The maximum block size is currently 1024 threads. Since 
the number of threads in a block can be higher than the warp size, a barrier 
synchronisation mechanism exists to ensure that threads within a block 
can communicate safely via the shared memory. 


Papers B, C, E and F (sections~\ref{sec:paperB},~\ref{sec:paperC},~\ref{sec:paperE},
~\ref{sec:paperF}) contain more information about GPUs, and the assumptions we make 
about them in relation to Obsidian. 

\subsection{CUDA programming} 

The CUDA programming model reflects the hierarchical architecture of GPUs. The programmer 
writes what are called {\em kernels}; in CUDA jargon a kernel refers to a sequential C program 
that is parameterised on a thread's identity. The kernel program is executed in parallel across 
all the threads of a block and many such blocks of threads can execute in parallel on the 
available MPs. The programmer needs to decompose the algorithm and data in such a way that 
communication will be local to the blocks. 

Below is a very simple CUDA example to give some indication of what a kernel and 
the code to launch that kernel looks like. 

\begin{small}
\begin{Verbatim}[samepage=true]
__global__ void kernel(int32_t *i1, int32_t *i2, int32_t *r) {
  
  unsigned int gid = blockIdx.x * 
                     blockDim.x + 
                     threadIdx.x;

  r[gid] = i1[gid] + i2[gid]; 

} 

\end{Verbatim}
\end{small}

This kernel takes two input arrays, {\tt i1} and {\tt i2}, and computes 
element wise sums into the result array, {\tt r}. The {\tt threadIdx} variable identifies 
a thread within a block. There is also a {\tt blockIdx} variable that identifies 
the block a thread belongs to and {\tt blockDim.x} specifies the number of threads per block. 


In CUDA there is a notion of {\em host} and {\em device}, the device is the GPU and 
its memory (Global memory, usually a few Gigabytes). The host refers to the system housing 
the GPU. The host runs a control program that launches kernels on the GPU. First memory 
needs to be allocated in the device to hold the inputs and outputs. Then data is copied from 
the host's system memory into device memory. 

\begin{small}
\begin{Verbatim}[samepage=true]
int main(void) {
  
  int32_t h_i1[32], h_i2[32], h_r[32]; 

  int32_t *d_i1, *d_i2, *d_r;

  
  for (int i = 0; i < 32; ++i) { 
    h_i1[i] = i;
    h_i2[i] = 32 - i;
  }

  // Allocate device memory
  cudaMalloc((void**)&d_i1,32*sizeof(int32_t));
  cudaMalloc((void**)&d_i2,32*sizeof(int32_t));
  cudaMalloc((void**)&d_r,32*sizeof(int32_t));
  
  // Copy inputs to device
  cudaMemcpy(d_i1,
             h_i1,
             32*sizeof(int32_t),
             cudaMemcpyHostToDevice);
  cudaMemcpy(d_i2,
             h_i2,
             32*sizeof(int32_t),
             cudaMemcpyHostToDevice);
  
  // Launch the kernel
  kernel<<<1,32,0>>>(d_i1,d_i2,d_r); 

  // Copy result to host
  cudaMemcpy(h_r,
             d_r,
             32*sizeof(int32_t),
             cudaMemcpyDeviceToHost);

  for (int i = 0; i < 32; ++i) { 
    printf("%d ",h_r[i]);
  }

  return 0;
}
\end{Verbatim} 
\end{small}

The syntax for launching a kernel is:

\begin{small}
\begin{Verbatim}[samepage=true]
name<<<Blocks, Threads/Block, BytesShared>>>(arg1,..,argN);
\end{Verbatim}
\end{small}

\noindent In this example above one block of 32 threads using no shared memory, is launched. 

A kernel that uses shared memory takes a form similar to whats shown below. 

\begin{small}
\begin{Verbatim}[samepage=true]
__global__ void kernel2(int32_t *i, int32_t *r) {

  extern __shared__ int32_t sm[]; 

  unsigned int tid = threadIdx.x; 
  unsigned int gid = blockIdx.x * 
                     blockDim.x + 
                     threadIdx.x;

  sm[tid] = i[gid]; 
  __syncthreads();

  /* Compute on sm */ 
     
  __syncthreads();
  r[gid] = sm[tid]; 
  
}
\end{Verbatim} 
\end{small} 

\noindent A portion of the input array is stored into shared memory. A 
barrier synchronisation is used to ensure that all threads within the block 
has stored its value into shared memory (only necessary if threads will 
communicate across warp boundaries). After computing on the data locally 
it is written back into global memory. 


\subsection{Writing efficient GPU kernels} 

Writing efficient GPU kernels is hard. There are many details that the CUDA 
programmer must be aware of in order to make optimal use of the GPU. 
the NVIDIA ``CUDA C Best Practices Guide''~\citet{BestPrac}, is a good source 
to learn more about these details. Here is a list with explanations of some 
of the practices that the guide deem most important. 

\begin{itemize} 

\item {\bf Minimise data transfers between host and device:} The bandwidth between device memory 
and GPU is over 20 times of the bandwidth between host memory and the GPU device via the PCIe bus.
This means that, first of all one must evaluate if the computation to be perform is significant 
enough to warrant a transfer to the GPU. If it computation is not arithmetic intensive it may be 
better to do the work on the CPU. However, if the data is already in device memory and 
the operation you are about to perform on it is more efficiently handled by the CPU, it may still 
be better to let the GPU do the work. In other words, compute near the data unless transferring it 
pays off. 

\item {\bf Ensure that global memory accesses are coalesced:} Certain access patterns into 
GPU device memory can be {\em coalesced} (combined into few memory transactions). Unfortunately 
what these access patterns are very some depending on the GPU. For a representative model of 
GPU the rule is that concurrent memory accesses (by threads within a warp) will be coalesced 
to as many transactions as cache lines touched (128b l1 cache lines). This means that strided 
accesses from within a warp are not favoured. 

\item {\bf Minimise use of global memory:} Accessing global device memory can take as 
many 400 to 600 clock cycles if the data is not cached. The programmer should 
use shared memory as much as possible to avoid redundant loading from global memory.
Shared memory can also be used to coalesce reads from global memory. Shared memory is 
a limited resource in the MP, so if a kernel uses very much shared memory it means 
fewer such blocks can be active on the MP. 

\item {\bf Use a multiple of 32 threads per block:} Groups of 32 threads, warps, are the 
scheduled unit on a GPU. If a warp accesses memory (and will be waiting for it arrive during 
600 cycles) it will be swapped out and another warp will take its place. This means that 
the number of available warps decides how well latency can be hidden. A block that is not a 
multiple of 32 leads to wasting GPU resources, some processing elements will stand idle during 
the execution of the not full warps. 

\item {\bf Avoid different execution paths within a block:} Any branching instructions that 
causes the execution paths to diverge within a warp will affect performance. When the execution 
paths within a warp diverges the computation is serialised. 

\item {\bf Do not use {\tt \_\_synchthreads()} in diverging code:} Extreme care must be taken 
when synchronising within any conditionally executed code; every thread must reach this barrier.
Failing to ensure this will likely lead to the kernel producing incorrect results. Note that 
this is a major limitation to the composability of CUDA functions. If it is safe or 
not to call some function at a certain place in your code, can only be decided by inspecting 
that functions implementation. 
\end{itemize} 

The list above provides some very general guide lines of what to think about when 
writing GPU kernels. The lesson is that fast memory near the processing element 
should be preferred over slow memory far away. However, this comes at the effort 
of decomposing computation and data in a way that makes this possible. 
 

%\citet{FERMIOPT}
%\citet{BestPrac} 
%\citet{merrill}


% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
%
% Embedded languages
%
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
\section{Embedded languages}

This thesis applies embedded language methodology to data-parallel programming. An 
embedded language is implemented as a library in a host language, Haskell in this case. 
The embedded language approach is beneficial since we are not entirely sure how 
to program current and future highly parallel computers. Implementing embedded languages allows 
us more rapid prototyping of ideas. Other benefits of embedded languages often listed 
are: making use of the host language's type system for guaranteeing generating correctly typed 
target code, parser and libraries (to some extent) 
and to have the host language as a powerful macro language for your embedded language. 

A very good paper, and highly relevant to our approach, is ``Compiling Embedded Languages''
by Conal Elliott et al~\citet{COMPILEEDSL}. This paper describes how to embed a compiler 
backend and generate efficient code in some target language. One key aspect of this 
approach is that the embedded language is a library of functions that create and 
compose Abstract Syntax Trees (ASTs). Obsidian (chapter \ref{chap:GPUProgramming}) as 
well as EmbArBB (chapter \ref{chap:ArBB}) are implemented in a similar way. An 
embedding that builds ASTs is called a deep embedding. Shallow embeddings, on the 
other hand, do not build ASTs. In ``Combining Deep and Shallow Embedding for EDSL'' 
Josef Svenningsson and Emil Axelsson combine the two methods, shallow and deep, 
in order to simplify the AST data type (fewer constructors) and make more extensible 
embedded languages \citet{DEEPSHALLOW}. Obsidian also uses a combination of shallow 
and deep embeddings, while EmbArBB is a more traditional deep embedding. 

The embedded language approach has become popular to use as a way of raising the 
level of abstraction in fields where the default is to use low-level languages. 
For example, Lava and Wired in the fields of hardware design and 
verification and Feldspar for digital signal processing~\citet{LAVA,Wired,FELDSPAR2010}. For GPU 
programming, there are numerous embedded approaches using various host languages. 
Two languages for GPU programming embedded in Haskell are  Accelerate and Nikola 
\citet{ACCELERATEDAMP11, NIKOLA}. And there is Intel ArBB, embedded in C++, with the 
goal of being parallel across platforms \citet{ARBB2011}.
Related work and information about the tools, libraries and language that make the 
wider context surrounding Obsidian can be found in section~\ref{sec:relatedwork}. 

% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
%
% Obsidian
%
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
\section{Obsidian: An embedded language for GPU kernel implementation}

Obsidian aspires to raise the level of abstraction of GPU kernel implementation, 
while maintaining enough control over details that influence performance on a GPU. 
We try to reach these goals by offering abstractions to the programmer that are 
compositional and parallel. We also ensure that these can easily be compiled 
to NVIDIA CUDA code. We have two different array representations, each with 
its strengths and weaknesses. We also feel that it is important that the programmer 
be able to use local shared memory in computations and have the ability to influence 
memory access patterns. What these array representations and abstractions are 
will be explained in the following sections, with references to the papers 
that introduce them or explain them the best. 


\subsection{Deep embedding: The {\tt Program} data type}

Obsidian uses a deeply embedded {\tt Program} data type that represents GPU Kernels. 
A CUDA GPU has a hierarchy of parallel resources. At the bottom there are threads, each 
executing sequential programs. There are groups of threads (of 32) called {\em Warps} that 
execute in lock-step. Warps are the scheduled unit of work on a GPU. There are {\em Blocks} 
of threads, a set of threads that run as a group and share local (shared) memory. And 
lastly there is a {\em Grid} of blocks that specifies the total number of threads involved 
in a computation (Number\_of\_Blocks * Number\_of\_Threads). The deeply embedded program 
data type of Obsidian models this hierarchy by parameterising programs on a hierarchy 
level type parameter (the  {\tt t} parameter below).

\begin{verbatim} 
data Program t a where
\end{verbatim}

The {\tt t} parameter can be either {\tt Thread}, {\tt Warp}, {\tt Block} or {\tt Grid}.
These types are related to each other via a {\tt Step} type constructor that represents 
going upward one step in the hierarchy. 

\begin{verbatim} 
data Step a -- A step in the hierarchy
data Zero
\end{verbatim} 

The {\tt Thread} type is level {\tt Zero} ({\tt type Thread = Zero}). Then {\tt Block} is 
{\tt Step Thread} and {\tt Grid} is {\tt Step Block}. Currently {\tt Warp} is not included 
in the hierarchy leading to warp programming being a special case. One benefit a warp has is 
that threads can communicate via shared memory without using synchronisation primitives. There 
is no programmer support for warp level programming in CUDA (as there is for block level 
programs). The programmer needs to take care to ensure that the communication patterns are 
within a warp and is then free to remove synchronisations. If warps were to be put in the 
program level hierarchy of Obsidian, its place would be between the thread and block level. 
This would force the programmer to go via warps when putting together a block computation even 
if the communication pattern is not suited for that, leading to inconvenience with no gain. 

As an example of how the hierarchy level parameter is used, I show the {\tt ForAll} constructor 
from the {\tt Program} data type.  This constructor represents parallelism either over threads 
or blocks. 

\begin{verbatim}
ForAll :: EWord32 
            -> (EWord32 -> Program t ())
            -> Program (Step t) ()
\end{verbatim}

{\tt ForAll} takes a number of parallel iterations, a body represented by a function 
from an index to a program. The body is a {\tt Program} at a some level {\tt t} while the 
resulting program is at a level step above. This means a thread program can be turned into 
a block program by using {\tt ForAll} or that a block program can be turned into a grid program. 

Information about the {\tt Program} data type can be found in paper F, \paperFTitle, in 
section \ref{sec:paperF}. 

\subsection{Scalars} 

Scalars and operations on scalars are represented by an expression data type ({\tt Exp a}) in 
Obsidian. This is another example of a deep embedding; the expression data type contains 
constructors for literal values, arithmetic operations, and conditionals. 

Haskell's type class system allows us to overload arithmetic operations such as {\tt (+)}, 
{\tt (-)} and {\tt (*)} by making expressions an instance of {\tt Num}. This allows the 
Obsidian arithmetic expressions to look exactly like corresponding native Haskell arithmetic. 

The approach taken to embed the scalar language is very similar to what is described 
in ``Compiling Embedded Languages''~\citet{COMPILEEDSL}; but we use a Generalised Algebraic 
Datatype (GADT) to obtain typed expressions. An alternative to the GADT, is to use phantom 
types. Earlier versions of Obsidian (section~\ref{sec:paperB} used phantom types. 


\subsection{Arrays} 

Obsidian has two array representations. These array representations are implemented as 
shallow embeddings on top of the expression and program data types. This means that these 
arrays will disappear during Haskell evaluation of the Obsidian program.
 
\subsubsection{Pull arrays}

Pull arrays have been part of Obsidian from the very beginning~\citet{JMT} but called either 
{\tt Array} or just {\tt Arr}. A pull array represents an array as an indexing function; given 
an index it provides an element: 

\begin{verbatim} 
data Pull s a = Pull {pullLen :: s, 
                      pullFun :: EWord32 -> a}
\end{verbatim} 

Pull arrays are parameterised on both element ({\tt a}) and length ({\tt s}) type. The length 
parameter is used to be able to represent arrays with both static (known at Haskell runtime) 
or dynamic (length represented by an expression, an unknown length). When creating block and 
warp level computations we want to know the exact lengths in order to avoid generating code 
riddled with conditionals concerning array lengths. Conditionals are problematic on GPUs and 
while general programs that run for any size is possible when performance is crucial a fixed 
size is recommended. 

One benefit of pull arrays are that they give fusion of all operations on them for free. for example {\tt map f} is implemented on pull arrays by composing {\tt f} with the indexing function: 

\begin{verbatim} 
map   f (Pull n ixf) = Pull n (f . ixf)
\end{verbatim}

Now, we see that {\tt map f . map g} is the same as {\tt map f . g} since both result in {\tt f}
and {\tt g} becoming composed onto the indexing function. No intermediate array is built in 
memory.  

Another benefit of pull arrays are that they are parallel. Simply put, early Obsidian was pull 
arrays in addition to a way to compute the array and store its elements to memory 
(called {\tt sync} in early versions of Obsidian and later {\tt force}). A very 
direct way to compile a pull array to a GPU is to launch as many threads as there are 
elements in the array and apply the indexing function to the thread id. Now each thread 
will have computed an element of the array and can store that to memory. 

 
\subsubsection{Push arrays}

Push arrays were added to Obsidian as a complement to pull arrays. Pull arrays are 
good because they are so simple to understand and easy to parallelise. However certain
operations on pull arrays yield code that is not suitable for GPU execution. Concatenation 
or interleaving of pull arrays result in conditionals in the indexing function. The 
interleaving case, which is worst, leads to diverging branching in every warp. If branches 
diverge the GPU will execute turn off the processing elements taking one path and 
allow the others to progress; then turning to the other branch. The computation of the 
branches has become serialised and compute resources are wasted. 

\begin{verbatim}
data Push p s a =
  Push s ((a -> EWord32 -> TProgram ()) -> Program p ())
\end{verbatim} 

\subsection{Results}

\emph{point to results in each paper} 

\emph{provide information around the results in papers}

\subsection{Compilation to CUDA}

% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
%
% ArBB work
%
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
\section{Embedding Intel Array Building Blocks} 

Intel ArBB~\citet{ARBB2011} is a system for high-level data-parallel programming with the 
ability to generate code for a variety of different hardware configurations. It is implemented 
as an embedded language in C++. One motivation for ArBB that is mentioned in~\citet{ARBB2011} is 
that while high-performance computing specialists are highly competent at implementing 
kernels using low-level programming techniques, mainstream developers are not. ArBB offers 
a more composable way to write programs that make use of core and vector (SIMD) parallelism and 
doing this while using a familiar language, C++.  

One of the greatest strengths of ArBB, as I see it, is that it also comes with a low-level C 
interface. The main purpose of this C interface is to make it easier to use the ArBB system 
from other languages. Many languages have foreign function interfaces that are geared towards C.
This gives ArBB a level of language independence on top of its cross platform abilities.  


ArBB is based on a set of parallel primitives (or structures) on dense one, two and three dimensional vectors and nested vectors: 
\begin{itemize} 
\item {\bf Reductions:}   {\tt add\_reduce}, {\tt mul\_reduce}, {\tt max\_reduce} ... 
\item {\bf Scans:}        {\tt add\_scan}, {\tt mul\_scan}, {\tt max\_scan} ... 
\item {\bf Sorting:}      {\tt sort} 
\item {\bf Permutations:} {\tt gather}, {\tt scatter}, {\tt reverse} ...
\item {\bf Sequential loops:} {\tt \_for}, {\tt \_while} 
\item {\bf Map, zipWith, stencil:}  {\tt map} 
\end{itemize}

These operations (and many more, there are about 120 operations in total) can be used 
by the programmer when writing programs using ArBB. Note that there are sequential looping 
constructs with special names, {\tt \_for} and {\tt \_while}. ArBB is a deeply embedded 
language and calling the ArBB functions does not actually execute them, it just ads a node to 
some internal representation. Therefore, the use of normal C++ for loops leads to unrolled 
ArBB programs and a special {\tt \_for} loop is needed to get a loop in the generated program. 
This is exactly the way these things work in Haskell embedded languages, but it may be even 
more surprising to a C++ programmer. Before an ArBB function can be computed it needs to be 
captured. For this, ArBB has a {\tt call} function that is used to run functions using ArBB 
functionality. The first time {\tt call} is used on a function pointer that function is 
executed and the ArBB functions used in it build an AST (we say the function has been 
captured). The AST is then compiled and specialised for the hardware available. Any 
subsequent calls of a function using ArBB functionality do not lead to a recompilation, 
the first call caches this compiled code. 

The first step of our work with embedding Intel ArBB functionality was to implement very direct 
and low-level bindings to the C interface. This provides a Haskell function (in the IO Monad) 
for each of the functions in the ArBB C interface. More details and results of this work 
can be found in paper G in section~\ref{sec:paperG}.

%\noindent\emph{What does ArBB do, what are the operations and datastructures it support} 

%\noindent\emph{What does EmbArBB do} 

\subsection{EmbArBB: Embedding ArBB in Haskell}
\label{sec:EmbArBB} 

The ArBB Haskell bindings provide a very rudimentary interface to ArBB functionality and 
is not useful for application implementation. A higher-level interface to ArBB to offer 
the Haskell programmer is needed. Our first attempt was to implement an Accelerate backend 
using the ArBB bindings. However, this had some hard problems to solve. Accelerate is a 
more richer language and there was an API mismatch between Accelerate and ArBB. These problems 
are mentioned in in paper G, section~\ref{sec:paperG}. As a way to offer ArBB functionality 
to the Haskell in a way that is useful and simpler to implement, we start working on EmbArBB. 

With EmbArBB we improve the interfacing with Haskell by implementing a deeply embedded language. 
Now, programs in EmbArBB create ASTs. These ASTs are compiled via the ArBB bindings creating 
function objects that can then be run. 

\subsubsection{Dense and nested arrays}

ArBB supports one, two and three dimensional arrays, called dense vectors. In EmbArBB we 
represent these with a data type, {\tt DVector}, parameterised by dimensionality and 
element type. The dimension parameter can be either {\tt Dim1}, {\tt Dim2} or {\tt Dim3}. 
There are also nested vectors, in EmbArBB called {\tt NVector}, parameterised on element type. 

\subsubsection{Operations on arrays} 



\subsubsection{Deep embedding}

EmbArBB is a traditional deep embedding, there is an AST data type with constructors 
representing each ArBB language feature. It is true that a deep embedding of ArBB 
leads to duplication of effort. The low-level operations exported by the 
ArBB C interface already construct an AST within the ArBB system. One possible benefit 
of the deep embedding is that it buys independence from the ArBB backend. It is possible 
to compile the AST generated on the Haskell side to some other target language. 

When compiling EmbArBB to ArBB a sharing detection pass is performed. This pass detects 
sharing in the AST using the method of A. Gill~\citet{Gill}. One benefit of this pass 
is that it reduces the numbers of calls into the ArBB C library. The efficiency of code 
generated is probably not affected since ArBB would discover that sharing in a Common 
Subexpression Elimination (CSE) pass. Another potential benefit is that sharing detection 
on the Haskell side results in a simpler AST being built on the ArBB side; potentially making 
the job for the ArBB optimisation passes simpler. What effects these 
operations truly had on performance has not been investigated. Actually, applying a sharing 
detection technique was one of my personal motivations for working on EmbArBB. In Obsidian 
we have not seen the need for this technique.

One benefit of a deep embedding is that transformations can be performed on the AST. Applying 
any optimisations in EmbArBB would very likely duplicate effort already put into ArBB. 
Therefore we apply no further transformations after sharing detection. However, if EmbArBB 
would be used to generate code in some other target language a set of optimisations 
would be needed.  

\subsubsection{Evaluation} 

In paper H ,section~\ref{sec:paperH}, we benchmark EmbArBB against ArBB in C++ and the Haskell 
Repa library. EmbArBB compares favourably to Repa and is often a lot faster. EmbArBB and the ArBB 
C++ performance is not distinguishable from EmbArBB's indicating that our Haskell embedding 
adds no extra overhead. Of course this is expected, since once the ArBB system has generated 
the code, that code is identical no matter if the AST was build via Haskell or C++ calls. 
Missing is a comparison of how well EmbArBB and C++ compares in regards to the time it actually 
takes to go through the process of making those ArBB C API calls. Although if the generated 
code is used many times the caching ArBB does of generated code means that the initial code 
generation cost can be amortised. 

\subsection{Current status of ArBB and EmbArBB} 

Since our work with ArBB, Intel has unfortunately retired the ArBB system. This 
leaves EmbArBB without a functioning code generating backend. This makes the choice of a
deep embedding (in hindsight) the correct one. EmbArBB could be resurrected by implementing 
a new code generating backend. 


% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
%
% Related work
%
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
\section{Related work} 
\label{sec:relatedwork}
\FloatBarrier

In figure~\ref{fig:researchmatrix} I our work is placed in relation to other languages,
embedded languages and libraries for parallel programming. The systems are roughly divided 
into three groups, based on their level of abstraction. There are low-level languages, here 
I place languages that are imperative and C-like. In the middle layer I place languages 
that go a bit further, they have higher level abstractions and are more easily composable. 
In the highest level I place languages that completely abstract away from details of the 
hardware they run on. The division is not free from personal bias and the placement of languages
in boxes was not always easy. 

The figure also divides the languages according to what hardware they support. There 
are CPU specific languages, GPU specific languages and languages that support or aspire 
to support CPU,GPU and accelerator (Larrabee) execution. 

With Obsidian we try to target the sparsely occupied area of mid-level GPU programming. With 
CUDA, the programmer has full control of how to divide the computation amongst threads and 
blocks. Using Accelerate, Nikola and Thrust the programmer would use high-level patterns
without direct insight into how the application would be mapped onto threads and blocks of the 
GPU. Using Obsidian we want to bridge that gap by both being more composable than CUDA 
but give the programmer more control of what the computation will look like on the GPU. This 
is done by allowing the programmer to specify and compose thread-level and block-level code. 
Recently, the CUB library arrived that is built on a similar idea, that GPU programmers need to 
have block/thread level control to get maximum performance, while maintaining a higher level 
and more Thrust-like programmer interface~\citet{CUB}. Before CUB, and as far as I am aware, 
we were alone at targeting this level.  

The ArBB and EmbArBB systems also provide a set of parallel primitives that the programmer
composes to build his application. I place the ArBB and EmbArBB in a box slightly lower than 
Accelerate since while ArBB/EmbArBB also have built in reduction primitives, just like Accelerate, 
they are not general primitives. Where Accelerate have a single higher order reduction 
operation, ArBB/EmbArBB have {\tt reduce\_add, reduce\_mul} and so on. Benefits of 
having ArBB embedded in Haskell compared to C++ are discussed in section~\ref{sec:EmbArBB}.
%I will go into 
%what benefits there are to having ArBB embedded in Haskell compared to C++ in 


\begin{figure}
\begin{center}
\begin{tikzpicture}[align=center, scale=2.5]
% \draw[help lines] (0,0) grid (3,3);


\draw[->] (-0.5,0) -- (-0.5,3) node[anchor=east] {Abstraction level};

\draw (0,3) node[anchor=south west] {CPUs};
\draw (1,3) node[anchor=south west] {GPUs};
%\draw (2,3) node[anchor=south west] {Accelerator};
\draw (2,3) node[anchor=south west] {CPUs, GPUs\\ Accelerators};

\node[anchor=west] (acc) at (1,2.7) {Accelerate\\Nikola\\Thrust};
\node[anchor=west] (acc) at (1,1.7) {CUB};
\node[anchor=west] (repa) at (0,2.6) {DPH\\Nesl\\Repa\\Meta-repa};
\node[anchor=west] (mixed) at (0,1.7) {Cilk+\\ TBB};
\node[anchor=west] (accelerator) at (2,1.7) {Accelerator};

\node[anchor=west] (cuda) at (1,0.3) {CUDA};
\node[anchor=west] (opencl) at (2,0.3) {OpenCL};

\node[anchor=west] (OpenMP) at (0,0.3) {OpenMP\\PThreads};

\foreach \x in {0,...,2} { 
  \foreach \y in {0,...,2} { 
    \draw (\x,\y) rectangle (\x+1,\y+1);
  }
}

\draw[red!75,rounded corners,ultra thick] (1,0.4) rectangle (2,2.4);
\node[red!75,rotate=45,thick] (obs) at (1.5,1.5) {Obsidian};

\draw[orange!90,rounded corners,ultra thick] (2,1.4) rectangle (3,2.6);
\node[orange!90,thick] (arbb) at (2.5,2) {EmbArBB\\ArBB};


\end{tikzpicture}
\end{center}
\caption{Placing our work in the landscape of languages and libraries for parallel 
general purpose programming of CPUs, GPUs or both. }
\label{fig:researchmatrix} 
\end{figure}



\subsection{Parallelism} 

\noindent\emph{DPH}\citet{DPH} \newline
\noindent\emph{D.A.Accelerate}\citet{ACCELERATEDAMP11} \newline
\noindent\emph{Nikola}\citet{NIKOLA}\newline
\noindent\emph{Repa}\citet{REPA}\newline
\noindent\emph{Meta-Repa}\citet{METAREPA}\newline
\noindent\emph{ArBB}\citet{ARBB2011}\newline
\noindent\emph{OpenCL}\citet{OpenCL}\newline
\noindent\emph{CUDA}\citet{CUDA}\newline
\noindent\emph{Microsoft Accelerator}\citet{ACCELERATOR}\newline
\noindent\emph{Copperhead}\citet{copperhead}\newline
\noindent\emph{Delite}\citet{DELITE}\newline
\noindent\emph{A monad for deterministic parallelism (R. Newton)} \citet{MonadPar} \newline
\noindent\emph{Nesl}\citet{NESL} \newline

\subsection{Embedded domain specific languages} 

\noindent\emph{Feldspar}\citet{FELDSPAR2010} \newline
\noindent\emph{Lava}\citet{lavaICFP} \newline
\noindent\emph{Vertigo}\citet{VERTIGO}  \newline


\subsection{Compiling Embedded Languages}

\noindent\emph{Generic monad constructs (josef + emil + anders}\citet{Generic} \newline
\noindent\emph{The Constrained Monad Problem (Gill)}\citet{sculthorpe2013constrained} \newline
\noindent\emph{Compiling Embedded Languages}\citet{COMPILEEDSL} \newline

\FloatBarrier
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
%
% Future work
%
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
\section{Future work} 

\noindent\emph{Find a niche for Obsidian} 

\noindent\emph{Improve on Warp level capabilities} 

\noindent\emph{Improve expressivity, code performance} 

% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
%
% Discussion
%
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------

\section{Discussion} 

\noindent\emph{What contributions are made by Obsidian} 

\noindent\emph{What works well with Obsidian}

\noindent\emph{EmbArBB contributions} 

% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
%
% Thesis overview
%
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
\section{Thesis overview} 

%This thesis contains an introduction and a selection of papers. The introduction 
%gives background and context to the papers. 

The papers contained in this thesis fall in three categories. 
\begin{itemize} 
\item General EDSL methodology in chapter~\ref{chap:EDSLImplementation}. 
\item GPU Programming using an EDSL in chapter~\ref{chap:GPUProgramming}. 
\item Retargetable parallel programming in chapter~\ref{chap:ArBB}. 
\end{itemize} 
%In the following sections, each paper is given a short description. Hopefully 
%there is enough information to wet the appetite. 

\subsection{EDSL implementation papers} 

\subsubsection{\paperA: \\ \paperATitle} 

Authors: Josef Svenningsson and Bo Joel Svensson 

\vspace{5mm}

\noindent This paper describes a simple and compositional method for 
reification of monads. This is useful for compilation of 
monadic embedded languages. We use a simple robot control language 
as the example. Robot programs can be expressed using Haskell do notation 
and is compiled down to a simple first order program representation. 
There is also a graphical simulator available that runs the compiled 
code. Robot language and simulator is available at github: \url{github.com/svenssonjoel/Robot}.

\subsection{GPU programming papers} 

\subsubsection{\paperB: \\ \paperBTitle}

Authors: Bo Joel Svensson, Mary Sheeran and Koen Claessen \newline

\vspace{5mm}

The first real paper about Obsidian, our embedded language for GPU 
programming. In this paper we identify similarities between 
connection patterns in hardware and parallel computations on GPUs. 
We introduce a structured and compositional way to write data-parallel 
programs on a GPU.

Basic parallel building blocks are implemented in a monadic style and 
composed using a sequential composition operator {\tt ->-}. 

\subsubsection{\paperC: \\ \paperCTitle}

Authors: Bo Joel Svensson, Koen Claessen and Mary Sheeran \newline

\vspace{5mm}

In this paper we describe a version of Obsidian implemented with a 
a different internal representation of GPU programs. We realised that 
most of our kernels have similar shape, they are parallel operations 
composed in sequence interspersed with barrier synchronisations. The 
internal representation used here captures this case exactly. Programming 
is now done is a style resembling arrows. 


\subsubsection{\paperD: \\ \paperDTitle}

Authors: Koen Claessen, Mary Sheeran and  Bo Joel Svensson \newline

\vspace{5mm}

This paper introduces {\em Push} arrays, invented by Koen Claessen. This 
array representation target specific performance problems that we want 
to solve with Obsidian. Here we have also gone back to using monadic 
style for programming in Obsidian. 


\subsubsection{\paperE: \\ \paperETitle} 

Authors: Josef Svenningsson, Bo Joel Svensson and Mary Sheeran \newline

\vspace{5mm}

This paper contains a sorting case study performed in Obsidian. It 
is however the algorithms that are in focus here. We describe 
an interesting variation on Counting sort that removes duplicate 
elements and is a nice fit for GPUs because of its little need for 
synchronisation. 

In order to implement these sorting algorithms we added atomic operations 
to Obsidian. 

\subsubsection{\paperF: \\ \paperFTitle}

Authors: Bo Joel Svensson and Mary Sheeran \newline
\noindent \emph{This work has not yet been published.}
\vspace{5mm} 

\noindent 

In this paper the most recent additions and improvements of Obsidian are 
described. Obsidian Programs are now parameterised on Hierarchy level. This forces
the programmer into writing programs that we know how to compile to a GPU easily 
(by limiting how parallel operations can be nested). The paper also gives a detailed  
optimisation story using Obsidian for implementing reduction kernels. 


\subsection{Retargetable parallel programming} 

\subsubsection{\paperG: \\ \paperGTitle}

Authors: Bo Joel Svensson and Ryan R. Newton

\vspace{5mm}

\noindent This position paper describes the work I did while on a 3 month internship at 
Intel. The main part of this work was implementation of Haskell bindings 
for the now retired Intel ArBB system. 

The paper also describes work we did towards implementing a backend for the 
Accelerate system using ArBB. We reached a proof-of-concept implementation 
capable of running some simple Accelerate programs using the ArBB backend.
However, much work would remain to be able to run the full scope of Accelerate 
programs on ArBB.  

\subsubsection{\paperH: \\ \paperHTitle}

Authors: Bo Joel Svensson and Mary Sheeran 

\vspace{5mm}

\noindent Here we explore another way to provide the capabilities of ArBB to the 
Haskell programmer. Rather than trying to use ArBB as a backend to Accelerate 
we provide a more direct mapping of ArBB's functionality to Haskell idioms. 
We call this embedded language EmbArBB and it provides a more Haskell-programmer-friendly 
interface compared to the raw ArBB bindings (which are a collection of functions 
in the IO monad). 

% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------
%
% BIBLIOGRAPHY
%
% ---------------------------------------------------------------------------
% ---------------------------------------------------------------------------

\bibliographystylet{alpha}
\bibliographyt{thesis}
%\addcontentsline{toc}{section}{Bibliography}




\clearpage{}

% --------------------------------------------------------------------------- %
% --------------------------------------------------------------------------- %
%
%  PAPERS 
%
% --------------------------------------------------------------------------- %
% --------------------------------------------------------------------------- %


\chapter{EDSL Implementation Papers}
\label{chap:EDSLImplementation}
% ---------------------------------------------------------------------------
% 
% ---------------------------------------------------------------------------
\cleardoublepage 


\section[\paperATitle]{\paperA: \\ \paperATitle}
\label{sec:paperA}
%\addcontentsline{toc}{chapter}{Simple and Compositional Reification of Monadic Embedded Languages}

% \paperATitle

\begin{center} 
Josef Svenningsson, Bo Joel Svensson
\end{center}


\input{./bb/paperThesis}



\chapter{GPU Programming Papers}
\label{chap:GPUProgramming}

% ---------------------------------------------------------------------------
% 
% ---------------------------------------------------------------------------
\cleardoublepage 

\section[\paperBTitle]{\paperB: \\ \paperBTitle}
\label{sec:paperB}
%\addcontentsline{toc}{chapter}{Obsidian: A Domain Specific Embedded Language for
%Parallel Programming of Graphics Processors}

%\paperBTitle

\begin{center} 
Bo Joel Svensson, Mary Sheeran, Koen Claessen
\end{center}

\input{./ifl/paperThesis}

% ---------------------------------------------------------------------------
% 
% ---------------------------------------------------------------------------
\cleardoublepage 

\section[\paperCTitle]{\paperC: \\ \paperCTitle}
\label{sec:paperC}
%\addcontentsline{toc}{chapter}{Obsidian: GPU Computing Using Haskell}

%\paperCTitle

\begin{center} 
Bo Joel Svensson, Koen Claessen, Mary Sheeran
\end{center}

\input{./papp/paperThesis}


% ---------------------------------------------------------------------------
% 
% ---------------------------------------------------------------------------
\cleardoublepage 

\section[\paperDTitle]{\paperD: \\ \paperDTitle}
\label{sec:paperD}
%\addcontentsline{toc}{chapter}{Expressive Array Constructs in an Embedded GPU Kernel Programming Language}

%\paperDTitle

\begin{center} 
Koen Claessen, Mary Sheeran, Bo Joel Svensson
\end{center}

\input{./expressive/paperThesis}

% ---------------------------------------------------------------------------
% 
% ---------------------------------------------------------------------------
\cleardoublepage 

\section[\paperETitle]{\paperE: \\ \paperETitle}
\label{sec:paperE}
%\addcontentsline{toc}{chapter}{Counting and Occurrence Sort for GPUs using an Embedded Language}

% \paperETitle

\begin{center} 
Josef Svenningsson, Bo Joel Svensson, Mary Sheeran 
\end{center}


\input{./csort/paperThesis}


% ---------------------------------------------------------------------------
% 
% ---------------------------------------------------------------------------
\cleardoublepage 


\section[\paperFTitle]{\paperF: \\ \paperFTitle}
\label{sec:paperF}
%\addcontentsline{toc}{chapter}{Simple and Compositional Reification of Monadic Embedded Languages}

% \paperFTitle

\begin{center} 
Bo Joel Svensson, Mary Sheeran
\end{center}


\input{./hl/paperThesis}



% ---------------------------------------------------------------------------
% 
% ---------------------------------------------------------------------------

\chapter{Retargetable Parallel Programming Papers}
\label{chap:ArBB}
% ---------------------------------------------------------------------------
% 
% ---------------------------------------------------------------------------
\cleardoublepage 


\section[\paperGTitle]{\paperG: \\ \paperGTitle}
\label{sec:paperG}
%\addcontentsline{toc}{chapter}{Parallel Programming in Haskell Almost for Free}

% \paperGTitle 

\begin{center} 
Bo Joel Svensson, Ryan R. Newton
\end{center}


\input{./arbb/paperThesis}

% ---------------------------------------------------------------------------
% 
% ---------------------------------------------------------------------------
\cleardoublepage 


\section[\paperHTitle]{\paperH: \\ \paperHTitle}
\label{sec:paperH}
%\addcontentsline{toc}{chapter}{Parallel Programming in Haskell Almost for Free}

% \paperHTitle

\begin{center} 
Bo Joel Svensson, Mary Sheeran
\end{center}


\input{./embarbb/paperThesis}


\cleardoublepage


% ---------------------------------------------------------------------------
% nocites 
% ---------------------------------------------------------------------------
\nocite{*}



\makeatletter
\renewenvironment{thebibliography}[1]
     {\chapter*{\bibname}%
      \@mkboth{\MakeUppercase\bibname}{\MakeUppercase\bibname}%
      \list{\@biblabel{\@arabic\c@enumiv}}%
           {\settowidth\labelwidth{\@biblabel{#1}}%
            \leftmargin\labelwidth
            \advance\leftmargin\labelsep
            \@openbib@code
            \usecounter{enumiv}%
            \let\p@enumiv\@empty
            \renewcommand\theenumiv{\@arabic\c@enumiv}}%
      \sloppy
      \clubpenalty4000
      \@clubpenalty \clubpenalty
      \widowpenalty4000%
      \sfcode`\.\@m}
     {\def\@noitemerr
       {\@latex@warning{Empty `thebibliography' environment}}%
       \endlist}
\makeatother

\bibliographystyle{alpha}
\bibliography{thesis}
\addcontentsline{toc}{chapter}{Bibliography}

\end{document}
